{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "picn6KMJpRs7"
   },
   "source": [
    "# task\n",
    "Участие в соревновании https://github.com/dialogue-evaluation/RuREBus по NER и RE.\n",
    "Форма сдачи: ноутбук с кодом и метриками качества ваших моделей.\n",
    "Оцениваение: минимально в 2 раза больше обычного дз (если сделаете совсем много, возможны дополнительные бонусы).\n",
    "На чуть больше половины баллов за проект достаточно правильно сделать CharCNN-BLSTM-CRF для NER и вариант модели Miwa & Bansal (2016) для RE.\n",
    "Дедлайн 1 мая 9 утра (4 недели)\n",
    "\n",
    "описание разметки текста: https://github.com/dialogue-evaluation/RuREBus/blob/master/markup_instruction.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2C18S3m-Kaz"
   },
   "source": [
    "# libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1682892085316,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "e1485YJR-4aA",
    "outputId": "4a76fecb-ec10-46e9-d104-8c8c973c05e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "razdel\n",
    "pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4983,
     "status": "ok",
     "timestamp": 1682892090862,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "BefWijZ-_CFY",
    "outputId": "96afe12b-3cff-4832-83a0-60bbb5ba7b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: razdel in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1682892091417,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "hZ31IF6r8k-3",
    "outputId": "4d266b2c-a3a5-4929-d399-fd721551ad12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-30 22:01:30--  https://raw.githubusercontent.com/dialogue-evaluation/RuREBus/master/eval_scripts/brat_format.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5030 (4.9K) [text/plain]\n",
      "Saving to: ‘brat_format.py’\n",
      "\n",
      "brat_format.py      100%[===================>]   4.91K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-04-30 22:01:30 (67.4 MB/s) - ‘brat_format.py’ saved [5030/5030]\n",
      "\n",
      "--2023-04-30 22:01:30--  https://raw.githubusercontent.com/dialogue-evaluation/RuREBus/master/eval_scripts/evaluate_ners.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4017 (3.9K) [text/plain]\n",
      "Saving to: ‘evaluate_ners.py’\n",
      "\n",
      "evaluate_ners.py    100%[===================>]   3.92K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-04-30 22:01:30 (40.1 MB/s) - ‘evaluate_ners.py’ saved [4017/4017]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/dialogue-evaluation/RuREBus/master/eval_scripts/brat_format.py\n",
    "!wget https://raw.githubusercontent.com/dialogue-evaluation/RuREBus/master/eval_scripts/evaluate_ners.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4993,
     "status": "ok",
     "timestamp": 1682892096408,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "YvEo3sBW-OAI"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from razdel import sentenize, tokenize\n",
    "from collections import namedtuple\n",
    "from itertools import chain\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchcrf import CRF #https://pytorch-crf.readthedocs.io/en/stable/\n",
    "\n",
    "from evaluate_ners import compute_precision_and_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMUChQkt0X8P"
   },
   "source": [
    "# 1. dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAuIejsKRKkx"
   },
   "source": [
    "## 1.1 download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682892096409,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "RMiYezIrxduR"
   },
   "outputs": [],
   "source": [
    "#удаление файлов\n",
    "for i in glob('*'):\n",
    "  !rm -rf {i}\n",
    "\n",
    "# # чек типа файла\n",
    "# !apt install file\n",
    "# !file RuREBus.git\n",
    "\n",
    "#создание папки\n",
    "!mkdir train_data\n",
    "!mkdir test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1755,
     "status": "ok",
     "timestamp": 1682892098162,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "TxSljguIxTis",
    "outputId": "4c65232e-9f09-4729-9170-6f20f95051ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RuREBus'...\n",
      "remote: Enumerating objects: 247, done.\u001b[K\n",
      "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 247 (delta 16), reused 4 (delta 1), pack-reused 217\u001b[K\n",
      "Receiving objects: 100% (247/247), 14.22 MiB | 24.04 MiB/s, done.\n",
      "Resolving deltas: 100% (118/118), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/dialogue-evaluation/RuREBus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682892098162,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "Ely8jZVFpMFR",
    "outputId": "e0225f46-f6c8-44ff-b142-ee0f229782f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_data/train_part_2',\n",
       " 'train_data/train_part_3',\n",
       " 'train_data/train_part_1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in glob('RuREBus/train_data/*'):\n",
    "  !unzip \"{i}\" -d \"train_data\"\n",
    "\n",
    "for i in glob('RuREBus/test_data/test_full*'):\n",
    "  !unzip \"{i}\" -d \"test_data\"\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "glob('train_data/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l7JSONX09m8"
   },
   "source": [
    "## 1.2 preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE6VNAH6eHBl"
   },
   "source": [
    "### 1.2.1 sentenize & tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1682892098707,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "RZ7jW4HZ9tp_"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_dataset(path_to_files: str):\n",
    "    \"\"\"\n",
    "    take txt files and tokenize them by razdel liba \n",
    "\n",
    "    params: path_to_files - path to train files\n",
    "    return: dict[path] = (file_sentences, file_tokenize)\n",
    "            file_sentences - [ Substring(start, stop, sentence),...]\n",
    "            file_tokenize  - [ Substring(start, stop, token),...]\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for file_i in glob(path_to_files+'/*.txt'):\n",
    "      short_file_i = file_i.replace(path_to_files+'/', '').replace('.txt','')\n",
    "      file_txt = open(file_i, 'r', encoding='utf-8').read()\n",
    "      file_sentences = list(sentenize(file_txt))\n",
    "      file_tokenize = [[tok for tok in tokenize(sent.text)] for sent in file_sentences ]\n",
    "      result[short_file_i] = (file_sentences, file_tokenize)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1682892099375,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "cba7YjveXIHd",
    "outputId": "1fd2b90d-46d8-4211-9af0-7cfff18f42a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_tokenized_dataset(\"train_data/train_part_1\")\n",
    "len(a['20336241021100524345002_22_part_1'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVCVMVlteUxv"
   },
   "source": [
    "### 1.2.2 сущности (NER) и отношения (RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682892099376,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "pCzybuyyYKie"
   },
   "outputs": [],
   "source": [
    "entuple = namedtuple('entuple', ['name', 'start', 'stop', 'text'])\n",
    "retuple = namedtuple('retuple', ['name', 'arg1', 'arg2'])\n",
    "\n",
    "def get_annotated_dataset(path_to_files: str):\n",
    "    \"\"\"\n",
    "    take ann files and parse them\n",
    "\n",
    "    params: path_to_files - path to train files\n",
    "    return: dict[path] = {'entity': entities_dict,\n",
    "                          'relate': relations_dict}\n",
    "            entities_dict  - {entity_id: ['name', 'start', 'stop', 'text']}\n",
    "            relations_dict - {relate_id: ['name', 'arg1', 'arg2']}\n",
    "                             arg1, arg2 taken from list of entity_id\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for file_i in glob(path_to_files+'/*.ann'):\n",
    "        short_file_i = file_i.replace(path_to_files+'/', '').replace('.ann','')\n",
    "\n",
    "        file_txt_list = open(file_i, 'r', encoding='utf-8').readlines()\n",
    "        file_txt_list = [i.strip().split('\\t') for i in file_txt_list]\n",
    "\n",
    "        if len(file_txt_list) > 0:\n",
    "            # сущности\n",
    "            entities = [i for i in file_txt_list if i[0][0]=='T']\n",
    "            entities = [(i[0], i[1], ' '.join(i[2:])) if len(i)>3 else i for i in entities] #на случай если в тексте была табуляция\n",
    "            assert len(entities) == len(set([i[0] for i in entities])), 'allert ent #1: имена сущностей (T***) не уникальны'\n",
    "            assert min([len(i[1].split()) for i in entities]) == max([len(i[1].split()) for i in entities]), 'allert ent #2: что-то не так в определении сущности'\n",
    "\n",
    "            entities_dict = dict( [(tag, entuple(attr.split()[0], int(attr.split()[1]), int(attr.split()[2]), text))\n",
    "                                      for (tag, attr, text) in entities] )\n",
    "            # отношения\n",
    "            relations = [i for i in file_txt_list if i[0][0]=='R']\n",
    "            assert len(relations) == len(set([i[0] for i in relations])), 'allert rel #1: имена сущностей (R***) не уникальны'\n",
    "            relations_dict = dict( [(tag, retuple(attr.split()[0], attr.split()[1].split(':')[1], attr.split()[2].split(':')[1]))\n",
    "                                      for (tag, attr) in relations] )\n",
    "            assert len( set( [i.arg1 for i in relations_dict.values()] + [i.arg2 for i in relations_dict.values()] ) - set(entities_dict.keys()) ) == 0, \"allert rel #2: встречены сущности не встречающиеся в entities\"\n",
    "\n",
    "            result[short_file_i] = {'entity': entities_dict,\n",
    "                                    'relate': relations_dict}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682892099376,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "dt6UvU0y_ysx",
    "outputId": "6b14109e-2c22-4f46-94d5-e4d5af55c109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = get_annotated_dataset('train_data/train_part_1')\n",
    "len(b['31339011021101006981005_6_part_0']['entity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KO0V-9muedue"
   },
   "source": [
    "### 1.2.3 сущности текущего текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1682892099376,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "nYoCB1wUYO1e"
   },
   "outputs": [],
   "source": [
    "def get_entities_from_text(sent_start, sent_stop, ent_dict):\n",
    "    \"\"\"\n",
    "    filtering entities_dict taking information about the boundaries of the sentence\n",
    "\n",
    "    params: sent_start - start of sentence\n",
    "            sent_stop  - end of sentence\n",
    "            ent_dict - dictionary of entities\n",
    "    return: entities_dict = {entity_id: ['name', 'start', 'stop', 'text']}\n",
    "    \"\"\"\n",
    "    entity_filter_list = [ entity_id for entity_id, (_, entity_start, entity_stop, _) in ent_dict['entity'].items()\n",
    "                      if sent_start <= entity_start <= entity_stop <= sent_stop ]\n",
    "    result = {'entity': {entity_id: ent_dict['entity'][entity_id] for entity_id in entity_filter_list}}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOYlN2ONekKK"
   },
   "source": [
    "### 1.2.4 словари сущностей и отношений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1682892100178,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "RZ-P0EzGYrs_",
    "outputId": "2f25f536-fa33-477c-a96e-d24283487b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity dictionary:\n",
      "{'B-ECO': 1, 'I-ECO': 2, 'B-INST': 3, 'I-INST': 4, 'B-SOC': 5, 'I-SOC': 6, 'B-ACT': 7, 'I-ACT': 8, 'B-CMP': 9, 'I-CMP': 10, 'B-MET': 11, 'I-MET': 12, 'B-QUA': 13, 'I-QUA': 14, 'B-BIN': 15, 'I-BIN': 16, 'O': 0}\n",
      "\n",
      "relate dictionary:\n",
      "{'FNT': 0, 'PPS': 1, 'NPS': 2, 'PNT': 3, 'TSK': 4, 'FNG': 5, 'NNG': 6, 'GOL': 7, 'NNT': 8, 'FPS': 9, 'PNG': 10}\n"
     ]
    }
   ],
   "source": [
    "entity_set = set()\n",
    "relate_set = set()\n",
    "for path_to_files in ['train_data/train_part_1', 'train_data/train_part_2', 'train_data/train_part_3']:\n",
    "    annotated_dataset_i = get_annotated_dataset(path_to_files)\n",
    "    for file_i in annotated_dataset_i:\n",
    "      for _, (ent_name,_,_,_) in annotated_dataset_i[file_i]['entity'].items():\n",
    "          entity_set = entity_set.union(set([ent_name]))\n",
    "      for _, (rel_name,_,_) in annotated_dataset_i[file_i]['relate'].items():\n",
    "          relate_set = relate_set.union(set([rel_name]))\n",
    "\n",
    "entity_list = [['B-'+x, 'I-'+x] for x in entity_set]\n",
    "entity_list = list(chain(*entity_list))\n",
    "\n",
    "entity_dict = {entity_list[i]: i+1 for i in range(len(entity_list))}\n",
    "entity_dict['O'] = 0\n",
    "\n",
    "rel_2_id = {y:x for x,y in enumerate(relate_set)}\n",
    "id_2_rel = {x:y for x,y in enumerate(relate_set)}\n",
    "\n",
    "print('entity dictionary:')\n",
    "print(entity_dict)\n",
    "\n",
    "print('\\nrelate dictionary:')\n",
    "print(rel_2_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1682867695301,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "EbuJ6ukTAaAv"
   },
   "outputs": [],
   "source": [
    "# # в качестве доп инфо для RE\n",
    "# ner_dict = {y:x for x,y in enumerate(entity_set)}\n",
    "# ner_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX9pjM6YPbMV"
   },
   "source": [
    "# 2. NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpgEpA-1onUL"
   },
   "source": [
    "## 2.1 BIO разметка\n",
    "B - begin, I - inner, O - outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682892100178,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "PWJgvGANUlqu"
   },
   "outputs": [],
   "source": [
    "Sample = namedtuple(\"Sample\", \"text, tokens, labels\")\n",
    "\n",
    "def get_ner_dataset(path_file_list: list):\n",
    "    samples = []\n",
    "    for path_to_files in path_file_list:\n",
    "\n",
    "        # path_to_files = 'train_data/train_part_1'\n",
    "\n",
    "        tokenized_dataset = get_tokenized_dataset(path_to_files)\n",
    "        annotated_dataset = get_annotated_dataset(path_to_files)\n",
    "\n",
    "        for file_name in tokenized_dataset:\n",
    "            #print(file_name)\n",
    "            sentences, tokenized_sentences = tokenized_dataset[file_name] #[ Substring(start, stop, sentence),...]\n",
    "            try:\n",
    "                annotations = annotated_dataset[file_name]                    #[ Substring(start, stop, token),...]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            for sent_id, (sentence, tokenized_sentence) in enumerate(zip(sentences, tokenized_sentences)):\n",
    "                entities_from_sentence = get_entities_from_text(sentence.start, sentence.stop, annotations) #{entity_id: ['name', 'start', 'stop', 'text']}\n",
    "                tokens = [token.text for token in tokenized_sentence]\n",
    "\n",
    "                if len(entities_from_sentence['entity'])==0:\n",
    "                    labels = [entity_dict['O']]*len(tokenized_sentence)\n",
    "                    # labels = [entity_dict[i] for i in labels]\n",
    "                else:\n",
    "                    labels = []\n",
    "                    for token in tokenized_sentence:\n",
    "                      label = entity_dict['O']\n",
    "                      for _, (ent_name,ent_start,ent_end,_) in entities_from_sentence['entity'].items(): #{entity_id: ['name', 'start', 'stop', 'text']}\n",
    "                          if sentence.start+token.start == ent_start:\n",
    "                              label = 'B-'+ent_name\n",
    "                              label = entity_dict.get(label, 'O')\n",
    "                          elif sentence.start+token.start > ent_start and sentence.start+token.stop <= ent_end:\n",
    "                              label = 'I-'+ent_name\n",
    "                              label = entity_dict.get(label, 'O')\n",
    "                      labels.append(label)\n",
    "\n",
    "                sample = Sample(sentence.text, tokens, labels)\n",
    "                samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-yxSdOVg-JV"
   },
   "source": [
    "## 2.2 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 6875,
     "status": "ok",
     "timestamp": 1682892107052,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "syWxUn1hgh0f"
   },
   "outputs": [],
   "source": [
    "data = get_ner_dataset(glob('train_data/*'))\n",
    "random.shuffle(data)\n",
    "\n",
    "train_size = int(len(data)*0.8)\n",
    "train = data[:train_size]\n",
    "val = data[train_size:]\n",
    "test = get_ner_dataset(['test_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj6w5Ghss9yC"
   },
   "source": [
    "множество слов и символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1682892107052,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "FNwQmKUMqnly",
    "outputId": "5d29612c-006d-4f75-f4a4-ace6384f1864"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = list( {ch for sample in train for token in sample.tokens for ch in token} )\n",
    "char_set.insert(0, '<unk>'), char_set.insert(0, '<pad>')\n",
    "word_set = list( {token for sample in train for token in sample.tokens} )\n",
    "word_set.insert(0, '<unk>'), word_set.insert(0, '<pad>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3kabRoirqDw"
   },
   "source": [
    "## 2.3 генерация батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1682892107053,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "8iT9MOt_ZtcN"
   },
   "outputs": [],
   "source": [
    "# max_seq_len=500\n",
    "# max_char_seq_len=40\n",
    "# batch_size=32\n",
    "def get_next_gen_batch(samples, max_seq_len=512, max_char_seq_len=40, batch_size=2):\n",
    "    indices = np.arange(len(samples))\n",
    "    np.random.shuffle(indices)\n",
    "    batch_begin = 0\n",
    "    with tqdm(total=len(samples)) as pbar:\n",
    "      while batch_begin < len(samples):\n",
    "          batch_indices = indices[batch_begin: batch_begin + batch_size]\n",
    "          batch_words = []\n",
    "          batch_chars = []\n",
    "          batch_labels = []\n",
    "          batch_max_len = 0\n",
    "          batch_masks = [] #for CRF\n",
    "          for data_ind in batch_indices:\n",
    "              sample = samples[data_ind] #беру одно предложение\n",
    "              words = torch.zeros(max_seq_len, dtype=torch.long).cuda()\n",
    "              inputs = torch.zeros((max_seq_len, max_char_seq_len), dtype=torch.long).cuda()\n",
    "              for token_num, token in enumerate(sample.tokens[:max_seq_len]): #цикл по токенам предложения, обрезанного до max_seq_len\n",
    "                  #слова\n",
    "                  words[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                  #символы\n",
    "                  for char_num, char in enumerate(token[:max_char_seq_len]):\n",
    "                      inputs[token_num][char_num] = char_set.index(char) if char in char_set else char_set.index('<unk>')\n",
    "              labels = sample.labels[:max_seq_len]         #аналогично с labels\n",
    "              masks = [1]*len(labels) + [0]*(max_seq_len - len(labels)) \n",
    "              labels += [0] * (max_seq_len - len(labels))  #аналогично с labels\n",
    "              \n",
    "              batch_words.append(words)\n",
    "              batch_chars.append(inputs)\n",
    "              batch_labels.append(labels)\n",
    "              batch_masks.append(masks)\n",
    "              \n",
    "          batch_begin += batch_size\n",
    "          pbar.update(batch_size)\n",
    "          \n",
    "          batch_words = torch.stack(batch_words)\n",
    "          batch_chars = torch.stack(batch_chars)\n",
    "          labels = torch.cuda.LongTensor(batch_labels)\n",
    "          batch_masks = torch.tensor(batch_masks).cuda()>0\n",
    "          yield batch_indices, batch_words, batch_chars, labels, batch_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1682892107053,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "86J8pN65nng3",
    "outputId": "7573c1ff-55ec-45d0-83b4-7bcfd12d1abb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/8368 [00:00<21:48,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512, 40])\n",
      "torch.Size([2, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, batch_words, batch_chars, labels, _ in get_next_gen_batch(train):\n",
    "    1+1\n",
    "    break\n",
    "print()\n",
    "print( batch_words.size() )\n",
    "print( batch_chars.size() )\n",
    "print( labels.size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYadOzdKxi4D"
   },
   "source": [
    "## 2.4 модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682892107053,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "ev8vBN9WY6GZ",
    "outputId": "6f568047-4f4f-401b-e507-df181802446c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='https://user-images.githubusercontent.com/31881382/35037901-f4a69b14-fb89-11e7-8c28-08a0cb1310cb.png' />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src='https://user-images.githubusercontent.com/31881382/35037901-f4a69b14-fb89-11e7-8c28-08a0cb1310cb.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1682892107053,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "gz_48-oPxnCT"
   },
   "outputs": [],
   "source": [
    "class SuperSimpleModel(nn.Module):\n",
    "    def __init__(self, char_set_size, char_embedding_dim=16, classes_count=len(entity_dict), char_max_seq_len=40):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_layer = nn.Embedding(char_set_size, char_embedding_dim)\n",
    "        self.out_layer = nn.Linear(char_max_seq_len * char_embedding_dim, classes_count)\n",
    "\n",
    "    def forward(self, words, chars):\n",
    "        projections = self.embeddings_layer(chars)\n",
    "        projections = projections.reshape(projections.size(0), projections.size(1), -1)\n",
    "        output = self.out_layer(projections)\n",
    "        return output\n",
    "\n",
    "class CharCNNBiLSTM(nn.Module):\n",
    "    def __init__(self, char_set_size, word_set_size,\n",
    "                 char_embedding_dim=8, word_embedding_dim=64,\n",
    "                 classes_count=len(entity_dict), \n",
    "                 char_max_seq_len=40, lstm_embedding_dim=16,\n",
    "                 kernel_size=3, max_seq_len=512 ):\n",
    "        super().__init__()\n",
    "        self.char_max_seq_len = char_max_seq_len\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.char_embedding = nn.Embedding(char_set_size, char_embedding_dim)\n",
    "        self.char_cnn = nn.Conv1d(in_channels=char_embedding_dim, out_channels=1, kernel_size=kernel_size)\n",
    "        self.word_embedding = nn.Embedding(word_set_size, word_embedding_dim  )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm_layer = nn.LSTM((char_max_seq_len-kernel_size+1)+word_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.out_layer = nn.Linear(lstm_embedding_dim*2, classes_count)\n",
    "\n",
    "    def forward(self, words, chars):\n",
    "        #работа с символами\n",
    "        c_emb = self.char_embedding(chars)\n",
    "        c_emb = c_emb.reshape(c_emb.size(0)*c_emb.size(1), self.char_max_seq_len, self.char_embedding_dim)\n",
    "        c_emb = c_emb.permute(0,2,1)\n",
    "\n",
    "        c_cnn = self.char_cnn(c_emb)\n",
    "        c_cnn = c_cnn.reshape(chars.size(0), self.max_seq_len, -1)\n",
    "\n",
    "        #работа со словами\n",
    "        w_emb = self.word_embedding(words)\n",
    "        com_emb = torch.cat((c_cnn, w_emb), 2)\n",
    "        rnn_output, _ = self.lstm_layer(com_emb)\n",
    "        result = self.out_layer.forward(rnn_output)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVq_ZFVDzGyo"
   },
   "source": [
    "один проход - отладка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1682892107053,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "AyAqLRr5T-9M"
   },
   "outputs": [],
   "source": [
    "model = CharCNNBiLSTM(len(char_set), len(word_set))\n",
    "\n",
    "lr=0.01\n",
    "device_name=\"cuda\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "model = model.to(device)\n",
    "\n",
    "logits = model(batch_words, batch_chars) # Прямой проход\n",
    "logits = logits.transpose(1, 2) # crossentropy require (N,C,d,d,d,d,d), N-batch size, C-number of classes\n",
    "loss = loss_function(logits, labels) # Подсчёт ошибки\n",
    "loss.backward() # Подсчёт градиентов dL/dw\n",
    "optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQOivaaoxnOD"
   },
   "source": [
    "\n",
    "\n",
    "## 2.5 ф-ция обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682892107535,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "pN2_dMMeqrrv"
   },
   "outputs": [],
   "source": [
    "def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n",
    "                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cuda\",\n",
    "                    early_stopping=True):\n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss().cuda()\n",
    "    prev_avg_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        model.train()\n",
    "        for step, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(train)):\n",
    "            logits = model(batch_words, batch_chars) # Прямой проход\n",
    "            logits = logits.transpose(1, 2)\n",
    "            loss = loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            loss.backward() # Подсчёт градиентов dL/dw\n",
    "            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n",
    "            total_loss += loss.item()\n",
    "        val_total_loss = 0\n",
    "        val_batch_count = 0\n",
    "        model.eval()\n",
    "        for _, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(val)):\n",
    "            logits = model(batch_words, batch_chars) # Прямой проход\n",
    "            logits = logits.transpose(1, 2)\n",
    "            val_total_loss += loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            val_batch_count += 1\n",
    "        avg_val_loss = val_total_loss/val_batch_count\n",
    "        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.eval()\n",
    "            break\n",
    "        prev_avg_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHOqFoTnYf1h"
   },
   "source": [
    "## 2.6 метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1682892107535,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "nJmJwa83Qfxi"
   },
   "outputs": [],
   "source": [
    "def predict(model, samples):\n",
    "    model.eval()\n",
    "    tp, fp, fn = 0,0,0\n",
    "    for _, (indices, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(samples)):\n",
    "        logits = model(batch_words, batch_chars)\n",
    "        plabels = logits.max(dim=2)[1]\n",
    "        tp += len([(i,j) for i,j in zip(batch_labels[0].tolist(), plabels[0].tolist()) if (j!=0) and (i==j)])\n",
    "        fp += len([(i,j) for i,j in zip(batch_labels[0].tolist(), plabels[0].tolist()) if (j!=0) and (i!=j)])\n",
    "        fn += len([(i,j) for i,j in zip(batch_labels[0].tolist(), plabels[0].tolist()) if (j==0) and (i!=j)])\n",
    "\n",
    "    recall, precision = compute_precision_and_recall(tp, fp, fn)\n",
    "    try:\n",
    "      f_measure = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "      f_measure = 'None'\n",
    "    print('precision:', precision)\n",
    "    print('recall:   ', recall)\n",
    "    print('f1:       ', f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q31aU6azl8z"
   },
   "source": [
    "## --SimpleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 461081,
     "status": "ok",
     "timestamp": 1682682699984,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "bt_q14aCslrM",
    "outputId": "bb9ceb6a-41b6-46bc-ad51-b75854018a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 13681\n",
      "Epoch = 0, Avg Train Loss = 0.4852, Avg val loss = 0.0956, Time = 460.83s\n"
     ]
    }
   ],
   "source": [
    "model = SuperSimpleModel(len(char_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 684603,
     "status": "ok",
     "timestamp": 1682683392775,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "EmnWPH9sAupk",
    "outputId": "2fada618-41e9-42e4-e5d9-29636ace3acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n",
      "precision: 0.38149556400506973\n",
      "recall:    0.09480314960629921\n",
      "f1:        0.1518668012108981\n",
      "\n",
      " -------on val dataset:--------\n",
      "precision: 0.3604060913705584\n",
      "recall:    0.0849536344600658\n",
      "f1:        0.13749697409828127\n",
      "\n",
      " ------on train dataset:-------\n",
      "precision: 0.3703271028037383\n",
      "recall:    0.09559710494571773\n",
      "f1:        0.15196548418024927\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx2iHZMRMzXW"
   },
   "source": [
    "## --CharCNN + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488640,
     "status": "ok",
     "timestamp": 1682860099785,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "Uaq7sKleM1l_",
    "outputId": "ed57cabe-7a91-49c1-8d42-3ffcffdfd2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 1537706\n",
      "Epoch = 0, Avg Train Loss = 0.2857, Avg val loss = 0.0759, Time = 488.59s\n"
     ]
    }
   ],
   "source": [
    "model = CharCNNBiLSTM(len(char_set), len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 770706,
     "status": "ok",
     "timestamp": 1682860877683,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "COeJijGQRhO-",
    "outputId": "85e5e950-f1f0-4a0c-c9c0-ff78aa4aae91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n",
      "precision: 0.3448667044809983\n",
      "recall:    0.26331745344304897\n",
      "f1:        0.2986247544204323\n",
      "\n",
      " -------on val dataset:--------\n",
      "precision: 0.31992687385740404\n",
      "recall:    0.21739130434782608\n",
      "f1:        0.25887573964497046\n",
      "\n",
      " ------on train dataset:-------\n",
      "precision: 0.336644591611479\n",
      "recall:    0.25459098497495825\n",
      "f1:        0.2899239543726236\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682869945223,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "BsjED-VL2EE5"
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# for step, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(train)):\n",
    "#             # print(batch)\n",
    "#             logits = model(batch_words, batch_chars)\n",
    "#             plabels = logits.max(dim=1)[1]\n",
    "#             # print(logits)\n",
    "#             # print(plabels)\n",
    "#             # print()\n",
    "#             # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "691SUO-lzsyb"
   },
   "source": [
    "## --CharCNN + BiLSTM + CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEzsyZb5TkON"
   },
   "source": [
    "#### без использования масок паддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682863618101,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "xSfpzwLA03eD"
   },
   "outputs": [],
   "source": [
    "def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n",
    "                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cuda\",\n",
    "                    early_stopping=True):\n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = CRF(len(entity_dict), batch_first=True).cuda()\n",
    "    prev_avg_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        model.train()\n",
    "        for step, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(train)):\n",
    "            logits = model(batch_words, batch_chars) # Прямой проход\n",
    "            loss = -loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            loss.backward() # Подсчёт градиентов dL/dw\n",
    "            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n",
    "            total_loss += loss.item()\n",
    "            print(loss.item())\n",
    "        val_total_loss = 0\n",
    "        val_batch_count = 0\n",
    "        model.eval()\n",
    "        for _, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(val)):\n",
    "            logits = model(batch_words, batch_chars) # Прямой проход\n",
    "            val_total_loss += -loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            val_batch_count += 1\n",
    "        avg_val_loss = val_total_loss/val_batch_count\n",
    "        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.eval()\n",
    "            break\n",
    "        prev_avg_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 580010,
     "status": "ok",
     "timestamp": 1682680623626,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "YGCAKBeisti_",
    "outputId": "5628ee53-4a24-48b2-d1f0-29aba3bc9622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 1574290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Avg Train Loss = 8280.9223, Avg val loss = 1504.9847, Time = 576.71s\n"
     ]
    }
   ],
   "source": [
    "model = CharCNNBiLSTM(len(char_set), len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 630948,
     "status": "ok",
     "timestamp": 1682681255535,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "HBXlOWTZWwre",
    "outputId": "a56cf656-e716-4b83-f734-6d04297b5276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n",
      "precision: 0.26618705035971224\n",
      "recall:    0.012701682114658427\n",
      "f1:        0.024246395806028834\n",
      "\n",
      " -------on val dataset:--------\n",
      "precision: 0.22758620689655173\n",
      "recall:    0.01071776550828191\n",
      "f1:        0.020471464019851116\n",
      "\n",
      " ------on train dataset:-------\n",
      "precision: 0.23076923076923078\n",
      "recall:    0.010361067503924647\n",
      "f1:        0.019831730769230768\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B9N8l1CTo0O"
   },
   "source": [
    "#### с использованием масок паддинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682892109615,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "RBAkxVcHTrWj"
   },
   "outputs": [],
   "source": [
    "def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n",
    "                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cuda\",\n",
    "                    early_stopping=True):\n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = CRF(len(entity_dict), batch_first=True).cuda()\n",
    "    prev_avg_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        model.train()\n",
    "        for step, (_, batch_words, batch_chars, batch_labels, batch_masks) in enumerate(get_next_gen_batch(train)):\n",
    "            logits = model(batch_words, batch_chars) # Прямой проход\n",
    "            loss = -loss_function(logits, batch_labels, batch_masks) # Подсчёт ошибки\n",
    "            loss.backward() # Подсчёт градиентов dL/dw\n",
    "            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n",
    "            total_loss += loss.item()\n",
    "        val_total_loss = 0\n",
    "        val_batch_count = 0\n",
    "        model.eval()\n",
    "        for _, (_, batch_words, batch_chars, batch_labels, batch_masks) in enumerate(get_next_gen_batch(val)):\n",
    "            logits = model(batch_words, batch_chars) # Прямой проход\n",
    "            val_total_loss += -loss_function(logits, batch_labels, batch_masks) # Подсчёт ошибки\n",
    "            val_batch_count += 1\n",
    "        avg_val_loss = val_total_loss/val_batch_count\n",
    "        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.eval()\n",
    "            break\n",
    "        prev_avg_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IR3eTLR8VnpC"
   },
   "outputs": [],
   "source": [
    "model = CharCNNBiLSTM(len(char_set), len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHLXS0CXdimf"
   },
   "outputs": [],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgCsRt6uBGtY"
   },
   "source": [
    "# 3. RE with NEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eh5zeKGSwhd"
   },
   "source": [
    "постановка задачи: на основе текста и выделенных в нем сущностей спрогнозировать связь между ними"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cLEucczHTyQ"
   },
   "source": [
    "## 3.1 разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1682892431444,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "6HImgE8sBFwT"
   },
   "outputs": [],
   "source": [
    "Sample = namedtuple(\"Sample\", \"text, tokenized_text, tokens, e1, e2, label\")\n",
    "\n",
    "def get_re_dataset(path_file_list: list):\n",
    "    samples = []\n",
    "    for path_to_files in path_file_list:\n",
    "        # path_to_files = 'train_data/train_part_1'\n",
    "\n",
    "        tokenized_dataset = get_tokenized_dataset(path_to_files)\n",
    "        annotated_dataset = get_annotated_dataset(path_to_files)\n",
    "\n",
    "        for file_name in tokenized_dataset:\n",
    "            #print(file_name)\n",
    "            sentences, tokenized_sentences = tokenized_dataset[file_name] #[ Substring(start, stop, sentence),...]\n",
    "            try:\n",
    "                annotations = annotated_dataset[file_name]                #[ Substring(start, stop, token),...]\n",
    "            except KeyError:\n",
    "                annotations = None\n",
    "\n",
    "            if annotations:\n",
    "                for sent_id, (sentence, tokenized_sentence) in enumerate(zip(sentences, tokenized_sentences)):\n",
    "                    entities_from_sentence = get_entities_from_text(sentence.start, sentence.stop, annotations) #{entity_id: ['name', 'start', 'stop', 'text']}\n",
    "                \n",
    "                    relates = [ i for i in annotations['relate'].values()\n",
    "                                if  (i.arg1 in entities_from_sentence['entity'].keys()) \n",
    "                                and (i.arg2 in entities_from_sentence['entity'].keys())]\n",
    "\n",
    "                    for rel_i in relates:\n",
    "                        arg1 = rel_i.arg1\n",
    "                        ent1 = entities_from_sentence['entity'][arg1]\n",
    "                        arg2 = rel_i.arg2\n",
    "                        ent2 = entities_from_sentence['entity'][arg2]\n",
    "\n",
    "                        tokens = [token.text for token in tokenized_sentence]\n",
    "                        #e1 = (ner_dict[ent1.name], ent1.start-sentence.start, ent1.stop-sentence.start)\n",
    "                        #e2 = (ner_dict[ent2.name], ent2.start-sentence.start, ent2.stop-sentence.start)\n",
    "                        e1 = (ent1.start-sentence.start, ent1.stop-sentence.start)\n",
    "                        e2 = (ent2.start-sentence.start, ent2.stop-sentence.start)\n",
    "                        label = rel_2_id[rel_i.name]\n",
    "\n",
    "                        sample = Sample(sentence.text,\n",
    "                                        [i.text for i in tokenized_sentence], \n",
    "                                        tokens, e1, e2, label)\n",
    "                        samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MLxjN4lrqW1"
   },
   "source": [
    "## 3.2 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 5223,
     "status": "ok",
     "timestamp": 1682892437475,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "TSOei0bx29zT"
   },
   "outputs": [],
   "source": [
    "data = get_re_dataset(glob('train_data/*'))\n",
    "random.shuffle(data)\n",
    "\n",
    "train_size = int(len(data)*0.8)\n",
    "train = data[:train_size]\n",
    "val = data[train_size:]\n",
    "test = get_re_dataset(['test_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_uNHseGZU4o"
   },
   "source": [
    "множество слов и символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1682892437974,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "AU_Fc4EcZSBj",
    "outputId": "cc72347f-032f-42d3-a48d-99abcdd3a51b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = list( {ch for sample in train for token in sample.tokens for ch in token} )\n",
    "char_set.insert(0, '<unk>'), char_set.insert(0, '<pad>')\n",
    "word_set = list( {token for sample in train for token in sample.tokens} )\n",
    "word_set.insert(0, '<unk>'), word_set.insert(0, '<pad>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1dJNqUaHfur"
   },
   "source": [
    "## 3.3 генерация батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1682894305378,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "FYKQs5RDDF--"
   },
   "outputs": [],
   "source": [
    "def get_next_gen_batch(samples, max_seq_len=1024, max_char_seq_len=40, batch_size=2):\n",
    "    indices = np.arange(len(train))\n",
    "    np.random.shuffle(indices)\n",
    "    batch_begin = 0\n",
    "    #with tqdm(total=len(train)) as pbar:\n",
    "    while batch_begin < len(train):\n",
    "          batch_indices = indices[batch_begin: batch_begin + batch_size]\n",
    "          batch_chars = []\n",
    "          batch_words = []\n",
    "          ent_info = []\n",
    "          batch_labels = []\n",
    "          batch_max_len = 0\n",
    "          for data_ind in batch_indices:\n",
    "              sample = train[data_ind] #беру одно предложение\n",
    "              words = torch.zeros(max_seq_len, dtype=torch.long).cuda()\n",
    "              inputs = []\n",
    "              for token_num, token in enumerate(sample.tokens[:max_seq_len]): #цикл по токенам предложения, обрезанного до max_seq_len\n",
    "                  #слова\n",
    "                  words[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                  #символы\n",
    "                  chars = [char_set.index(ch) if ch in char_set else char_set.index(\"<unk>\") for ch in token][:max_char_seq_len] #максимальная символьная длина токена - max_char_seq_len\n",
    "                  chars += [0] * (max_char_seq_len - len(chars)) #каждый токен должен быть представлен max_char_seq_len символами\n",
    "                  inputs.append(chars)\n",
    "              batch_max_len = max(batch_max_len, len(inputs)) #кол-во токенов в предложении\n",
    "              inputs += [[0]*max_char_seq_len] * (max_seq_len - len(inputs)) #приводим предложение к длине max_seq_len\n",
    "              \n",
    "              # отношения взятые по символам\n",
    "              # ent_info.append(((min(sample.e1[0], max_seq_len-1), min(sample.e1[1], max_seq_len-1)), \n",
    "              #                 (min(sample.e2[0], max_seq_len-1), min(sample.e2[1], max_seq_len-1)))\n",
    "              #                 )\n",
    "\n",
    "              # отношения взятые по словам\n",
    "              e1 = (sample.e1[0], sample.e1[1])\n",
    "              e2 = (sample.e2[0], sample.e2[1])\n",
    "              \n",
    "              # записываем наблюдение только если оно попало в наблюдаемое предложение (по максимальной длине предложения)\n",
    "              if (e1[0] < max_seq_len) or (e2[0] < max_seq_len):\n",
    "                try:\n",
    "                  # сущность #1\n",
    "                  e1_txt = [i.text for i in tokenize(sample.text[e1[0]:e1[1]])]\n",
    "                  e1_start = sample.tokenized_text.index(e1_txt[0])\n",
    "                  e1_stop = sample.tokenized_text.index(e1_txt[-1])\n",
    "                  # сущность #2\n",
    "                  e2_txt = [i.text for i in tokenize(sample.text[e2[0]:e2[1]])]\n",
    "                  e2_start = sample.tokenized_text.index(e2_txt[0])\n",
    "                  e2_stop = sample.tokenized_text.index(e2_txt[-1])\n",
    "                  # запись наблюдений\n",
    "                  ent_info.append(((e1_start, e1_stop),\n",
    "                                  (e2_start, e2_stop)))\n",
    "                  batch_chars.append(inputs)\n",
    "                  batch_words.append(words)\n",
    "                  batch_labels.append(sample.label)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "          batch_begin += batch_size\n",
    "          #pbar.update(batch_size)\n",
    "\n",
    "          if len(batch_words)>0:\n",
    "            batch_words = torch.stack(batch_words).cuda()\n",
    "            batch_chars = torch.cuda.LongTensor(batch_chars)\n",
    "            labels = torch.cuda.LongTensor(batch_labels)\n",
    "\n",
    "            yield batch_indices, batch_words, batch_chars, ent_info, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1682894525600,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "ulPdlaxLM9oy",
    "outputId": "cbcfff0b-87b7-40e7-cf85-096202966b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([2, 1024])\n",
      "torch.Size([2, 1024, 40])\n",
      "2\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for _, batch_words, batch_chars, batch_ents, labels in get_next_gen_batch(train):\n",
    "    1+1\n",
    "    break\n",
    "\n",
    "print()\n",
    "print( batch_words.size() )\n",
    "print( batch_chars.size() )\n",
    "print( len(batch_ents) )\n",
    "print( labels.size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a3aPdHVMwgu"
   },
   "source": [
    "## 3.4 модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1682892437975,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "pz3YIyu2Myei"
   },
   "outputs": [],
   "source": [
    "class ReModel(nn.Module):\n",
    "    def __init__(self, char_set_size, word_set_size,\n",
    "                 char_embedding_dim=8, word_embedding_dim=8,\n",
    "                 classes_count=len(rel_2_id), \n",
    "                 char_max_seq_len=40, lstm_embedding_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_layer = nn.Embedding(word_set_size, word_embedding_dim)\n",
    "        #self.lstm_layer_1 = nn.LSTM(word_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        self.hw = nn.Linear(1024*word_embedding_dim, 2*lstm_embedding_dim)\n",
    "        self.h1 = nn.Linear(word_embedding_dim, lstm_embedding_dim)\n",
    "        self.h2 = nn.Linear(word_embedding_dim, lstm_embedding_dim)\n",
    "\n",
    "        self.out_layer = nn.Linear(4*lstm_embedding_dim, classes_count)\n",
    "        \n",
    "    def forward(self, words, chars, ents):\n",
    "        # обработка токенов\n",
    "        projections = self.embeddings_layer(words)\n",
    "        #print(projections)\n",
    "        projections = projections.reshape(projections.size(0),-1)\n",
    "        hw = self.hw(projections)\n",
    "       \n",
    "        # обработка сущностей\n",
    "        pr_e1_batch = []\n",
    "        pr_e2_batch = []\n",
    "        for inp_num, (e1, e2) in enumerate(ents):\n",
    "          #сущность 1\n",
    "          inp_1 = words[inp_num,e1[0]:e1[1]+1]\n",
    "          pr_e1 = self.embeddings_layer(inp_1)\n",
    "          pr_e1 = pr_e1.mean(dim=0)\n",
    "          pr_e1_batch.append(pr_e1)\n",
    "\n",
    "          #сущность 2\n",
    "          inp_2 = words[inp_num,e2[0]:e2[1]+1]\n",
    "          pr_e2 = self.embeddings_layer(inp_2)\n",
    "          pr_e2 = pr_e2.mean(dim=0)\n",
    "          pr_e2_batch.append(pr_e2)\n",
    "        pr_e1 = torch.stack(pr_e1_batch)\n",
    "        pr_e2 = torch.stack(pr_e2_batch)\n",
    "        h1 = self.h1(pr_e1)\n",
    "        h2 = self.h2(pr_e2)\n",
    "        h_cat = torch.cat([h1, h1], dim=1)\n",
    "\n",
    "        #объединяем инфо\n",
    "        com = torch.cat([hw, h_cat], dim=1)\n",
    "        output = self.out_layer.forward(com)\n",
    "        output = torch.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n31uZ-CDMuaV"
   },
   "source": [
    "## 3.5 ф-ция обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBj_v4SYn3Sa"
   },
   "source": [
    "один проход - отладка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1682894534715,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "nfPTvrC-JFpN"
   },
   "outputs": [],
   "source": [
    "model = ReModel(len(char_set), len(word_set))\n",
    "\n",
    "lr=0.01\n",
    "device_name=\"cuda\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss().cuda()\n",
    "model = model.to(device)\n",
    "\n",
    "logits = model(batch_words, batch_chars, batch_ents) # Прямой проход\n",
    "loss = loss_function(logits, labels) # Подсчёт ошибки\n",
    "loss.backward() # Подсчёт градиентов dL/dw\n",
    "optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "optimizer.zero_grad() # Занул\\ение градиентов, чтобы их спокойно менять на следующей итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DZOE3qUn-Nr"
   },
   "source": [
    "полный проход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1682894313351,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "9-Xx9MkXPBHl"
   },
   "outputs": [],
   "source": [
    "def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n",
    "                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cuda\",\n",
    "                    early_stopping=True):\n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss().cuda()\n",
    "    prev_avg_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        model.train()\n",
    "        for step, (_, batch_words, batch_chars, batch_ents, batch_labels) in enumerate(get_next_gen_batch(train)):\n",
    "            if batch_words.size(0)>0:\n",
    "              logits = model(batch_words, batch_chars, batch_ents) # Прямой проход\n",
    "              loss = loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "              loss.backward() # Подсчёт градиентов dL/dw\n",
    "              optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "              optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n",
    "              total_loss += loss.item()\n",
    "        val_total_loss = 0\n",
    "        val_batch_count = 0\n",
    "        model.eval()\n",
    "        for _, (_, batch_words, batch_chars, batch_ents, batch_labels) in enumerate(get_next_gen_batch(val)):\n",
    "            logits = model(batch_words, batch_chars, batch_ents) # Прямой проход\n",
    "            val_total_loss += loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            val_batch_count += 1\n",
    "        avg_val_loss = val_total_loss/val_batch_count\n",
    "        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.eval()\n",
    "            break\n",
    "        prev_avg_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeGxxZVFtZPT"
   },
   "source": [
    "## 3.6 метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1682892455514,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "ztw9qEuMeDqa"
   },
   "outputs": [],
   "source": [
    "def predict(model, samples):\n",
    "    model.eval()\n",
    "    tp, fp, fn = 0,0,0\n",
    "    for _, (_, batch, batch_ents, batch_labels) in enumerate(get_next_gen_batch(samples)):\n",
    "            logits = model(batch, batch_ents)\n",
    "            plabels = logits.max(dim=1)[1]\n",
    "            tp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if i==j])\n",
    "            fp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if i!=j])\n",
    "            fn += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if i!=j])\n",
    "\n",
    "    recall, precision = compute_precision_and_recall(tp, fp, fn)\n",
    "    try:\n",
    "          f_measure = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "          f_measure = 'None'\n",
    "    print('precision:', precision)\n",
    "    #print('recall:   ', recall)\n",
    "    #print('f1:       ', f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUuf43RAeHPv"
   },
   "source": [
    "## --ReModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1819791,
     "status": "ok",
     "timestamp": 1682441728789,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "GVvac5Gs1eQl",
    "outputId": "fb791d46-8135-496d-c561-713c211e82f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 172011\n",
      "Epoch = 0, Avg Train Loss = 5.8837, Avg val loss = 1.7912, Time = 60.10s\n",
      "Epoch = 1, Avg Train Loss = 5.8076, Avg val loss = 1.8443, Time = 60.61s\n",
      "Epoch = 2, Avg Train Loss = 5.8544, Avg val loss = 1.9233, Time = 62.07s\n",
      "Epoch = 3, Avg Train Loss = 5.8812, Avg val loss = 1.8286, Time = 62.12s\n",
      "Epoch = 4, Avg Train Loss = 5.8330, Avg val loss = 1.8156, Time = 62.05s\n",
      "Epoch = 5, Avg Train Loss = 5.8089, Avg val loss = 1.8610, Time = 61.22s\n",
      "Epoch = 6, Avg Train Loss = 5.7973, Avg val loss = 1.8575, Time = 61.57s\n",
      "Epoch = 7, Avg Train Loss = 5.8268, Avg val loss = 1.8676, Time = 62.06s\n",
      "Epoch = 8, Avg Train Loss = 5.8475, Avg val loss = 1.8522, Time = 61.15s\n",
      "Epoch = 9, Avg Train Loss = 5.8249, Avg val loss = 1.8259, Time = 61.57s\n",
      "Epoch = 10, Avg Train Loss = 5.8285, Avg val loss = 1.8529, Time = 60.96s\n",
      "Epoch = 11, Avg Train Loss = 5.8460, Avg val loss = 1.8263, Time = 61.75s\n",
      "Epoch = 12, Avg Train Loss = 5.8192, Avg val loss = 1.8396, Time = 61.08s\n",
      "Epoch = 13, Avg Train Loss = 5.8371, Avg val loss = 1.8213, Time = 59.05s\n",
      "Epoch = 14, Avg Train Loss = 5.8195, Avg val loss = 1.8130, Time = 59.69s\n",
      "Epoch = 15, Avg Train Loss = 5.8297, Avg val loss = 1.8141, Time = 61.01s\n",
      "Epoch = 16, Avg Train Loss = 5.8182, Avg val loss = 1.8601, Time = 61.65s\n",
      "Epoch = 17, Avg Train Loss = 5.8158, Avg val loss = 1.8158, Time = 61.59s\n",
      "Epoch = 18, Avg Train Loss = 5.8484, Avg val loss = 1.8550, Time = 62.02s\n",
      "Epoch = 19, Avg Train Loss = 5.8176, Avg val loss = 1.8113, Time = 60.73s\n",
      "Epoch = 20, Avg Train Loss = 5.8298, Avg val loss = 1.8169, Time = 60.40s\n",
      "Epoch = 21, Avg Train Loss = 5.8189, Avg val loss = 1.8853, Time = 59.37s\n",
      "Epoch = 22, Avg Train Loss = 5.8260, Avg val loss = 1.8299, Time = 59.65s\n",
      "Epoch = 23, Avg Train Loss = 5.8050, Avg val loss = 1.8129, Time = 59.88s\n",
      "Epoch = 24, Avg Train Loss = 5.7984, Avg val loss = 1.8013, Time = 59.81s\n",
      "Epoch = 25, Avg Train Loss = 5.7988, Avg val loss = 1.8615, Time = 59.81s\n",
      "Epoch = 26, Avg Train Loss = 5.8234, Avg val loss = 1.8679, Time = 58.99s\n",
      "Epoch = 27, Avg Train Loss = 5.8471, Avg val loss = 1.8068, Time = 59.14s\n",
      "Epoch = 28, Avg Train Loss = 5.8000, Avg val loss = 1.9208, Time = 59.42s\n",
      "Epoch = 29, Avg Train Loss = 5.8201, Avg val loss = 1.8410, Time = 59.34s\n"
     ]
    }
   ],
   "source": [
    "model = ReModel(len(char_set))\n",
    "train_gen_model(model, train, val, epochs_count=30, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36877,
     "status": "ok",
     "timestamp": 1682495303641,
     "user": {
      "displayName": "Денис Исаев",
      "userId": "06168685232953428184"
     },
     "user_tz": -180
    },
    "id": "4H54BKYuoAkP",
    "outputId": "370b32be-b10a-453c-a940-925bf89382ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n",
      "precision: 0.36665677840403443\n",
      "\n",
      " -------on val dataset:--------\n",
      "precision: 0.3582443653618031\n",
      "\n",
      " ------on train dataset:-------\n",
      "precision: 0.27603143418467585\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on train dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSwuCqx5LPa9qPpkhBTpAf",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

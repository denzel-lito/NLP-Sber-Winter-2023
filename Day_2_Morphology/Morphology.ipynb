{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфология\n",
    "В этом ноутбуке описана подготовка данных для задачи POS-tagging. А также пара простых моделей на keras, решающих данную задачу. Оригинальная задача и ноутбук есть в контесте: https://www.kaggle.com/c/rupos2018/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Загрузка корпуса\n",
    "Здесь мы прочитаем корпуса из csv и разложим их по спискам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:34:21.288108Z",
     "start_time": "2023-03-21T15:34:21.278152Z"
    }
   },
   "outputs": [],
   "source": [
    "# для совместимости со вторым питоном\n",
    "# from __future__ import print_function\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:34:22.478836Z",
     "start_time": "2023-03-21T15:34:22.474741Z"
    },
    "_uuid": "ad156ee50218601b689e546907d0ce3109e155bd"
   },
   "outputs": [],
   "source": [
    "# Имена файлов с данными.\n",
    "TRAIN_FILENAME = \"data/input/train.csv\"\n",
    "TEST_FILENAME = \"data/input/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:34:28.796638Z",
     "start_time": "2023-03-21T15:34:24.263021Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_17027/2972923367.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  pd.read_csv(TRAIN_FILENAME,sep='\\\\t').head(3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>LocalNum</th>\n",
       "      <th>Word</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>А</td>\n",
       "      <td>CONJ#_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>ведь</td>\n",
       "      <td>PART#_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>для</td>\n",
       "      <td>ADP#_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  LocalNum  Word Prediction\n",
       "0   0         1     А     CONJ#_\n",
       "1   1         2  ведь     PART#_\n",
       "2   2         3   для      ADP#_"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# пример данных\n",
    "import pandas as pd\n",
    "pd.read_csv(TRAIN_FILENAME,sep='\\\\t').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:34:28.810534Z",
     "start_time": "2023-03-21T15:34:28.801827Z"
    },
    "_uuid": "fdbea001cf6b2aa7b763b4836050dc4fdabb6457"
   },
   "outputs": [],
   "source": [
    "# Считывание файлов.\n",
    "from collections import namedtuple\n",
    "WordForm = namedtuple(\"WordForm\", \"word pos gram\")\n",
    "\n",
    "def get_sentences(filename, is_train):\n",
    "    sentences = []\n",
    "    with io.open(filename, \"r\", encoding='utf-8') as r:\n",
    "        # Пропускаем заголовок\n",
    "        next(r)\n",
    "        sentence = [] # будем заполнять список предложений\n",
    "        for line in r:\n",
    "            # предложения отделены по '\\n'\n",
    "            if len(line.strip()) == 0:\n",
    "                if len(sentence) == 0:\n",
    "                    continue\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            if is_train:\n",
    "                # Формат: индекс\\tномер_в_предложении\\tсловоформа\\tPOS#Грамемы\n",
    "                word = line.strip().split(\"\\t\")[2]\n",
    "                pos = line.strip().split(\"\\t\")[3].split(\"#\")[0]\n",
    "                gram = line.strip().split(\"\\t\")[3].split(\"#\")[1]\n",
    "                sentence.append(WordForm(word, pos, gram))\n",
    "            else:\n",
    "                word = line.strip().split(\"\\t\")[2]\n",
    "                sentence.append(word)\n",
    "        if len(sentence) != 0:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:34:32.888777Z",
     "start_time": "2023-03-21T15:34:30.035782Z"
    },
    "_uuid": "fe4e1691bc5ef95abc711c5f403b27b897647878",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = get_sentences(TRAIN_FILENAME, True)\n",
    "test = get_sentences(TEST_FILENAME, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:34:41.827371Z",
     "start_time": "2023-03-21T15:34:41.822505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "А \t CONJ \t _\n",
      "ведь \t PART \t _\n",
      "для \t ADP \t _\n",
      "конкретных \t ADJ \t Case=Gen|Degree=Pos|Number=Plur\n",
      "изделий \t NOUN \t Animacy=Inan|Case=Gen|Gender=Neut|Number=Plur\n",
      "зачастую \t ADV \t Degree=Pos\n",
      "нужен \t ADJ \t Degree=Pos|Gender=Masc|Number=Sing|Variant=Brev\n",
      "монокристалл \t NOUN \t Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "не \t PART \t _\n",
      "только \t PART \t _\n"
     ]
    }
   ],
   "source": [
    "# Выыедем, что получилось\n",
    "for wordform in train[0][:10]:\n",
    "    print(wordform.word, '\\t', wordform.pos, '\\t', wordform.gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты далее будем использовать токены слов и POS-теги. Но чтобы определять грамматические значения нужно еще провести некоторые манипуляции с данными, описанные в оригинальном ноутубке. Мы же ограничимся только определением частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Подготовка эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно в качестве признаков для обучения сеток используются словные эмбеддинги. Для этого можно скачать предобученные и сохранить их в матрицу, где в расположатся векторы эмбеддингах по индексам, соответсвующих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:36:18.087861Z",
     "start_time": "2023-03-21T15:36:17.657198Z"
    }
   },
   "outputs": [],
   "source": [
    "#запомним все уникальные слова и POS-теги в корпусе\n",
    "word_set = set()\n",
    "pos_set = set()\n",
    "for sent in train:\n",
    "    for wordform in sent:\n",
    "        word_set.add(wordform.word.lower())\n",
    "        pos_set.add(wordform.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:36:30.023515Z",
     "start_time": "2023-03-21T15:36:30.014973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кризисом, бункера, каталогов, расплывчатость, пятерку, начинка, прижав, обрусевший, учебная, дырки, \n",
      "{'ADP', 'VERB', 'INTJ', 'ADV', 'PART', 'CONJ', 'PRON', 'SCONJ', 'NUM', 'AUX', 'NOUN', 'DET', 'X', 'SYM', 'PUNCT', 'PROPN', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "for word in list(word_set)[:10]: \n",
    "    print(word, end=', ')\n",
    "print()\n",
    "print(pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:36:57.816877Z",
     "start_time": "2023-03-21T15:36:56.347482Z"
    }
   },
   "outputs": [],
   "source": [
    "#Загрузите эмбеддинги c https://nlp.stanford.edu/projects/glove/ или другие, которые вам нравятся и пропишите путь к ним\n",
    "import numpy as np\n",
    "\n",
    "word_embeddings_path = 'data/glove.6B.50d.txt'\n",
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "embedding_size = None\n",
    "#Загружаем эмбеддинги\n",
    "with io.open(word_embeddings_path, 'r', encoding=\"utf-8\") as f_em:\n",
    "    for line in f_em:\n",
    "        split = line.strip().split(\" \")\n",
    "        # Совсем короткие строки пропускаем\n",
    "        if len(split) <= 2:\n",
    "            continue\n",
    "        # Встретив первую подходящую строку, фиксируем размер эмбеддингов\n",
    "        if embedding_size is None:\n",
    "            embedding_size = len(split) - 1\n",
    "            # Также нициализируем эмбеддинги для паддингов и неизвестных слов\n",
    "            word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "            word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n",
    "        # После этого все эмбеддинги должны быть одинаковой длины\n",
    "        if len(split) - 1 != embedding_size:\n",
    "            continue\n",
    "            \n",
    "        #Если слова нет в корпусе, то не будем для него запоминать эмбеддинг        \n",
    "        if (split[0] not in word_set):\n",
    "            continue\n",
    "        \n",
    "        word_embeddings.append(np.asarray(split[1:], dtype='float32'))\n",
    "        word2idx[split[0]] = len(word2idx)\n",
    "\n",
    "word_embeddings = np.array(word_embeddings, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:37:00.864823Z",
     "start_time": "2023-03-21T15:37:00.856918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1948"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set & set(word2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:37:01.230773Z",
     "start_time": "2023-03-21T15:37:01.223792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98880"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как-то эмбеддинги не сильно подходят для данного корпуса поэтому, просто инициализируем рандмно матрицу эмбеддингов при определении сетки. Вам же предлагается все-таки поискать подходящие эмбеддинги и использовать их при обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Подготовка данных\n",
    "Теперь нам остается только пронумеровать все слова и POS-теги и можно переходить к обучению сеток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:41:41.755707Z",
     "start_time": "2023-03-21T15:41:41.715901Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_index = {'PAD' : 0, 'UNK' : 1}\n",
    "for word in word_set:\n",
    "    word_to_index[word] = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:41:52.174483Z",
     "start_time": "2023-03-21T15:41:52.169276Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_to_index = {}\n",
    "index_to_pos = {}\n",
    "for pos in pos_set:\n",
    "    pos_to_index[pos] = len(pos_to_index)\n",
    "    index_to_pos[len(index_to_pos)] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:55:00.143778Z",
     "start_time": "2023-03-21T15:54:59.597092Z"
    }
   },
   "outputs": [],
   "source": [
    "# для полносвязной сетки просто захреначим все в один список\n",
    "data_X = []\n",
    "data_Y = []\n",
    "for sent in train:\n",
    "    for wordform in sent:\n",
    "        data_X.append(word_to_index[wordform.word.lower()])\n",
    "        data_Y.append(pos_to_index[wordform.pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:55:00.569195Z",
     "start_time": "2023-03-21T15:55:00.563887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64376, 46455, 64783, 92522, 59404, 71161, 89654, 9088, 75681, 50023]\n",
      "[5, 4, 0, 16, 10, 3, 16, 10, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(data_X[:10])\n",
    "print(data_Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. Полносвязная сеть\n",
    "Самой простой моделью является обычный перцептрон. На вход сетки будем подавать просто эмдеддинг каждого слова, на выходе ожидать распредедение вероятностей по тегам. В качестве фреймворка достаточно будет использовать keras и его Sequential модель (https://keras.io/models/sequential/), в которую слои добавляются последовательно, с помощью метода `add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:42:33.223250Z",
     "start_time": "2023-03-21T15:42:26.262447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 18:42:26.317511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:42:33.387775Z",
     "start_time": "2023-03-21T15:42:33.225907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 50)             4944100   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               5100      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17)                1717      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 17)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,950,917\n",
      "Trainable params: 6,817\n",
      "Non-trainable params: 4,944,100\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 18:42:33.240637: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# на самом деле на вход сетки будет добавляться индекс слова, а слой эмбеддинга будет возвращать для него вектор\n",
    "model.add(Embedding(input_length=1, input_dim=len(word_to_index), output_dim=50, embeddings_initializer='random_uniform',\n",
    "                    trainable=False)) # матрицу эмбеддингов просто инициализируем нормальным распределением и отключим обучение\n",
    "# далее нам нужно схлопнуть трехмерный тензор с одной фиктивной размерностью в двумерный\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100)) # основной полносвязный слой\n",
    "model.add(Activation('relu')) # для приличия добавим функцию активации\n",
    "model.add(Dense(len(pos_to_index))) # выходной слой тоже полносвязный размерности по кол-ву тегов\n",
    "model.add(Activation('softmax')) # ну и в конце делаем softmax, чтобы получить распределение\n",
    "model.summary() # вывод получившейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:42:34.147826Z",
     "start_time": "2023-03-21T15:42:34.130317Z"
    }
   },
   "outputs": [],
   "source": [
    "# компилируем модель\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:42:39.993908Z",
     "start_time": "2023-03-21T15:42:35.246067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3324/3324 [==============================] - 4s 1ms/step - loss: 1.4256 - accuracy: 0.5411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1ed2f8400>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# и обучаем\n",
    "model.fit(np.array(data_X), np.array(data_Y), epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка обученности модели остается за вами. Этот пример лишь для того, чтобы показать как собрать сетку и скормить ей данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 5. Рекуррентая сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее рассмотрим более приближенную к SOTA модель. Ей является рекуррентая сеть, которая принимает эмбеддинги слов в предложении и генерирует для них распределение вероятностей. Основным отличием от прошлой в том, что теперь мы будем использовать соседние слова как раз за счет рекуррентого слоя. Для этой модели мы уже будем использовать функциональный способ задания модели все того же кераса (https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:45:20.046774Z",
     "start_time": "2023-03-21T15:45:20.042591Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, TimeDistributed,Bidirectional, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:45:22.157081Z",
     "start_time": "2023-03-21T15:45:21.628417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 50)          4944100   \n",
      "                                                                 \n",
      " blstm (Bidirectional)       (None, None, 200)         120800    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 17)         3417      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,068,317\n",
      "Trainable params: 124,217\n",
      "Non-trainable params: 4,944,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# В начале задается входной слой, в котором мы укажем входную размерность. \n",
    "# Это будет None, т.к. мы заранее не знаем, какой будет длина каждого предложения \n",
    "input_layer = Input(shape=(None,), name='input')\n",
    "# Далее идет все тот же слой эмеддинга, которому мы на вход подаем предыдущий слой (в этом и суть functional APO)\n",
    "embeddings_layer = Embedding(input_dim=len(word_to_index), output_dim=50, \n",
    "                             trainable=False, embeddings_initializer='random_uniform',\n",
    "                             name='embedding')(input_layer)\n",
    "# Итак, основным слоем здесь будет двусторонний LSTM, который будет возвращать вектор для каждого слова (return_sequences=True) \n",
    "blstm_layer = Bidirectional(LSTM(100, return_sequences=True), name='blstm')(embeddings_layer)\n",
    "# Аналогично т.к. у нас здесь вектора для каждого слоя, то и полносвязный слой должен применяться для каждого слоя \n",
    "# по-отдельности. Поэтому полносвязный слой оборачивается в  TimeDistributed\n",
    "result_layer = TimeDistributed(Dense(len(pos_to_index), activation='softmax', name='result'))(blstm_layer)\n",
    "# собственно определяем модель входными и выходными слоями\n",
    "model = Model(inputs=[input_layer], outputs=result_layer)\n",
    "# компилируем\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# выводим архитектуру\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам нужно было бы распределить слова по предложениям, распределить по группам по длине, выравнить предложения по длине в одной групе, заполнив недостающие слова паддингами. Но это довольно неприятный процесс, а мне просто хочется запустить сетку и проверить, что она вообще работает, что сошлись все разверности. Поэтому просто раскидаем по 10 слов с помощью `numpy.reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:45:27.646745Z",
     "start_time": "2023-03-21T15:45:27.504763Z"
    }
   },
   "outputs": [],
   "source": [
    "rnnX = np.reshape(data_X[:850000], (-1,10))\n",
    "rnnY = np.reshape(data_Y[:850000], (-1,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:45:27.988275Z",
     "start_time": "2023-03-21T15:45:27.982235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85000, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(rnnX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и проверим, что оно вообще работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T15:45:42.293228Z",
     "start_time": "2023-03-21T15:45:29.419936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/333 [==============================] - 13s 30ms/step - loss: 2.0799 - accuracy: 0.3569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1ee788f40>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(rnnX, rnnY, epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 6. Задание\n",
    "В качестве упражения предлагается довести до ума обучения второй модели: распределить слова по предложениям, написать тестирование модели и собственно посмотреть как оно обучилось. Тестировать предлагаю на последней 1000 предложений, обучать - на остальном. Кто уверен в своих желаниях, то может решить оригинальную задачу: предсказывать также грамматические категории. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T16:15:13.305050Z",
     "start_time": "2023-03-21T16:15:09.310605Z"
    }
   },
   "outputs": [],
   "source": [
    "# для полносвязной сетки просто захреначим все в один список\n",
    "data_X_sent = []\n",
    "data_Y_sent = []\n",
    "\n",
    "max_len = max(len(sent) for sent in train)\n",
    "none_pos = max(pos_to_index.values())+1\n",
    "\n",
    "for sent in train:\n",
    "    word_index_list = [word_to_index[word_i.word.lower()] for word_i in sent]\n",
    "    pos_index_list = [pos_to_index[word_i.pos] for word_i in sent]\n",
    "    \n",
    "    while len(word_index_list) < max_len:\n",
    "        word_index_list.append(word2idx[\"PADDING_TOKEN\"])\n",
    "        pos_index_list.append(none_pos)\n",
    "        \n",
    "    data_X_sent.append( word_index_list )\n",
    "    data_Y_sent.append( pos_index_list )\n",
    "    \n",
    "data_X_sent = np.array(data_X_sent)\n",
    "data_Y_sent = np.reshape(data_Y_sent, (-1,max_len,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T16:14:42.822492Z",
     "start_time": "2023-03-21T16:14:42.319844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 50)          4944100   \n",
      "                                                                 \n",
      " blstm (Bidirectional)       (None, None, 200)         120800    \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, None, 18)         3618      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,068,518\n",
      "Trainable params: 124,418\n",
      "Non-trainable params: 4,944,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# В начале задается входной слой, в котором мы укажем входную размерность. \n",
    "# Это будет None, т.к. мы заранее не знаем, какой будет длина каждого предложения \n",
    "input_layer = Input(shape=(None,), name='input')\n",
    "# Далее идет все тот же слой эмеддинга, которому мы на вход подаем предыдущий слой (в этом и суть functional APO)\n",
    "embeddings_layer = Embedding(input_dim=len(word_to_index), output_dim=50, \n",
    "                             trainable=False, embeddings_initializer='random_uniform',\n",
    "                             name='embedding')(input_layer)\n",
    "# Итак, основным слоем здесь будет двусторонний LSTM, который будет возвращать вектор для каждого слова (return_sequences=True) \n",
    "blstm_layer = Bidirectional(LSTM(100, return_sequences=True), name='blstm')(embeddings_layer)\n",
    "# Аналогично т.к. у нас здесь вектора для каждого слоя, то и полносвязный слой должен применяться для каждого слоя \n",
    "# по-отдельности. Поэтому полносвязный слой оборачивается в  TimeDistributed\n",
    "result_layer = TimeDistributed(Dense(len(pos_to_index)+1, activation='softmax', name='result'))(blstm_layer)\n",
    "# собственно определяем модель входными и выходными слоями\n",
    "model = Model(inputs=[input_layer], outputs=result_layer)\n",
    "# компилируем\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "# выводим архитектуру\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T16:16:55.605959Z",
     "start_time": "2023-03-21T16:16:55.601279Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test = data_X_sent[:-1000], data_X_sent[-1000:]\n",
    "y_train, y_test = data_Y_sent[:-1000], data_Y_sent[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T16:19:25.091337Z",
     "start_time": "2023-03-21T16:17:20.357551Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 125s 659ms/step - loss: 0.4029 - accuracy: 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa171e0d130>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T16:20:09.449335Z",
     "start_time": "2023-03-21T16:20:08.470064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 216ms/step - loss: 0.2202 - accuracy: 0.9310\n",
      "test loss, test acc: [0.22021479904651642, 0.9309853911399841]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=256)\n",
    "print('test loss, test acc:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

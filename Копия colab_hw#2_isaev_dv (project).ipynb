{"cells":[{"cell_type":"markdown","metadata":{"id":"picn6KMJpRs7"},"source":["# task\n","Участие в соревновании https://github.com/dialogue-evaluation/RuREBus по NER и RE.\n","Форма сдачи: ноутбук с кодом и метриками качества ваших моделей.\n","Оцениваение: минимально в 2 раза больше обычного дз (если сделаете совсем много, возможны дополнительные бонусы).\n","На чуть больше половины баллов за проект достаточно правильно сделать CharCNN-BLSTM-CRF для NER и вариант модели Miwa & Bansal (2016) для RE.\n","Дедлайн 1 мая 9 утра (4 недели)\n","\n","описание разметки текста: https://github.com/dialogue-evaluation/RuREBus/blob/master/markup_instruction.pdf"]},{"cell_type":"markdown","metadata":{"id":"M2C18S3m-Kaz"},"source":["# libs"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":645,"status":"ok","timestamp":1683028593468,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"e1485YJR-4aA","outputId":"ab16e409-51c6-4a68-d2e0-e287e5a4f8d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}],"source":["%%writefile requirements.txt\n","razdel\n","pytorch-crf"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6034,"status":"ok","timestamp":1683028600220,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"BefWijZ-_CFY","outputId":"96a19e66-b359-4edd-e0fb-4b8f58469f2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting razdel (from -r requirements.txt (line 1))\n","  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n","Collecting pytorch-crf (from -r requirements.txt (line 2))\n","  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n","Installing collected packages: razdel, pytorch-crf\n","Successfully installed pytorch-crf-0.7.2 razdel-0.5.0\n"]}],"source":["!pip install --upgrade -r requirements.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1112,"status":"ok","timestamp":1683028601328,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"hZ31IF6r8k-3","outputId":"57131443-08e4-4773-c5da-695215ca93f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-05-02 11:56:40--  https://raw.githubusercontent.com/dialogue-evaluation/RuREBus/master/eval_scripts/brat_format.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5030 (4.9K) [text/plain]\n","Saving to: ‘brat_format.py’\n","\n","brat_format.py      100%[===================>]   4.91K  --.-KB/s    in 0s      \n","\n","2023-05-02 11:56:40 (68.6 MB/s) - ‘brat_format.py’ saved [5030/5030]\n","\n","--2023-05-02 11:56:40--  https://raw.githubusercontent.com/dialogue-evaluation/RuREBus/master/eval_scripts/evaluate_ners.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4017 (3.9K) [text/plain]\n","Saving to: ‘evaluate_ners.py’\n","\n","evaluate_ners.py    100%[===================>]   3.92K  --.-KB/s    in 0s      \n","\n","2023-05-02 11:56:40 (65.8 MB/s) - ‘evaluate_ners.py’ saved [4017/4017]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/dialogue-evaluation/RuREBus/master/eval_scripts/brat_format.py\n","!wget https://raw.githubusercontent.com/dialogue-evaluation/RuREBus/master/eval_scripts/evaluate_ners.py"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"YvEo3sBW-OAI","executionInfo":{"status":"ok","timestamp":1683028604745,"user_tz":-180,"elapsed":3422,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["from glob import glob\n","from razdel import sentenize, tokenize\n","from collections import namedtuple\n","from itertools import chain\n","import random\n","import numpy as np\n","import time\n","from tqdm import tqdm\n","\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","\n","from torchcrf import CRF #https://pytorch-crf.readthedocs.io/en/stable/\n","\n","from evaluate_ners import compute_precision_and_recall"]},{"cell_type":"markdown","metadata":{"id":"YMUChQkt0X8P"},"source":["# 1. dataset"]},{"cell_type":"markdown","metadata":{"id":"aAuIejsKRKkx"},"source":["## 1.1 download"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"RMiYezIrxduR","executionInfo":{"status":"ok","timestamp":1683028605954,"user_tz":-180,"elapsed":1219,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["#удаление файлов\n","for i in glob('*'):\n","  !rm -rf {i}\n","\n","# # чек типа файла\n","# !apt install file\n","# !file RuREBus.git\n","\n","#создание папки\n","!mkdir train_data\n","!mkdir test_data"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3916,"status":"ok","timestamp":1683028609852,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"TxSljguIxTis","outputId":"758d6243-147b-4de2-91e0-059bb3069747"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'RuREBus'...\n","remote: Enumerating objects: 247, done.\u001b[K\n","remote: Counting objects: 100% (30/30), done.\u001b[K\n","remote: Compressing objects: 100% (29/29), done.\u001b[K\n","remote: Total 247 (delta 16), reused 4 (delta 1), pack-reused 217\u001b[K\n","Receiving objects: 100% (247/247), 14.22 MiB | 6.72 MiB/s, done.\n","Resolving deltas: 100% (118/118), done.\n"]}],"source":["!git clone https://github.com/dialogue-evaluation/RuREBus.git"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1683028610369,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"Ely8jZVFpMFR","outputId":"82dffd01-7f1d-451b-8910-d6c1a28ccdd8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['train_data/train_part_2',\n"," 'train_data/train_part_1',\n"," 'train_data/train_part_3']"]},"metadata":{},"execution_count":7}],"source":["for i in glob('RuREBus/train_data/*'):\n","  !unzip \"{i}\" -d \"train_data\"\n","\n","for i in glob('RuREBus/test_data/test_full*'):\n","  !unzip \"{i}\" -d \"test_data\"\n","\n","from IPython.display import clear_output\n","clear_output()\n","\n","glob('train_data/*')"]},{"cell_type":"markdown","metadata":{"id":"5l7JSONX09m8"},"source":["## 1.2 preprocessing"]},{"cell_type":"markdown","metadata":{"id":"EE6VNAH6eHBl"},"source":["### 1.2.1 sentenize & tokenize"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"RZ7jW4HZ9tp_","executionInfo":{"status":"ok","timestamp":1683028610369,"user_tz":-180,"elapsed":3,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["def get_tokenized_dataset(path_to_files: str):\n","    \"\"\"\n","    take txt files and tokenize them by razdel liba \n","\n","    params: path_to_files - path to train files\n","    return: dict[path] = (file_sentences, file_tokenize)\n","            file_sentences - [ Substring(start, stop, sentence),...]\n","            file_tokenize  - [ Substring(start, stop, token),...]\n","    \"\"\"\n","    result = {}\n","    for file_i in glob(path_to_files+'/*.txt'):\n","      short_file_i = file_i.replace(path_to_files+'/', '').replace('.txt','')\n","      file_txt = open(file_i, 'r', encoding='utf-8').read()\n","      file_sentences = list(sentenize(file_txt))\n","      file_tokenize = [[tok for tok in tokenize(sent.text)] for sent in file_sentences ]\n","      result[short_file_i] = (file_sentences, file_tokenize)\n","    return result"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":695,"status":"ok","timestamp":1683028611061,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"cba7YjveXIHd","outputId":"fc5b52c9-7aa3-4a0f-b6dc-e7d6814bb8ef"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{},"execution_count":9}],"source":["a = get_tokenized_dataset(\"train_data/train_part_1\")\n","len(a['20336241021100524345002_22_part_1'][0])"]},{"cell_type":"markdown","metadata":{"id":"xVCVMVlteUxv"},"source":["### 1.2.2 сущности (NER) и отношения (RE)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"pCzybuyyYKie","executionInfo":{"status":"ok","timestamp":1683028611061,"user_tz":-180,"elapsed":3,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["entuple = namedtuple('entuple', ['name', 'start', 'stop', 'text'])\n","retuple = namedtuple('retuple', ['name', 'arg1', 'arg2'])\n","\n","def get_annotated_dataset(path_to_files: str):\n","    \"\"\"\n","    take ann files and parse them\n","\n","    params: path_to_files - path to train files\n","    return: dict[path] = {'entity': entities_dict,\n","                          'relate': relations_dict}\n","            entities_dict  - {entity_id: ['name', 'start', 'stop', 'text']}\n","            relations_dict - {relate_id: ['name', 'arg1', 'arg2']}\n","                             arg1, arg2 taken from list of entity_id\n","    \"\"\"\n","    result = {}\n","    for file_i in glob(path_to_files+'/*.ann'):\n","        short_file_i = file_i.replace(path_to_files+'/', '').replace('.ann','')\n","\n","        file_txt_list = open(file_i, 'r', encoding='utf-8').readlines()\n","        file_txt_list = [i.strip().split('\\t') for i in file_txt_list]\n","\n","        if len(file_txt_list) > 0:\n","            # сущности\n","            entities = [i for i in file_txt_list if i[0][0]=='T']\n","            entities = [(i[0], i[1], ' '.join(i[2:])) if len(i)>3 else i for i in entities] #на случай если в тексте была табуляция\n","            assert len(entities) == len(set([i[0] for i in entities])), 'allert ent #1: имена сущностей (T***) не уникальны'\n","            assert min([len(i[1].split()) for i in entities]) == max([len(i[1].split()) for i in entities]), 'allert ent #2: что-то не так в определении сущности'\n","\n","            entities_dict = dict( [(tag, entuple(attr.split()[0], int(attr.split()[1]), int(attr.split()[2]), text))\n","                                      for (tag, attr, text) in entities] )\n","            # отношения\n","            relations = [i for i in file_txt_list if i[0][0]=='R']\n","            assert len(relations) == len(set([i[0] for i in relations])), 'allert rel #1: имена сущностей (R***) не уникальны'\n","            relations_dict = dict( [(tag, retuple(attr.split()[0], attr.split()[1].split(':')[1], attr.split()[2].split(':')[1]))\n","                                      for (tag, attr) in relations] )\n","            assert len( set( [i.arg1 for i in relations_dict.values()] + [i.arg2 for i in relations_dict.values()] ) - set(entities_dict.keys()) ) == 0, \"allert rel #2: встречены сущности не встречающиеся в entities\"\n","\n","            result[short_file_i] = {'entity': entities_dict,\n","                                    'relate': relations_dict}\n","    return result"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":967,"status":"ok","timestamp":1683028612026,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"dt6UvU0y_ysx","outputId":"44d0afd0-7edd-46a4-b878-b4a9fe5a3974"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["132"]},"metadata":{},"execution_count":11}],"source":["b = get_annotated_dataset('train_data/train_part_1')\n","len(b['31339011021101006981005_6_part_0']['entity'])"]},{"cell_type":"markdown","metadata":{"id":"KO0V-9muedue"},"source":["### 1.2.3 сущности текущего текста"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"nYoCB1wUYO1e","executionInfo":{"status":"ok","timestamp":1683028612027,"user_tz":-180,"elapsed":16,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["def get_entities_from_text(sent_start, sent_stop, ent_dict):\n","    \"\"\"\n","    filtering entities_dict taking information about the boundaries of the sentence\n","\n","    params: sent_start - start of sentence\n","            sent_stop  - end of sentence\n","            ent_dict - dictionary of entities\n","    return: entities_dict = {entity_id: ['name', 'start', 'stop', 'text']}\n","    \"\"\"\n","    entity_filter_list = [ entity_id for entity_id, (_, entity_start, entity_stop, _) in ent_dict['entity'].items()\n","                      if sent_start <= entity_start <= entity_stop <= sent_stop ]\n","    result = {'entity': {entity_id: ent_dict['entity'][entity_id] for entity_id in entity_filter_list}}\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"yOYlN2ONekKK"},"source":["### 1.2.4 словари сущностей и отношений"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1683028612027,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"RZ-P0EzGYrs_","outputId":"4bb19326-c6b4-4e49-960c-ef91d2745a00"},"outputs":[{"output_type":"stream","name":"stdout","text":["entity dictionary:\n","{'B-CMP': 1, 'I-CMP': 2, 'B-INST': 3, 'I-INST': 4, 'B-QUA': 5, 'I-QUA': 6, 'B-SOC': 7, 'I-SOC': 8, 'B-ACT': 9, 'I-ACT': 10, 'B-BIN': 11, 'I-BIN': 12, 'B-ECO': 13, 'I-ECO': 14, 'B-MET': 15, 'I-MET': 16, 'O': 0}\n","\n","relate dictionary:\n","{'GOL': 0, 'PNG': 1, 'FNG': 2, 'TSK': 3, 'PNT': 4, 'NNT': 5, 'FNT': 6, 'NPS': 7, 'NNG': 8, 'PPS': 9, 'FPS': 10}\n"]}],"source":["entity_set = set()\n","relate_set = set()\n","for path_to_files in ['train_data/train_part_1', 'train_data/train_part_2', 'train_data/train_part_3']:\n","    annotated_dataset_i = get_annotated_dataset(path_to_files)\n","    for file_i in annotated_dataset_i:\n","      for _, (ent_name,_,_,_) in annotated_dataset_i[file_i]['entity'].items():\n","          entity_set = entity_set.union(set([ent_name]))\n","      for _, (rel_name,_,_) in annotated_dataset_i[file_i]['relate'].items():\n","          relate_set = relate_set.union(set([rel_name]))\n","\n","entity_list = [['B-'+x, 'I-'+x] for x in entity_set]\n","entity_list = list(chain(*entity_list))\n","\n","entity_dict = {entity_list[i]: i+1 for i in range(len(entity_list))}\n","entity_dict['O'] = 0\n","\n","rel_2_id = {y:x for x,y in enumerate(relate_set)}\n","id_2_rel = {x:y for x,y in enumerate(relate_set)}\n","\n","print('entity dictionary:')\n","print(entity_dict)\n","\n","print('\\nrelate dictionary:')\n","print(rel_2_id)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"EbuJ6ukTAaAv","executionInfo":{"status":"ok","timestamp":1683028612028,"user_tz":-180,"elapsed":7,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["# # в качестве доп инфо для RE\n","# ner_dict = {y:x for x,y in enumerate(entity_set)}\n","# ner_dict"]},{"cell_type":"markdown","metadata":{"id":"GX9pjM6YPbMV"},"source":["# 2. NER"]},{"cell_type":"markdown","metadata":{"id":"YpgEpA-1onUL"},"source":["## 2.1 BIO разметка\n","B - begin, I - inner, O - outer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWJgvGANUlqu"},"outputs":[],"source":["Sample = namedtuple(\"Sample\", \"text, tokens, labels\")\n","\n","def get_ner_dataset(path_file_list: list):\n","    samples = []\n","    for path_to_files in path_file_list:\n","\n","        # path_to_files = 'train_data/train_part_1'\n","\n","        tokenized_dataset = get_tokenized_dataset(path_to_files)\n","        annotated_dataset = get_annotated_dataset(path_to_files)\n","\n","        for file_name in tokenized_dataset:\n","            #print(file_name)\n","            sentences, tokenized_sentences = tokenized_dataset[file_name] #[ Substring(start, stop, sentence),...]\n","            try:\n","                annotations = annotated_dataset[file_name]                    #[ Substring(start, stop, token),...]\n","            except KeyError:\n","                continue\n","\n","            for sent_id, (sentence, tokenized_sentence) in enumerate(zip(sentences, tokenized_sentences)):\n","                entities_from_sentence = get_entities_from_text(sentence.start, sentence.stop, annotations) #{entity_id: ['name', 'start', 'stop', 'text']}\n","                tokens = [token.text for token in tokenized_sentence]\n","\n","                if len(entities_from_sentence['entity'])==0:\n","                    labels = [entity_dict['O']]*len(tokenized_sentence)\n","                    # labels = [entity_dict[i] for i in labels]\n","                else:\n","                    labels = []\n","                    for token in tokenized_sentence:\n","                      label = entity_dict['O']\n","                      for _, (ent_name,ent_start,ent_end,_) in entities_from_sentence['entity'].items(): #{entity_id: ['name', 'start', 'stop', 'text']}\n","                          if sentence.start+token.start == ent_start:\n","                              label = 'B-'+ent_name\n","                              label = entity_dict.get(label, 'O')\n","                          elif sentence.start+token.start > ent_start and sentence.start+token.stop <= ent_end:\n","                              label = 'I-'+ent_name\n","                              label = entity_dict.get(label, 'O')\n","                      labels.append(label)\n","\n","                sample = Sample(sentence.text, tokens, labels)\n","                samples.append(sample)\n","\n","    return samples"]},{"cell_type":"markdown","metadata":{"id":"D-yxSdOVg-JV"},"source":["## 2.2 train-test split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syWxUn1hgh0f"},"outputs":[],"source":["data = get_ner_dataset(glob('train_data/*'))\n","random.shuffle(data)\n","\n","train_size = int(len(data)*0.8)\n","train = data[:train_size]\n","val = data[train_size:]\n","test = get_ner_dataset(['test_data'])"]},{"cell_type":"markdown","metadata":{"id":"sj6w5Ghss9yC"},"source":["множество слов и символов"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682892107052,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"FNwQmKUMqnly","outputId":"5d29612c-006d-4f75-f4a4-ace6384f1864"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{},"execution_count":22}],"source":["char_set = list( {ch for sample in train for token in sample.tokens for ch in token} )\n","char_set.insert(0, '<unk>'), char_set.insert(0, '<pad>')\n","word_set = list( {token for sample in train for token in sample.tokens} )\n","word_set.insert(0, '<unk>'), word_set.insert(0, '<pad>')"]},{"cell_type":"markdown","metadata":{"id":"e3kabRoirqDw"},"source":["## 2.3 генерация батчей"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8iT9MOt_ZtcN"},"outputs":[],"source":["# max_seq_len=500\n","# max_char_seq_len=40\n","# batch_size=32\n","def get_next_gen_batch(samples, max_seq_len=512, max_char_seq_len=40, batch_size=2):\n","    indices = np.arange(len(train))\n","    np.random.shuffle(indices)\n","    batch_begin = 0\n","    with tqdm(total=len(train)) as pbar:\n","      while batch_begin < len(train):\n","          batch_indices = indices[batch_begin: batch_begin + batch_size]\n","          batch_words = []\n","          batch_chars = []\n","          batch_labels = []\n","          batch_max_len = 0\n","          batch_masks = [] #for CRF\n","          for data_ind in batch_indices:\n","              sample = train[data_ind] #беру одно предложение\n","              words = torch.zeros(max_seq_len, dtype=torch.long).cuda()\n","              inputs = torch.zeros((max_seq_len, max_char_seq_len), dtype=torch.long).cuda()\n","              for token_num, token in enumerate(sample.tokens[:max_seq_len]): #цикл по токенам предложения, обрезанного до max_seq_len\n","                  #слова\n","                  words[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n","                  #символы\n","                  for char_num, char in enumerate(token[:max_char_seq_len]):\n","                      inputs[token_num][char_num] = char_set.index(char) if char in char_set else char_set.index('<unk>')\n","              labels = sample.labels[:max_seq_len]         #аналогично с labels\n","              masks = [1]*len(labels) + [0]*(max_seq_len - len(labels)) \n","              labels += [0] * (max_seq_len - len(labels))  #аналогично с labels\n","              \n","              batch_words.append(words)\n","              batch_chars.append(inputs)\n","              batch_labels.append(labels)\n","              batch_masks.append(masks)\n","              \n","          batch_begin += batch_size\n","          pbar.update(batch_size)\n","          \n","          batch_words = torch.stack(batch_words)\n","          batch_chars = torch.stack(batch_chars)\n","          labels = torch.cuda.LongTensor(batch_labels)\n","          batch_masks = torch.tensor(batch_masks).cuda()>0\n","          yield batch_indices, batch_words, batch_chars, labels, batch_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682892107053,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"86J8pN65nng3","outputId":"7573c1ff-55ec-45d0-83b4-7bcfd12d1abb"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 2/8368 [00:00<21:48,  6.39it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","torch.Size([2, 512])\n","torch.Size([2, 512, 40])\n","torch.Size([2, 512])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["for _, batch_words, batch_chars, labels, _ in get_next_gen_batch(train):\n","    1+1\n","    break\n","print()\n","print( batch_words.size() )\n","print( batch_chars.size() )\n","print( labels.size() )"]},{"cell_type":"markdown","metadata":{"id":"BYadOzdKxi4D"},"source":["## 2.4 модели"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":376},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682892107053,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"ev8vBN9WY6GZ","outputId":"6f568047-4f4f-401b-e507-df181802446c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<img src='https://user-images.githubusercontent.com/31881382/35037901-f4a69b14-fb89-11e7-8c28-08a0cb1310cb.png' />\n"]},"metadata":{}}],"source":["%%html\n","<img src='https://user-images.githubusercontent.com/31881382/35037901-f4a69b14-fb89-11e7-8c28-08a0cb1310cb.png' />"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gz_48-oPxnCT"},"outputs":[],"source":["class SuperSimpleModel(nn.Module):\n","    def __init__(self, char_set_size, char_embedding_dim=16, classes_count=len(entity_dict), char_max_seq_len=40):\n","        super().__init__()\n","        \n","        self.embeddings_layer = nn.Embedding(char_set_size, char_embedding_dim)\n","        self.out_layer = nn.Linear(char_max_seq_len * char_embedding_dim, classes_count)\n","\n","    def forward(self, words, chars):\n","        projections = self.embeddings_layer(chars)\n","        projections = projections.reshape(projections.size(0), projections.size(1), -1)\n","        output = self.out_layer(projections)\n","        return output\n","\n","class CharCNNBiLSTM(nn.Module):\n","    def __init__(self, char_set_size, word_set_size,\n","                 char_embedding_dim=8, word_embedding_dim=64,\n","                 classes_count=len(entity_dict), \n","                 char_max_seq_len=40, lstm_embedding_dim=16,\n","                 kernel_size=3, max_seq_len=512 ):\n","        super().__init__()\n","        self.char_max_seq_len = char_max_seq_len\n","        self.char_embedding_dim = char_embedding_dim\n","        self.max_seq_len = max_seq_len\n","\n","        self.char_embedding = nn.Embedding(char_set_size, char_embedding_dim)\n","        self.char_cnn = nn.Conv1d(in_channels=char_embedding_dim, out_channels=1, kernel_size=kernel_size)\n","        self.word_embedding = nn.Embedding(word_set_size, word_embedding_dim  )\n","        self.dropout = nn.Dropout(0.2)\n","        self.lstm_layer = nn.LSTM((char_max_seq_len-kernel_size+1)+word_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n","        self.out_layer = nn.Linear(lstm_embedding_dim*2, classes_count)\n","\n","    def forward(self, words, chars):\n","        #работа с символами\n","        c_emb = self.char_embedding(chars)\n","        c_emb = c_emb.reshape(c_emb.size(0)*c_emb.size(1), self.char_max_seq_len, self.char_embedding_dim)\n","        c_emb = c_emb.permute(0,2,1)\n","\n","        c_cnn = self.char_cnn(c_emb)\n","        c_cnn = c_cnn.reshape(chars.size(0), self.max_seq_len, -1)\n","\n","        #работа со словами\n","        w_emb = self.word_embedding(words)\n","        com_emb = torch.cat((c_cnn, w_emb), 2)\n","        rnn_output, _ = self.lstm_layer(com_emb)\n","        result = self.out_layer.forward(rnn_output)\n","\n","        return result"]},{"cell_type":"markdown","metadata":{"id":"UVq_ZFVDzGyo"},"source":["один проход - отладка"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyAqLRr5T-9M"},"outputs":[],"source":["model = CharCNNBiLSTM(len(char_set), len(word_set))\n","\n","lr=0.01\n","device_name=\"cuda\"\n","device = torch.device(device_name)\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","loss_function = nn.CrossEntropyLoss().cuda()\n","model = model.to(device)\n","\n","logits = model(batch_words, batch_chars) # Прямой проход\n","logits = logits.transpose(1, 2) # crossentropy require (N,C,d,d,d,d,d), N-batch size, C-number of classes\n","loss = loss_function(logits, labels) # Подсчёт ошибки\n","loss.backward() # Подсчёт градиентов dL/dw\n","optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n","optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации"]},{"cell_type":"markdown","metadata":{"id":"nQOivaaoxnOD"},"source":["\n","\n","## 2.5 ф-ция обучения"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pN2_dMMeqrrv"},"outputs":[],"source":["def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n","                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cuda\",\n","                    early_stopping=True):\n","    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params: {}\".format(params_count))\n","    device = torch.device(device_name)\n","    model = model.to(device)\n","    total_loss = 0\n","    start_time = time.time()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_function = nn.CrossEntropyLoss().cuda()\n","    prev_avg_val_loss = None\n","    for epoch in range(epochs_count):\n","        model.train()\n","        for step, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(train)):\n","            logits = model(batch_words, batch_chars) # Прямой проход\n","            logits = logits.transpose(1, 2)\n","            loss = loss_function(logits, batch_labels) # Подсчёт ошибки\n","            loss.backward() # Подсчёт градиентов dL/dw\n","            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n","            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n","            total_loss += loss.item()\n","        val_total_loss = 0\n","        val_batch_count = 0\n","        model.eval()\n","        for _, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(val)):\n","            logits = model(batch_words, batch_chars) # Прямой проход\n","            logits = logits.transpose(1, 2)\n","            val_total_loss += loss_function(logits, batch_labels) # Подсчёт ошибки\n","            val_batch_count += 1\n","        avg_val_loss = val_total_loss/val_batch_count\n","        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n","        total_loss = 0\n","        start_time = time.time()\n","\n","        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n","            model.load_state_dict(torch.load(save_path))\n","            model.eval()\n","            break\n","        prev_avg_val_loss = avg_val_loss\n","        torch.save(model.state_dict(), save_path)"]},{"cell_type":"markdown","metadata":{"id":"OHOqFoTnYf1h"},"source":["## 2.6 метрики"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJmJwa83Qfxi"},"outputs":[],"source":["def predict(model, samples):\n","    model.eval()\n","    tp, fp, fn = 0,0,0\n","    for _, (indices, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(samples)):\n","        logits = model(batch_words, batch_chars)\n","        plabels = logits.max(dim=2)[1]\n","        tp += len([(i,j) for i,j in zip(batch_labels[0].tolist(), plabels[0].tolist()) if (j!=0) and (i==j)])\n","        fp += len([(i,j) for i,j in zip(batch_labels[0].tolist(), plabels[0].tolist()) if (j!=0) and (i!=j)])\n","        fn += len([(i,j) for i,j in zip(batch_labels[0].tolist(), plabels[0].tolist()) if (j==0) and (i!=j)])\n","\n","    recall, precision = compute_precision_and_recall(tp, fp, fn)\n","    try:\n","      f_measure = 2 * precision * recall / (precision + recall)\n","    except ZeroDivisionError:\n","      f_measure = 'None'\n","    print('precision:', precision)\n","    print('recall:   ', recall)\n","    print('f1:       ', f_measure)"]},{"cell_type":"markdown","metadata":{"id":"1q31aU6azl8z"},"source":["## --SimpleModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":461081,"status":"ok","timestamp":1682682699984,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"bt_q14aCslrM","outputId":"bb9ceb6a-41b6-46bc-ad51-b75854018a0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable params: 13681\n","Epoch = 0, Avg Train Loss = 0.4852, Avg val loss = 0.0956, Time = 460.83s\n"]}],"source":["model = SuperSimpleModel(len(char_set))\n","train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":684603,"status":"ok","timestamp":1682683392775,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"EmnWPH9sAupk","outputId":"2fada618-41e9-42e4-e5d9-29636ace3acb"},"outputs":[{"name":"stdout","output_type":"stream","text":["------on train dataset:-------\n","precision: 0.38149556400506973\n","recall:    0.09480314960629921\n","f1:        0.1518668012108981\n","\n"," -------on val dataset:--------\n","precision: 0.3604060913705584\n","recall:    0.0849536344600658\n","f1:        0.13749697409828127\n","\n"," ------on train dataset:-------\n","precision: 0.3703271028037383\n","recall:    0.09559710494571773\n","f1:        0.15196548418024927\n"]}],"source":["print('on train dataset:'.center(30, '-'))\n","predict(model, train)\n","\n","print('\\n', 'on val dataset:'.center(30, '-'))\n","predict(model, val)\n","\n","print('\\n', 'on test dataset:'.center(30, '-'))\n","predict(model, test)"]},{"cell_type":"markdown","metadata":{"id":"hx2iHZMRMzXW"},"source":["## --CharCNN + BiLSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":488640,"status":"ok","timestamp":1682860099785,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"Uaq7sKleM1l_","outputId":"ed57cabe-7a91-49c1-8d42-3ffcffdfd2a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Trainable params: 1537706\n","Epoch = 0, Avg Train Loss = 0.2857, Avg val loss = 0.0759, Time = 488.59s\n"]}],"source":["model = CharCNNBiLSTM(len(char_set), len(word_set))\n","train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":770706,"status":"ok","timestamp":1682860877683,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"COeJijGQRhO-","outputId":"85e5e950-f1f0-4a0c-c9c0-ff78aa4aae91"},"outputs":[{"output_type":"stream","name":"stdout","text":["------on train dataset:-------\n","precision: 0.3448667044809983\n","recall:    0.26331745344304897\n","f1:        0.2986247544204323\n","\n"," -------on val dataset:--------\n","precision: 0.31992687385740404\n","recall:    0.21739130434782608\n","f1:        0.25887573964497046\n","\n"," ------on train dataset:-------\n","precision: 0.336644591611479\n","recall:    0.25459098497495825\n","f1:        0.2899239543726236\n"]}],"source":["print('on train dataset:'.center(30, '-'))\n","predict(model, train)\n","\n","print('\\n', 'on val dataset:'.center(30, '-'))\n","predict(model, val)\n","\n","print('\\n', 'on test dataset:'.center(30, '-'))\n","predict(model, test)"]},{"cell_type":"code","source":["# model.eval()\n","# for step, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(train)):\n","#             # print(batch)\n","#             logits = model(batch_words, batch_chars)\n","#             plabels = logits.max(dim=1)[1]\n","#             # print(logits)\n","#             # print(plabels)\n","#             # print()\n","#             # break"],"metadata":{"id":"BsjED-VL2EE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"691SUO-lzsyb"},"source":["## --CharCNN + BiLSTM + CRF"]},{"cell_type":"markdown","source":["#### без использования масок паддинга"],"metadata":{"id":"NEzsyZb5TkON"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSfpzwLA03eD"},"outputs":[],"source":["def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n","                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cuda\",\n","                    early_stopping=True):\n","    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params: {}\".format(params_count))\n","    device = torch.device(device_name)\n","    model = model.to(device)\n","    total_loss = 0\n","    start_time = time.time()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_function = CRF(len(entity_dict), batch_first=True).cuda()\n","    prev_avg_val_loss = None\n","    for epoch in range(epochs_count):\n","        model.train()\n","        for step, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(train)):\n","            logits = model(batch_words, batch_chars) # Прямой проход\n","            loss = -loss_function(logits, batch_labels) # Подсчёт ошибки\n","            loss.backward() # Подсчёт градиентов dL/dw\n","            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n","            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n","            total_loss += loss.item()\n","            print(loss.item())\n","        val_total_loss = 0\n","        val_batch_count = 0\n","        model.eval()\n","        for _, (_, batch_words, batch_chars, batch_labels, _) in enumerate(get_next_gen_batch(val)):\n","            logits = model(batch_words, batch_chars) # Прямой проход\n","            val_total_loss += -loss_function(logits, batch_labels) # Подсчёт ошибки\n","            val_batch_count += 1\n","        avg_val_loss = val_total_loss/val_batch_count\n","        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n","        total_loss = 0\n","        start_time = time.time()\n","\n","        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n","            model.load_state_dict(torch.load(save_path))\n","            model.eval()\n","            break\n","        prev_avg_val_loss = avg_val_loss\n","        torch.save(model.state_dict(), save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580010,"status":"ok","timestamp":1682680623626,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"YGCAKBeisti_","outputId":"5628ee53-4a24-48b2-d1f0-29aba3bc9622"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable params: 1574290\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n","  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch = 0, Avg Train Loss = 8280.9223, Avg val loss = 1504.9847, Time = 576.71s\n"]}],"source":["model = CharCNNBiLSTM(len(char_set), len(word_set))\n","train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":630948,"status":"ok","timestamp":1682681255535,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"HBXlOWTZWwre","outputId":"a56cf656-e716-4b83-f734-6d04297b5276"},"outputs":[{"name":"stdout","output_type":"stream","text":["------on train dataset:-------\n","precision: 0.26618705035971224\n","recall:    0.012701682114658427\n","f1:        0.024246395806028834\n","\n"," -------on val dataset:--------\n","precision: 0.22758620689655173\n","recall:    0.01071776550828191\n","f1:        0.020471464019851116\n","\n"," ------on train dataset:-------\n","precision: 0.23076923076923078\n","recall:    0.010361067503924647\n","f1:        0.019831730769230768\n"]}],"source":["print('on train dataset:'.center(30, '-'))\n","predict(model, train)\n","\n","print('\\n', 'on val dataset:'.center(30, '-'))\n","predict(model, val)\n","\n","print('\\n', 'on test dataset:'.center(30, '-'))\n","predict(model, test)"]},{"cell_type":"markdown","source":["#### с использованием масок паддинга"],"metadata":{"id":"9B9N8l1CTo0O"}},{"cell_type":"code","source":["def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n","                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cuda\",\n","                    early_stopping=True):\n","    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params: {}\".format(params_count))\n","    device = torch.device(device_name)\n","    model = model.to(device)\n","    total_loss = 0\n","    start_time = time.time()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_function = CRF(len(entity_dict), batch_first=True).cuda()\n","    prev_avg_val_loss = None\n","    for epoch in range(epochs_count):\n","        model.train()\n","        for step, (_, batch_words, batch_chars, batch_labels, batch_masks) in enumerate(get_next_gen_batch(train)):\n","            logits = model(batch_words, batch_chars) # Прямой проход\n","            loss = -loss_function(logits, batch_labels, batch_masks) # Подсчёт ошибки\n","            loss.backward() # Подсчёт градиентов dL/dw\n","            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n","            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n","            total_loss += loss.item()\n","        val_total_loss = 0\n","        val_batch_count = 0\n","        model.eval()\n","        for _, (_, batch_words, batch_chars, batch_labels, batch_masks) in enumerate(get_next_gen_batch(val)):\n","            logits = model(batch_words, batch_chars) # Прямой проход\n","            val_total_loss += -loss_function(logits, batch_labels, batch_masks) # Подсчёт ошибки\n","            val_batch_count += 1\n","        avg_val_loss = val_total_loss/val_batch_count\n","        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n","        total_loss = 0\n","        start_time = time.time()\n","\n","        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n","            model.load_state_dict(torch.load(save_path))\n","            model.eval()\n","            break\n","        prev_avg_val_loss = avg_val_loss\n","        torch.save(model.state_dict(), save_path)"],"metadata":{"id":"RBAkxVcHTrWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CharCNNBiLSTM(len(char_set), len(word_set))\n","train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"],"metadata":{"id":"IR3eTLR8VnpC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('on train dataset:'.center(30, '-'))\n","predict(model, train)\n","\n","print('\\n', 'on val dataset:'.center(30, '-'))\n","predict(model, val)\n","\n","print('\\n', 'on test dataset:'.center(30, '-'))\n","predict(model, test)"],"metadata":{"id":"iHLXS0CXdimf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LgCsRt6uBGtY"},"source":["# 3. RE with NEs"]},{"cell_type":"markdown","source":["постановка задачи: на основе текста и выделенных в нем сущностей спрогнозировать связь между ними"],"metadata":{"id":"0eh5zeKGSwhd"}},{"cell_type":"markdown","metadata":{"id":"9cLEucczHTyQ"},"source":["## 3.1 разметка"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"6HImgE8sBFwT","executionInfo":{"status":"ok","timestamp":1683028613285,"user_tz":-180,"elapsed":6,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["Sample = namedtuple(\"Sample\", \"text, tokenized_text, tokens, e1, e2, label\")\n","\n","def get_re_dataset(path_file_list: list):\n","    samples = []\n","    for path_to_files in path_file_list:\n","        # path_to_files = 'train_data/train_part_1'\n","\n","        tokenized_dataset = get_tokenized_dataset(path_to_files)\n","        annotated_dataset = get_annotated_dataset(path_to_files)\n","\n","        for file_name in tokenized_dataset:\n","            #print(file_name)\n","            sentences, tokenized_sentences = tokenized_dataset[file_name] #[ Substring(start, stop, sentence),...]\n","            try:\n","                annotations = annotated_dataset[file_name]                #[ Substring(start, stop, token),...]\n","            except KeyError:\n","                annotations = None\n","\n","            if annotations:\n","                for sent_id, (sentence, tokenized_sentence) in enumerate(zip(sentences, tokenized_sentences)):\n","                    entities_from_sentence = get_entities_from_text(sentence.start, sentence.stop, annotations) #{entity_id: ['name', 'start', 'stop', 'text']}\n","                \n","                    relates = [ i for i in annotations['relate'].values()\n","                                if  (i.arg1 in entities_from_sentence['entity'].keys()) \n","                                and (i.arg2 in entities_from_sentence['entity'].keys())]\n","\n","                    for rel_i in relates:\n","                        arg1 = rel_i.arg1\n","                        ent1 = entities_from_sentence['entity'][arg1]\n","                        arg2 = rel_i.arg2\n","                        ent2 = entities_from_sentence['entity'][arg2]\n","\n","                        tokens = [token.text for token in tokenized_sentence]\n","                        #e1 = (ner_dict[ent1.name], ent1.start-sentence.start, ent1.stop-sentence.start)\n","                        #e2 = (ner_dict[ent2.name], ent2.start-sentence.start, ent2.stop-sentence.start)\n","                        e1 = (ent1.start-sentence.start, ent1.stop-sentence.start)\n","                        e2 = (ent2.start-sentence.start, ent2.stop-sentence.start)\n","                        label = rel_2_id[rel_i.name]\n","\n","                        sample = Sample(sentence.text,\n","                                        [i.text for i in tokenized_sentence], \n","                                        tokens, e1, e2, label)\n","                        samples.append(sample)\n","    return samples"]},{"cell_type":"markdown","metadata":{"id":"0MLxjN4lrqW1"},"source":["## 3.2 train-test split"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"TSOei0bx29zT","executionInfo":{"status":"ok","timestamp":1683028624735,"user_tz":-180,"elapsed":11454,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["data = get_re_dataset(glob('train_data/*'))\n","random.shuffle(data)\n","\n","train_size = int(len(data)*0.8)\n","train = data[:train_size]\n","val = data[train_size:]\n","test = get_re_dataset(['test_data'])"]},{"cell_type":"markdown","metadata":{"id":"a_uNHseGZU4o"},"source":["множество слов и символов"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1683028625301,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"AU_Fc4EcZSBj","outputId":"b054264b-c900-4224-91f8-30c0eb3ee006"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{},"execution_count":17}],"source":["char_set = list( {ch for sample in train for token in sample.tokens for ch in token} )\n","char_set.insert(0, '<unk>'), char_set.insert(0, '<pad>')\n","word_set = list( {token for sample in train for token in sample.tokens} )\n","word_set.insert(0, '<unk>'), word_set.insert(0, '<pad>')"]},{"cell_type":"markdown","metadata":{"id":"K1dJNqUaHfur"},"source":["## 3.3 генерация батчей"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"FYKQs5RDDF--","executionInfo":{"status":"ok","timestamp":1683028625303,"user_tz":-180,"elapsed":7,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}}},"outputs":[],"source":["def get_next_gen_batch(samples, max_seq_len=1024, max_char_seq_len=40, batch_size=1):\n","    indices = np.arange(len(train))\n","    np.random.shuffle(indices)\n","    batch_begin = 0\n","    #with tqdm(total=len(train)) as pbar:\n","    while batch_begin < len(train):\n","          batch_indices = indices[batch_begin: batch_begin + batch_size]\n","          batch_chars = []\n","          batch_words = []\n","          ent_info = []\n","          batch_labels = []\n","          batch_max_len = 0\n","          for data_ind in batch_indices:\n","              sample = train[data_ind] #беру одно предложение\n","              words = torch.zeros(max_seq_len, dtype=torch.long).cuda()\n","              inputs = []\n","              for token_num, token in enumerate(sample.tokens[:max_seq_len]): #цикл по токенам предложения, обрезанного до max_seq_len\n","                  #слова\n","                  words[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n","                  #символы\n","                  chars = [char_set.index(ch) if ch in char_set else char_set.index(\"<unk>\") for ch in token][:max_char_seq_len] #максимальная символьная длина токена - max_char_seq_len\n","                  chars += [0] * (max_char_seq_len - len(chars)) #каждый токен должен быть представлен max_char_seq_len символами\n","                  inputs.append(chars)\n","              batch_max_len = max(batch_max_len, len(inputs)) #кол-во токенов в предложении\n","              inputs += [[0]*max_char_seq_len] * (max_seq_len - len(inputs)) #приводим предложение к длине max_seq_len\n","              \n","              # отношения взятые по символам\n","              # ent_info.append(((min(sample.e1[0], max_seq_len-1), min(sample.e1[1], max_seq_len-1)), \n","              #                 (min(sample.e2[0], max_seq_len-1), min(sample.e2[1], max_seq_len-1)))\n","              #                 )\n","\n","              # отношения взятые по словам\n","              e1 = (sample.e1[0], sample.e1[1])\n","              e2 = (sample.e2[0], sample.e2[1])\n","              \n","              # записываем наблюдение только если оно попало в наблюдаемое предложение (по максимальной длине предложения)\n","              if (e1[0] < max_seq_len) or (e2[0] < max_seq_len):\n","                try:\n","                  # сущность #1\n","                  e1_txt = [i.text for i in tokenize(sample.text[e1[0]:e1[1]])]\n","                  e1_start = sample.tokenized_text.index(e1_txt[0])\n","                  e1_stop = sample.tokenized_text.index(e1_txt[-1])\n","                  # сущность #2\n","                  e2_txt = [i.text for i in tokenize(sample.text[e2[0]:e2[1]])]\n","                  e2_start = sample.tokenized_text.index(e2_txt[0])\n","                  e2_stop = sample.tokenized_text.index(e2_txt[-1])\n","                  # запись наблюдений\n","                  ent_info.append(((e1_start, e1_stop),\n","                                  (e2_start, e2_stop)))\n","                  batch_chars.append(inputs)\n","                  batch_words.append(words)\n","                  batch_labels.append(sample.label)\n","                except ValueError:\n","                    continue\n","          batch_begin += batch_size\n","          #pbar.update(batch_size)\n","\n","          if len(batch_words)>0:\n","            batch_words = torch.stack(batch_words).cuda()\n","            batch_chars = torch.cuda.LongTensor(batch_chars)\n","            labels = torch.cuda.LongTensor(batch_labels)\n","\n","            yield batch_indices, batch_words, batch_chars, ent_info, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682892437975,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"ulPdlaxLM9oy","outputId":"12ec43a9-baaf-452b-a0b7-fa592972bf0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","torch.Size([2, 1024])\n","torch.Size([2, 1024, 40])\n","2\n","torch.Size([2])\n"]}],"source":["for _, batch_words, batch_chars, batch_ents, labels in get_next_gen_batch(train):\n","    1+1\n","    break\n","\n","print()\n","print( batch_words.size() )\n","print( batch_chars.size() )\n","print( len(batch_ents) )\n","print( labels.size() )"]},{"cell_type":"markdown","metadata":{"id":"3a3aPdHVMwgu"},"source":["## 3.4 модель"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pz3YIyu2Myei"},"outputs":[],"source":["class ReModel(nn.Module):\n","    def __init__(self, char_set_size, char_embedding_dim=16, classes_count=len(rel_2_id), char_max_seq_len=40, lstm_embedding_dim=16):\n","        super().__init__()\n","        \n","        self.embeddings_layer = nn.Embedding(char_set_size, char_embedding_dim)\n","        self.lstm_layer_1 = nn.LSTM(char_max_seq_len * char_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n","        self.lstm_layer_2 = nn.LSTM(char_max_seq_len * char_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n","        self.dropout = nn.Dropout(0.2)\n","        self.out_layer = nn.Linear(4*lstm_embedding_dim, classes_count)\n","\n","    def forward(self, inputs, ents):\n","        #print(ents[0])\n","\n","        # обработка токенов\n","        #print('inputs'.ljust(30), inputs.size())\n","        projections = self.embeddings_layer.forward(inputs)\n","        #print('projections'.ljust(30), projections.size())\n","        projections = projections.reshape(projections.size(0), projections.size(1), -1)\n","        #print('projections'.ljust(30), projections.size())\n","        _, (final_hidden_state, _) = self.lstm_layer_1(projections)\n","        rnn_output_1 = torch.cat((final_hidden_state[-2], final_hidden_state[-1]), dim=1)\n","        rnn_output_1 = self.dropout(rnn_output_1)\n","        #print('rnn_output_1'.ljust(30), rnn_output_1.size())\n","\n","        # обработка сущностей\n","        pr_e1_batch = []\n","        pr_e2_batch = []\n","        for (e1, e2) in ents:\n","          #сущность 1\n","          pr_e1 = projections[:,e1[0]:e1[1]+1,:]\n","          pr_e1 = pr_e1.mean(dim=1)\n","          pr_e1_batch.append(pr_e1)\n","          #сущность 2\n","          pr_e2 = projections[:,e2[0]:e2[1]+1,:]\n","          pr_e2 = pr_e2.mean(dim=1)\n","          pr_e2_batch.append(pr_e2)\n","        pr_e1 = torch.stack(pr_e1_batch)\n","        pr_e2 = torch.stack(pr_e2_batch)\n","        pr_cat = torch.cat([pr_e1, pr_e2], dim=1)\n","        #print('pr_cat'.ljust(30), pr_cat.size())\n","        #lstm над сущностями\n","        _, (final_hidden_state, _) = self.lstm_layer_2(pr_cat)\n","        rnn_output_2 = torch.cat((final_hidden_state[-2], final_hidden_state[-1]), dim=1)\n","        rnn_output_2 = self.dropout(rnn_output_2)\n","        #print('rnn_output_2'.ljust(30), rnn_output_2.size())\n","        #объединяем инфо\n","        com = torch.cat([rnn_output_1, rnn_output_2], dim=1)\n","        #print('com'.ljust(30), com.size())\n","        output = self.out_layer.forward(com)\n","        #print('output'.ljust(30), output.size())\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"n31uZ-CDMuaV"},"source":["## 3.5 ф-ция обучения модели"]},{"cell_type":"markdown","metadata":{"id":"hBj_v4SYn3Sa"},"source":["один проход - отладка"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfPTvrC-JFpN","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1682893791780,"user_tz":-180,"elapsed":552,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"}},"outputId":"0e226f6a-4189-48e1-fcfe-5795c1ae7f3c"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-77-e0b5833bc510>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ReModel.__init__() missing 1 required positional argument: 'word_set_size'"]}],"source":["model = ReModel(len(char_set))\n","\n","lr=0.01\n","device_name=\"cuda\"\n","device = torch.device(device_name)\n","\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","loss_function = nn.CrossEntropyLoss().cuda()\n","model = model.to(device)\n","\n","logits = model(batch, batch_ents) # Прямой проход\n","loss = loss_function(logits, labels) # Подсчёт ошибки\n","loss.backward() # Подсчёт градиентов dL/dw\n","optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n","optimizer.zero_grad() # Занул\\ение градиентов, чтобы их спокойно менять на следующей итерации"]},{"cell_type":"markdown","metadata":{"id":"1DZOE3qUn-Nr"},"source":["полный проход"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-Xx9MkXPBHl"},"outputs":[],"source":["def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n","                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cuda\",\n","                    early_stopping=True):\n","    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params: {}\".format(params_count))\n","    device = torch.device(device_name)\n","    model = model.to(device)\n","    total_loss = 0\n","    start_time = time.time()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_function = nn.CrossEntropyLoss().cuda()\n","    prev_avg_val_loss = None\n","    for epoch in range(epochs_count):\n","        model.train()\n","        for step, (_, batch_words, batch_chars, batch_ents, batch_labels) in enumerate(get_next_gen_batch(train)):\n","            if batch_words.size(0)>0:\n","              logits = model(batch_words, batch_chars, batch_ents) # Прямой проход\n","              loss = loss_function(logits, batch_labels) # Подсчёт ошибки\n","              loss.backward() # Подсчёт градиентов dL/dw\n","              optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n","              optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n","              total_loss += loss.item()\n","        val_total_loss = 0\n","        val_batch_count = 0\n","        model.eval()\n","        for _, (_, batch_words, batch_chars, batch_ents, batch_labels) in enumerate(get_next_gen_batch(val)):\n","            logits = model(batch_words, batch_chars, batch_ents) # Прямой проход\n","            val_total_loss += loss_function(logits, batch_labels) # Подсчёт ошибки\n","            val_batch_count += 1\n","        avg_val_loss = val_total_loss/val_batch_count\n","        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n","        total_loss = 0\n","        start_time = time.time()\n","\n","        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n","            model.load_state_dict(torch.load(save_path))\n","            model.eval()\n","            break\n","        prev_avg_val_loss = avg_val_loss\n","        torch.save(model.state_dict(), save_path)"]},{"cell_type":"markdown","metadata":{"id":"qeGxxZVFtZPT"},"source":["## 3.6 метрики"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ztw9qEuMeDqa"},"outputs":[],"source":["def predict(model, samples):\n","    model.eval()\n","    tp, fp, fn = 0,0,0\n","    for _, (_, batch, batch_ents, batch_labels) in enumerate(get_next_gen_batch(samples)):\n","            logits = model(batch, batch_ents)\n","            plabels = logits.max(dim=1)[1]\n","            tp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if i==j])\n","            fp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if i!=j])\n","            fn += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if i!=j])\n","\n","    recall, precision = compute_precision_and_recall(tp, fp, fn)\n","    try:\n","          f_measure = 2 * precision * recall / (precision + recall)\n","    except ZeroDivisionError:\n","          f_measure = 'None'\n","    print('precision:', precision)\n","    #print('recall:   ', recall)\n","    #print('f1:       ', f_measure)"]},{"cell_type":"markdown","metadata":{"id":"qUuf43RAeHPv"},"source":["## --ReModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1819791,"status":"ok","timestamp":1682441728789,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"GVvac5Gs1eQl","outputId":"fb791d46-8135-496d-c561-713c211e82f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trainable params: 172011\n","Epoch = 0, Avg Train Loss = 5.8837, Avg val loss = 1.7912, Time = 60.10s\n","Epoch = 1, Avg Train Loss = 5.8076, Avg val loss = 1.8443, Time = 60.61s\n","Epoch = 2, Avg Train Loss = 5.8544, Avg val loss = 1.9233, Time = 62.07s\n","Epoch = 3, Avg Train Loss = 5.8812, Avg val loss = 1.8286, Time = 62.12s\n","Epoch = 4, Avg Train Loss = 5.8330, Avg val loss = 1.8156, Time = 62.05s\n","Epoch = 5, Avg Train Loss = 5.8089, Avg val loss = 1.8610, Time = 61.22s\n","Epoch = 6, Avg Train Loss = 5.7973, Avg val loss = 1.8575, Time = 61.57s\n","Epoch = 7, Avg Train Loss = 5.8268, Avg val loss = 1.8676, Time = 62.06s\n","Epoch = 8, Avg Train Loss = 5.8475, Avg val loss = 1.8522, Time = 61.15s\n","Epoch = 9, Avg Train Loss = 5.8249, Avg val loss = 1.8259, Time = 61.57s\n","Epoch = 10, Avg Train Loss = 5.8285, Avg val loss = 1.8529, Time = 60.96s\n","Epoch = 11, Avg Train Loss = 5.8460, Avg val loss = 1.8263, Time = 61.75s\n","Epoch = 12, Avg Train Loss = 5.8192, Avg val loss = 1.8396, Time = 61.08s\n","Epoch = 13, Avg Train Loss = 5.8371, Avg val loss = 1.8213, Time = 59.05s\n","Epoch = 14, Avg Train Loss = 5.8195, Avg val loss = 1.8130, Time = 59.69s\n","Epoch = 15, Avg Train Loss = 5.8297, Avg val loss = 1.8141, Time = 61.01s\n","Epoch = 16, Avg Train Loss = 5.8182, Avg val loss = 1.8601, Time = 61.65s\n","Epoch = 17, Avg Train Loss = 5.8158, Avg val loss = 1.8158, Time = 61.59s\n","Epoch = 18, Avg Train Loss = 5.8484, Avg val loss = 1.8550, Time = 62.02s\n","Epoch = 19, Avg Train Loss = 5.8176, Avg val loss = 1.8113, Time = 60.73s\n","Epoch = 20, Avg Train Loss = 5.8298, Avg val loss = 1.8169, Time = 60.40s\n","Epoch = 21, Avg Train Loss = 5.8189, Avg val loss = 1.8853, Time = 59.37s\n","Epoch = 22, Avg Train Loss = 5.8260, Avg val loss = 1.8299, Time = 59.65s\n","Epoch = 23, Avg Train Loss = 5.8050, Avg val loss = 1.8129, Time = 59.88s\n","Epoch = 24, Avg Train Loss = 5.7984, Avg val loss = 1.8013, Time = 59.81s\n","Epoch = 25, Avg Train Loss = 5.7988, Avg val loss = 1.8615, Time = 59.81s\n","Epoch = 26, Avg Train Loss = 5.8234, Avg val loss = 1.8679, Time = 58.99s\n","Epoch = 27, Avg Train Loss = 5.8471, Avg val loss = 1.8068, Time = 59.14s\n","Epoch = 28, Avg Train Loss = 5.8000, Avg val loss = 1.9208, Time = 59.42s\n","Epoch = 29, Avg Train Loss = 5.8201, Avg val loss = 1.8410, Time = 59.34s\n"]}],"source":["model = ReModel(len(char_set))\n","train_gen_model(model, train, val, epochs_count=30, early_stopping=False, lr=0.02)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36877,"status":"ok","timestamp":1682495303641,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"4H54BKYuoAkP","outputId":"370b32be-b10a-453c-a940-925bf89382ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["------on train dataset:-------\n","precision: 0.36665677840403443\n","\n"," -------on val dataset:--------\n","precision: 0.3582443653618031\n","\n"," ------on train dataset:-------\n","precision: 0.27603143418467585\n"]}],"source":["print('on train dataset:'.center(30, '-'))\n","predict(model, train)\n","\n","print('\\n', 'on val dataset:'.center(30, '-'))\n","predict(model, val)\n","\n","print('\\n', 'on train dataset:'.center(30, '-'))\n","predict(model, test)"]},{"cell_type":"markdown","metadata":{"id":"3zjhcyIp3-NV"},"source":[" # тест"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JkparPxC8hJe"},"outputs":[],"source":["class ReModel(nn.Module):\n","    def __init__(self, char_set_size, word_set_size,\n","                 char_embedding_dim=8, word_embedding_dim=8,\n","                 classes_count=len(rel_2_id), \n","                 char_max_seq_len=40, lstm_embedding_dim=16):\n","        super().__init__()\n","        \n","        self.embeddings_layer = nn.Embedding(word_set_size, word_embedding_dim)\n","        #self.lstm_layer_1 = nn.LSTM(word_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n","        # self.dropout = nn.Dropout(0.2)\n","        self.hw = nn.Linear(1024*word_embedding_dim, 2*lstm_embedding_dim)\n","        self.h1 = nn.Linear(word_embedding_dim, lstm_embedding_dim)\n","        self.h2 = nn.Linear(word_embedding_dim, lstm_embedding_dim)\n","\n","        self.out_layer = nn.Linear(4*lstm_embedding_dim, classes_count)\n","        \n","    def forward(self, words, chars, ents):\n","        # обработка токенов\n","        print('-'*30)\n","        print(words)\n","        projections = self.embeddings_layer(words)\n","        print(projections)\n","        projections = projections.reshape(projections.size(0),-1)\n","        # _, (final_hidden_state_1, _) = self.lstm_layer_1(projections)\n","        # rnn_output_1 = torch.cat((final_hidden_state_1[-2], final_hidden_state_1[-1]), dim=1)\n","        hw = self.hw(projections)\n","       \n","        # обработка сущностей\n","        pr_e1_batch = []\n","        pr_e2_batch = []\n","        for inp_num, (e1, e2) in enumerate(ents):\n","          #сущность 1\n","          inp_1 = words[inp_num,e1[0]:e1[1]+1]\n","          pr_e1 = self.embeddings_layer(inp_1)\n","          pr_e1 = pr_e1.mean(dim=0)\n","          pr_e1_batch.append(pr_e1)\n","\n","          #сущность 2\n","          inp_2 = words[inp_num,e2[0]:e2[1]+1]\n","          pr_e2 = self.embeddings_layer(inp_2)\n","          pr_e2 = pr_e2.mean(dim=0)\n","          pr_e2_batch.append(pr_e2)\n","        pr_e1 = torch.stack(pr_e1_batch)\n","        pr_e2 = torch.stack(pr_e2_batch)\n","        h1 = self.h1(pr_e1)\n","        h2 = self.h2(pr_e2)\n","        h_cat = torch.cat([h1, h1], dim=1)\n","\n","        #объединяем инфо\n","        com = torch.cat([hw, h_cat], dim=1)\n","        output = self.out_layer.forward(com)\n","        #print(output.size())\n","        output = torch.log_softmax(output, dim=1)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gLiQcYe8xY21"},"outputs":[],"source":["model = ReModel(len(char_set), len(word_set))\n","train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.002)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9822,"status":"error","timestamp":1682894156411,"user":{"displayName":"Денис Исаев","userId":"06168685232953428184"},"user_tz":-180},"id":"bW4e3U0WbEND","outputId":"14de9748-31ce-46b2-faed-037ee2643e9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------\n","tensor([[ 5052, 10943,  6650,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[17134,  2821,   293,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 2067,  2835, 11755,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15178, 18247, 17947,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16764, 17130,  2821,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16536, 18508,  1693,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 17415, 12273,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 6529,  9064, 12534,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 3.0021,  0.1677,  0.2414,  ...,  0.9020, -1.1592,  1.0399],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14411,  6049, 13036,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[8402, 1541, 2821,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11923, 12051,  5930,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10688, 11178,  3154,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.6402,  1.2295,  0.2117,  ..., -0.6052, -1.8463, -1.9335],\n","         [ 1.0121, -1.2936,  1.8017,  ...,  1.7878, -0.0696, -0.2357],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16357,  9606, 18508,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[  239, 17926,  2821,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 11653,  2481,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12487,  8344,  9506,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-1.3754,  1.1585, -1.5116,  ..., -0.1377,  0.8042, -1.7924],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[5939,  142, 3682,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11424, 18247,  6400,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.1159, -1.9152,  0.3318,  ..., -0.3415,  1.2576, -0.7801],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-0.5206, -2.0407,  0.8734,  ...,  0.6825,  0.2819,  0.0850],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 7435, 16250,  5027,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 7435,  6096, 12640,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14810,  7241, 15645,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[5939, 3335, 9449,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11586, 14807,  2978,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 1.5633,  0.7850, -0.5308,  ..., -0.2638, -0.0829, -0.0612],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[3365, 2602, 3708,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 10931, 10672,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 3365, 14957, 14807,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15178, 18247, 15240,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[13659,  9918, 15634,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-2.2682,  2.8900, -1.9998,  ..., -0.0211,  1.4684,  2.1380],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10160,  6720, 13417,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[6425, 2821, 5027,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[-0.9599,  0.6455,  0.7646,  ..., -0.1789,  1.5237,  1.4949],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 3465, 13924, 18477,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 0.1900,  1.1792, -0.6043,  ...,  1.2878, -0.1217, -1.3882],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 2067,  2698, 13813,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 16543,  8848,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11091,  1768,  9953,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 1.1028,  0.1418, -0.1300,  ...,  0.8128, -0.7760,  0.3202],\n","         [-0.9426, -0.8779, -2.1226,  ...,  0.3649, -0.0182,  0.8294],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11104,  8645, 15317,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15129, 18247, 10247,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10637,  4439, 17749,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.1686, -0.6626, -0.6286,  ...,  0.2566, -1.1995, -0.7326],\n","         [-0.6400,  0.3914, -0.2203,  ...,  0.7850,  0.4564, -0.4028],\n","         [-0.1045, -0.1464,  0.8714,  ...,  0.8690, -0.3454, -0.5628],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 2067, 18017,   324,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  9258, 13036,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[9214, 5094, 2362,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12175, 18247, 17426,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15282, 10798,  1693,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.0604,  0.9555, -0.1179,  ..., -2.7726, -0.9101,  0.0694],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 8152, 18247,  9316,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[5939,  550, 7978,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753, 16566,  5440,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[   41, 10197, 16005,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.5053,  0.6372, -1.9878,  ...,  1.3625,  1.7246,  0.0150],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  9528, 12264,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  9606,  1502,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12175, 18247, 11923,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12722,  4296,  8951,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16617,  2000,  9323,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 1.7342, -0.2972,  0.5133,  ...,  0.1175,  0.1992, -1.0232],\n","         [-0.3649,  0.8425,  0.7542,  ..., -0.0564, -1.2955,  0.9324],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12175,  4296, 10134,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12922,  8379,  8645,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 0.8508, -1.2200, -0.0327,  ..., -1.1214,  0.4309, -1.0065],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14810,  4451, 14037,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  9258, 13036,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15240,  1077, 17417,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 2340, 15864,  3977,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 7612, 13791,  8917,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 8152, 18247,  9316,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 6769, 13857, 18115,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 6136, 12984,  4496,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.5589,  0.5213, -0.7099,  ...,  0.4565,  0.6190,  0.0982],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10922, 17413, 17110,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 8402, 11732,  1077,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4343,  1077, 13507,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 1.3729, -0.8446, -0.4506,  ...,  0.6587,  0.7106,  0.4007],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[5939, 3132, 9606,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 11653, 16535,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15321,  1541, 15832,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.8888, -0.4151, -0.1165,  ...,  0.1638,  0.3733,  0.2422],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10661, 13614,  6732,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 1.1896,  0.9427, -0.1142,  ...,  0.1130,  1.3315, -0.0748],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10247, 14967,  1077,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 17763,  6587,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[5939, 8235, 1205,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4756, 16553,  2821,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[17947, 14962,  4283,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 0.5265, -0.3710, -0.1422,  ...,  0.6461,  0.2175, -0.2155],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12175, 18247,  6892,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-1.0363, -2.7579,  0.6518,  ..., -1.4407,  1.1560,  0.7619],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14411, 13036,  4352,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[5859, 4296, 5312,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[  598, 16222, 15864,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14416,   489,  2932,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753, 11934,  4665,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-0.2357,  0.3379, -0.3735,  ..., -0.7009, -1.1826,  1.7800],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15178, 18247, 11753,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16357,  9606, 18508,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939,  3108, 10920,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 0.3066, -1.7002,  0.0032,  ..., -1.1750,  0.8435, -0.8613],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[1299, 6505, 8824,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[ 0.6548,  0.0935,  0.6821,  ..., -2.1408, -0.3559, -0.9988],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 11653,  9606,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16070, 11204, 11763,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[9623, 8039, 2821,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[8152, 4296, 2213,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 11754, 12273,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 8152, 18247,  2444,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[   22,  2821, 12019,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14411, 13036, 14018,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[17127, 18247,   295,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-0.5252, -1.7331,  0.0733,  ..., -0.3355, -0.8950,  0.3890],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[8189, 1077, 8372,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11586,  5550,  5591,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16214,  5993, 16214,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-1.0624,  0.2512,  2.3877,  ...,  1.2322,  0.6865,  0.2866],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-1.0624,  0.2512,  2.3877,  ...,  1.2322,  0.6865,  0.2866],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 7300,  2821, 17047,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.8311,  0.9475, -0.6207,  ...,  1.3979, -0.0150, -1.2153],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 12292, 10151,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14357,  4675, 10606,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[9623,  959, 9606,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15129,  4296, 18017,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[17134,  8645,  9619,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 3501, 14829, 10581,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[18453, 13348,   319,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 0.4007, -0.6027, -1.3738,  ...,  0.6069,  1.5197,  1.5364],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5898, 16968, 11732,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-0.6241,  0.1063, -1.6781,  ..., -0.8556,  0.0077, -1.2487],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[8242, 9097, 2654,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-0.0446,  0.9807,  0.2797,  ...,  0.0827,  0.0328,  1.0514],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[17127, 18247,  9623,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[  315, 11770,  2821,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[13239, 12722, 12969,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753, 15832,  1077,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 3796, 17443,  4559,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[5939,  142, 2453,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 6265, 18247,  7946,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14810,   445, 13196,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4514, 12784,  3657,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-1.3557, -1.0758, -2.4470,  ..., -1.6733,  0.2469, -0.4721],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12722,  4296,  4718,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 2323, 16535,  5027,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 0.3907, -1.2788,  0.1929,  ..., -1.5529, -0.1060, -0.5574],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 18069, 12364,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12722, 18247,  4356,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 1172, 16632, 12621,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10922, 17493,  9606,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 0.5244,  1.6896,  1.0422,  ...,  1.4069, -0.0666, -1.4678],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11923, 16625,  4434,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16666, 14773, 16954,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 3.0018, -1.3974,  0.0947,  ..., -1.0026, -0.4616, -0.6710],\n","         [ 0.6050,  0.0342,  0.9989,  ...,  1.0955,  1.4845,  0.0954],\n","         [ 0.1207,  1.0067,  1.0116,  ...,  0.6629,  0.2342, -1.0417],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16908,  3794, 13386,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.0098, -0.0931,  1.4208,  ..., -1.2733, -1.2886, -0.9056],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14382, 17110, 17102,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[13620,  7784,   247,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.4304,  1.6696, -0.2536,  ..., -0.3980, -0.1598,  1.2262],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10872, 12074, 10581,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-1.4518, -0.7968, -1.2688,  ...,  0.4360, -0.1278, -0.7754],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4756, 16553, 10478,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5514,  1969, 17912,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 1.3691, -0.8208, -0.7336,  ..., -1.2431,  1.8379, -0.3745],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11304, 13616, 12784,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12175, 18247, 12823,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11841, 13727,   927,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 0.5813,  0.6548,  0.1577,  ...,  0.4947, -1.2517,  0.1274],\n","         [-2.1410, -0.1205,  1.4970,  ...,  0.2863, -1.2975, -0.8658],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12175, 18247,  1136,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 7120, 13200,  5027,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12688,  5878, 10624,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-5.5628e-01, -2.5252e-02,  2.0156e-03,  ..., -2.0929e-01,\n","          -2.1443e-01,  7.4575e-01],\n","         [        nan,         nan,         nan,  ...,         nan,\n","                  nan,         nan],\n","         [ 9.4401e-01,  2.3303e+00, -2.4934e-01,  ...,  2.0412e+00,\n","          -2.0356e+00, -1.3679e-01],\n","         ...,\n","         [        nan,         nan,         nan,  ...,         nan,\n","                  nan,         nan],\n","         [        nan,         nan,         nan,  ...,         nan,\n","                  nan,         nan],\n","         [        nan,         nan,         nan,  ...,         nan,\n","                  nan,         nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 2545,  2189, 13417,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14197, 11376,  5027,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 1.1328, -0.4422,  0.0537,  ...,  0.1526,  0.1179,  0.1723],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[   22, 16250, 18079,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[15849,    57,  9695,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[7435, 7454, 9323,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14810,   445,  4436,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 1795,  4876, 18462,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 11653, 16056,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 0.6032, -1.3943,  0.3628,  ...,  0.0179, -0.7941, -0.6076],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5402, 17047, 16946,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.9594,  0.7103,  1.0611,  ...,  0.3032,  0.1081,  1.0271],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-0.0707, -1.1699,  0.5365,  ...,  0.7048,  0.7049,  0.5499],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5753, 12245,  5468,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14810,  9120,  7272,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 16075, 14361,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16764, 17130,  2821,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 8152, 18247, 15699,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  3422,  8824,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-0.0886,  0.4772,  0.4278,  ..., -0.6505, -2.3706,  1.8677],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14810,  6883,  4777,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 6159,  1541, 18351,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.4163,  0.0826,  0.4764,  ...,  0.2265, -0.1459, -1.0976],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939,  1932, 12273,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753, 16130,  3759,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10862,  1499, 14807,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  5602,  1075,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 9869, 14522, 14180,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  3682, 17004,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 18252,  6482,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14343,  2828,  4115,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 0.7191, -0.1832,  0.4379,  ...,  0.1801,  1.3733,  0.1130],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4191, 16535, 18261,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4525, 18247,  2545,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 16896, 16910,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16764, 17130,  2821,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14523, 17298,  4278,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.6627, -0.6463, -1.1034,  ..., -0.8981,  0.8666, -2.2000],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[8402, 4896, 9606,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[4544, 7398, 1146,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[  40, 9555, 3747,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16837, 17697, 17110,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[-0.2056,  0.1040,  1.0507,  ..., -0.2482,  1.2577,  0.5329],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 11653, 12305,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 3365,  2602, 14765,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12195, 15609, 10119,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [-1.4142, -0.6462, -0.7574,  ..., -0.1477,  0.1378, -0.5639],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 8152, 18247,  2852,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12175, 18247, 14701,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 1.2622,  0.7420, -0.6267,  ..., -1.9047,  0.3875, -1.2297],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939,   755, 12273,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5939, 16712,  7078,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[3326, 6330, 5027,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[ 1.2599,  0.0718,  0.2126,  ..., -0.1767,  1.8613,  1.4162],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12784, 12235,  1077,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4823, 14180, 14807,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[ 0.5286,  0.8005,  0.9278,  ..., -0.5247,  0.0199, -2.1571],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[16764, 17130,  2821,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4758,  2662, 10531,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 8621,  4089, 12807,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 0.0655, -0.5937,  2.4303,  ...,  1.2782,  2.1532, -0.9265],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5544, 18098, 14786,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11841,  1596, 10421,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 4191, 16535, 18261,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11039,  8676, 17883,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[  239, 17926,  2821,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 1399,  5027, 11824,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  6099,  5593,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [ 0.2831, -1.8784, -0.0900,  ..., -0.5611,  0.7875, -0.4613],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 5753, 12245, 15804,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[9869, 3835, 4352,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 8155,  8525, 14962,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[10247,  7288, 15001,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12175, 18247,  7435,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11923, 17417, 10581,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12722, 18247,  1136,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[13239,  6364, 12175,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[4213, 9084, 2821,  ...,    0,    0,    0]], device='cuda:0')\n","tensor([[[-1.4783, -0.5761, -0.8650,  ...,  1.2717,  0.3590,  1.3033],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         ...,\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n","         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[12185, 18247,  3229,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[14810, 14451, 17235,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[11753,  9258, 13036,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n","------------------------------\n","tensor([[ 2291,  3036, 18351,  ...,     0,     0,     0]], device='cuda:0')\n","tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         ...,\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan],\n","         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n","       grad_fn=<EmbeddingBackward0>)\n","tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n","       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-a6d51bfb1c4a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_next_gen_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0;31m# print(batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_chars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mplabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-84-a37451a651be>\u001b[0m in \u001b[0;36mget_next_gen_batch\u001b[0;34m(samples, max_seq_len, max_char_seq_len, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m                   \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_set\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                   \u001b[0;31m#символы\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                   \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchar_set\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mchar_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_char_seq_len\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#максимальная символьная длина токена - max_char_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                   \u001b[0mchars\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_char_seq_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#каждый токен должен быть представлен max_char_seq_len символами\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                   \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-84-a37451a651be>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m                   \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_set\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                   \u001b[0;31m#символы\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                   \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchar_set\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mchar_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_char_seq_len\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#максимальная символьная длина токена - max_char_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                   \u001b[0mchars\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_char_seq_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#каждый токен должен быть представлен max_char_seq_len символами\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                   \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model.eval()\n","for step, (_, batch_words, batch_chars, batch_ents, batch_labels) in enumerate(get_next_gen_batch(train)):\n","            # print(batch)\n","            logits = model(batch_words, batch_chars, batch_ents)\n","            plabels = logits.max(dim=1)[1]\n","            print(logits)\n","            # print(plabels)\n","            print()\n","            # break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EW_nN8aLfCNT"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyMNvfSI/UWdQ9uFuWr1pBO5"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
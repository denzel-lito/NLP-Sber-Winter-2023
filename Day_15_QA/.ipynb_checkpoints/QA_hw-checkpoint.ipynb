{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T15:04:04.756707Z",
     "start_time": "2023-05-12T15:04:04.750494Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, namedtuple\n",
    "import urllib.request\n",
    "from razdel import tokenize, sentenize\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import itertools\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ\n",
    "## Yes/No Questions\n",
    "\n",
    "–í —ç—Ç–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ –≤—ã –±—É–¥–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∫–æ—Ä–ø—É—Å–æ–º BoolQ. –ö–æ—Ä–ø—É—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–æ–ø—Ä–æ—Å–æ–≤, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—â–∏—Ö –±–∏–Ω–∞—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç (–¥–∞ / –Ω–µ—Ç), –∞–±–∑–∞—Ü–µ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏,  —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –∏–∑–≤–ª–µ—á–µ–Ω –∞–±–∑–∞—Ü –∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–≤–µ—Ç–∞ (true / false).\n",
    "\n",
    "–ö–æ—Ä–ø—É—Å –æ–ø–∏—Å–∞–Ω –≤ —Å—Ç–∞—Ç—å–µ:\n",
    "\n",
    "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova\n",
    "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n",
    "\n",
    "https://arxiv.org/abs/1905.10044\n",
    "\n",
    "\n",
    "–ö–æ—Ä–ø—É—Å (train-dev split) –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞:  https://github.com/google-research-datasets/boolean-questions\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è train —á–∞—Å—Ç—å –∫–æ—Ä–ø—É—Å–∞, –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äì dev —á–∞—Å—Ç—å. \n",
    "\n",
    "–ö–∞–∂–¥—ã–π –±–æ–Ω—É—Å –ø—É–Ω–∫—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—Å—è –≤ 1 –±–∞–ª–ª. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞: \n",
    "question: is batman and robin a sequel to batman forever\n",
    "\n",
    "title: Batman & Robin (film)\n",
    "\n",
    "answer: true\n",
    "\n",
    "passage: With the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. They hired director Joel Schumacher and writer Akiva Goldsman to reprise their duties the following August, and decided it was best to fast track production for a June 1997 target release date, which is a break from the usual 3-year gap between films. Schumacher wanted to homage both the broad camp style of the 1960s television series and the work of Dick Sprang. The storyline of Batman & Robin was conceived by Schumacher and Goldsman during pre-production on A Time to Kill. Portions of Mr. Freeze's back-story were based on the Batman: The Animated Series episode ''Heart of Ice'', written by Paul Dini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–†–ê–í–ò–õ–ê\n",
    "1. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –≥—Ä—É–ø–ø–µ –¥–æ 2-—Ö —á–µ–ª–æ–≤–µ–∫.\n",
    "2. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –æ—Ñ–æ—Ä–º–ª—è–µ—Ç—Å—è –≤ –≤–∏–¥–µ –æ—Ç—á–µ—Ç–∞ –≤ ipython-—Ç–µ—Ç—Ä–∞–¥–∫–µ. \n",
    "3. –û—Ç—á–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å: –Ω—É–º–µ—Ä–∞—Ü–∏—é –∑–∞–¥–∞–Ω–∏–π –∏ –ø—É–Ω–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –≤—ã–ø–æ–ª–Ω–∏–ª–∏, –∫–æ–¥ —Ä–µ—à–µ–Ω–∏—è, –∏ –ø–æ–Ω—è—Ç–Ω–æ–µ –ø–æ—à–∞–≥–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –≤—ã —Å–¥–µ–ª–∞–ª–∏. –û—Ç—á–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–º —Å—Ç–∏–ª–µ, –±–µ–∑ –∏–∑–ª–∏—à–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–µ–Ω–≥–∞ –∏ —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –Ω–æ—Ä–º —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\n",
    "4. –ù–µ —Å—Ç–æ–∏—Ç –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ª–µ–∫—Ü–∏–π, —Å—Ç–∞—Ç–µ–π –∏ –í–∏–∫–∏–ø–µ–¥–∏–∏ –≤ –≤–∞—à –æ—Ç—á–µ—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 1. [1 –±–∞–ª–ª] –≠–∫—Å–ø–ª–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "1. –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–æ–ª—é yes –∏ no –∫–ª–∞—Å—Å–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ\n",
    "2. –û—Ü–µ–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –≤–æ–ø—Ä–æ—Å–∞\n",
    "3. –û—Ü–µ–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞\n",
    "4. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ, –ø–æ –∫–∞–∫–∏–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º –±—ã–ª–∏ —Å–æ–±—Ä–∞–Ω—ã –≤–æ–ø—Ä–æ—Å—ã (–∏–ª–∏ –Ω–∞–π–¥–∏—Ç–µ –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∞—Ç—å–µ). –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—É–π—Ç–µ, –∫–∞–∫ —ç—Ç–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ–≤–ª–∏—è–ª–∏ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ—Ä–ø—É—Å–∞. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T12:30:37.685822Z",
     "start_time": "2023-05-12T12:30:37.498660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>Persian language</td>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/Ààp…úÀêr í…ôn, - É…ôn/), also known by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>Good Samaritan law</td>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is windows movie maker part of windows essentials</td>\n",
       "      <td>Windows Movie Maker</td>\n",
       "      <td>True</td>\n",
       "      <td>Windows Movie Maker (formerly known as Windows...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question                title  \\\n",
       "0    do iran and afghanistan speak the same language     Persian language   \n",
       "1  do good samaritan laws protect those who help ...   Good Samaritan law   \n",
       "2  is windows movie maker part of windows essentials  Windows Movie Maker   \n",
       "\n",
       "   answer                                            passage  \n",
       "0    True  Persian (/Ààp…úÀêr í…ôn, - É…ôn/), also known by its ...  \n",
       "1    True  Good Samaritan laws offer legal protection to ...  \n",
       "2    True  Windows Movie Maker (formerly known as Windows...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.read_json('train.jsonl', lines=True, orient='records')\n",
    "dev_data_df = pd.read_json('dev.jsonl', lines=True, orient='records')\n",
    "train_data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:27:18.453278Z",
     "start_time": "2023-05-11T15:27:18.441479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>cnt</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>5874</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>3553</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer   cnt    rt\n",
       "0    True  5874  0.62\n",
       "1   False  3553  0.38"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_1 = train_data_df.answer.value_counts()\\\n",
    "                   .reset_index()\\\n",
    "                   .rename(columns={'index':'answer', 'answer':'cnt'})\n",
    "a_1['rt'] = a_1['cnt']/a_1['cnt'].sum()\n",
    "a_1['rt'] = a_1['rt'].round(2)\n",
    "a_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:28:00.533994Z",
     "start_time": "2023-05-11T15:28:00.530310Z"
    }
   },
   "source": [
    "##### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:28:44.322252Z",
     "start_time": "2023-05-11T15:28:44.312053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.question.apply(len).mean().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:29:15.441166Z",
     "start_time": "2023-05-11T15:29:15.418927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.passage.apply(len).mean().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) <br>\n",
    "–≤–æ–ø—Ä–æ—Å—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å–æ —Å–ª–æ–≤-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:50:30.469285Z",
     "start_time": "2023-05-11T15:50:30.444815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st_qst_word</th>\n",
       "      <th>cnt</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>4190</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can</td>\n",
       "      <td>1136</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does</td>\n",
       "      <td>952</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are</td>\n",
       "      <td>693</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do</td>\n",
       "      <td>664</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>did</td>\n",
       "      <td>461</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>was</td>\n",
       "      <td>335</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>has</td>\n",
       "      <td>302</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>will</td>\n",
       "      <td>181</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>91</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>have</td>\n",
       "      <td>70</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>in</td>\n",
       "      <td>35</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>were</td>\n",
       "      <td>25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>if</td>\n",
       "      <td>17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a</td>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>when</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>could</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1st_qst_word   cnt    rt\n",
       "0            is  4190  0.44\n",
       "1           can  1136  0.12\n",
       "2          does   952  0.10\n",
       "3           are   693  0.07\n",
       "4            do   664  0.07\n",
       "5           did   461  0.05\n",
       "6           was   335  0.04\n",
       "7           has   302  0.03\n",
       "8          will   181  0.02\n",
       "9           the    91  0.01\n",
       "10         have    70  0.01\n",
       "11           in    35  0.00\n",
       "12         were    25  0.00\n",
       "13           if    17  0.00\n",
       "14            a    16  0.00\n",
       "15         what    11  0.00\n",
       "16         when    10  0.00\n",
       "17        could     9  0.00"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_4 = train_data_df.question.apply(lambda x: x.split()[0])\\\n",
    "                            .value_counts()\\\n",
    "                            .reset_index()\\\n",
    "                            .rename(columns={'index':'1st_qst_word','question':'cnt'})\n",
    "a_4 = a_4[a_4.cnt>5]\n",
    "a_4['rt'] = (a_4.cnt / len(train_data_df)).round(2)\n",
    "a_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 2. [1 –±–∞–ª–ª] Baseline\n",
    "1. –û—Ü–µ–Ω–∏—Ç–µ accuracy —Ç–æ—á–Ω–æ—Å—Ç—å —Å–æ–≤—Å–µ–º –ø—Ä–æ—Å—Ç–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: –ø—Ä–∏—Å–≤–æ–∏—Ç—å –∫–∞–∂–¥–æ–π –ø–∞—Ä–µ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –≤ dev —á–∞—Å—Ç–∏ —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å –∏–∑ train —á–∞—Å—Ç–∏\n",
    "2. –û—Ü–µ–Ω–∏—Ç–µ accuracy —á—É—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: fasttext –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ —Å–∫–ª–µ–µ–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∞–±–∑–∞—Ü–µ–≤ (' '.join([question, passage]))\n",
    "\n",
    "–ü–æ—á–µ–º—É fasttext –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —ç—Ç–æ–π –∑–∞–¥–∞—á–µ–π?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:57:18.442466Z",
     "start_time": "2023-05-11T15:57:18.432946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6217125382262997"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frq_ans = Counter(train_data_df.answer).most_common(1)[0][0]\n",
    "(dev_data_df.answer == most_frq_ans).sum() / len(dev_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:57:07.670426Z",
     "start_time": "2023-05-11T15:57:07.662985Z"
    }
   },
   "source": [
    "##### 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/fasttext-for-text-classification-a4b38cbff27c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T12:59:13.238048Z",
     "start_time": "2023-05-12T12:59:01.751432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9427/9427 [00:08<00:00, 1108.87it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3270/3270 [00:02<00:00, 1111.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_text = [  ' '.join([question, passage])\n",
    "                for question, passage \n",
    "                in zip(train_data_df.question.tolist(), train_data_df.passage.tolist())]\n",
    "\n",
    "for i in tqdm(range(len(train_text))):\n",
    "    text_i = []\n",
    "    for sent in sentenize(train_text[i]):\n",
    "        sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "        text_i.extend(sent_i)\n",
    "    train_text[i] = text_i\n",
    "    \n",
    "# dev\n",
    "dev_text = [  ' '.join([question, passage])\n",
    "                for question, passage \n",
    "                in zip(dev_data_df.question.tolist(), dev_data_df.passage.tolist())]\n",
    "\n",
    "for i in tqdm(range(len(dev_text))):\n",
    "    text_i = []\n",
    "    for sent in sentenize(dev_text[i]):\n",
    "        sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "        text_i.extend(sent_i)\n",
    "    dev_text[i] = text_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving txt-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:13.538012Z",
     "start_time": "2023-05-12T13:15:13.265219Z"
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_dataset = pd.DataFrame({'text': train_text, 'target': train_data_df.answer.tolist()})\n",
    "train_dataset['text'] = train_dataset['text'].apply(lambda x: ' '.join(x))\n",
    "train_dataset['target'] = train_dataset['target'].apply(lambda x: '__label__' + str(x))\n",
    "train_dataset[['target', 'text']].to_csv('train.txt', \n",
    "                                         index = False, \n",
    "                                         sep = ' ',\n",
    "                                         header = None, \n",
    "                                         quoting = csv.QUOTE_NONE, \n",
    "                                         quotechar = \"\", \n",
    "                                         escapechar = \" \")\n",
    "# dev\n",
    "dev_dataset = pd.DataFrame({'text': dev_text, 'target': dev_data_df.answer.tolist()})\n",
    "dev_dataset['text'] = dev_dataset['text'].apply(lambda x: ' '.join(x))\n",
    "dev_dataset['target'] = dev_dataset['target'].apply(lambda x: '__label__' + str(x))\n",
    "dev_dataset[['target', 'text']].to_csv('dev.txt', \n",
    "                                         index = False, \n",
    "                                         sep = ' ',\n",
    "                                         header = None, \n",
    "                                         quoting = csv.QUOTE_NONE, \n",
    "                                         quotechar = \"\", \n",
    "                                         escapechar = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:18.205834Z",
     "start_time": "2023-05-12T13:15:16.145093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  47170\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  995967 lr:  0.000000 avg.loss:  0.654642 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised('train.txt', wordNgrams = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:24.690909Z",
     "start_time": "2023-05-12T13:15:24.541792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3270, 0.6290519877675841, 0.6290519877675841)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('dev.txt')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:16:02.331851Z",
     "start_time": "2023-05-12T13:16:02.324819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__label__True',)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(dev_dataset['text'].iloc[0])[0]\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 3. [1 –±–∞–ª–ª] –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "1. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–æ–ø—Ä–æ—Å–∞ –∏ –∞–±–∑–∞—Ü–∞. –û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö –≤–æ–ø—Ä–æ—Å–∞ –∏ –∞–±–∑–∞—Ü–∞ –∏ –æ—Ü–µ–Ω–∏—Ç–µ accuracy —ç—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. \n",
    "\n",
    "[bonus] –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –¥–æ—Å—Ç—É–ø–Ω—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Transformers. –ö–∞–∫–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–∞—Å—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã?\n",
    "\n",
    "[bonus] –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –º–µ—Ç–æ–¥ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–π—Ç–µ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:12:35.925154Z",
     "start_time": "2023-05-12T14:12:28.182051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# train\n",
    "train_dataset = train_data_df[['question', 'passage', 'answer']].copy()\n",
    "train_dataset['question'] = train_dataset.question.apply(lambda x: tokenizer(x)['input_ids']) #tokenizer.tokenize(x))\n",
    "train_dataset['question'] = train_dataset.question.apply(lambda x: x[:30] + [0]*(30-len(x[:30]))) \n",
    "train_dataset['passage'] = train_dataset.passage.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "train_dataset['passage'] = train_dataset.passage.apply(lambda x: x[:1000] + [0]*(1000-len(x[:1000])))\n",
    "train_dataset['q&p'] = train_dataset[['question', 'passage']].apply(lambda x: x[0]+x[1], axis=1)\n",
    "\n",
    "# dev\n",
    "dev_dataset = dev_data_df[['question', 'passage', 'answer']].copy()\n",
    "dev_dataset['question'] = dev_dataset.question.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "dev_dataset['question'] = dev_dataset.question.apply(lambda x: x[:30] + [0]*(30-len(x[:30]))) \n",
    "dev_dataset['passage'] = dev_dataset.passage.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "dev_dataset['passage'] = dev_dataset.passage.apply(lambda x: x[:1000] + [0]*(1000-len(x[:1000])))\n",
    "dev_dataset['q&p'] = dev_dataset[['question', 'passage']].apply(lambda x: x[0]+x[1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:32:01.422386Z",
     "start_time": "2023-05-12T14:32:00.207439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.6375304975071603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/16971921/.conda/envs/py38_venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = np.array(train_dataset['q&p'].tolist()), train_dataset['answer'].astype(int)\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "predict = clf.predict(X_train)\n",
    "print('train accuracy:', (predict == y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:32:52.180859Z",
     "start_time": "2023-05-12T14:32:51.890150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.6067278287461774\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = np.array(dev_dataset['q&p'].tolist()), dev_dataset['answer'].astype(int)\n",
    "predict = clf.predict(X_test)\n",
    "print('test accuracy:', (predict == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 3. [3 –±–∞–ª–ª–∞] DrQA-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Å—Ç–∞—Ç—å–µ: Reading Wikipedia to Answer Open-Domain Questions\n",
    "\n",
    "Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes\n",
    "\n",
    "https://arxiv.org/abs/1704.00051\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DrQA –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ SQuAD, –Ω–æ –ª–µ–≥–∫–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫ —Ç–µ–∫—É—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –±–ª–æ–∫–æ–≤:\n",
    "1. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –∞–±–∑–∞—Ü–∞ [paragraph encoding] ‚Äì LSTM, –ø–æ–ª—É—á–∞—è—â–∞—è –Ω–∞ –≤—Ö–æ–¥ –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑: \n",
    "* —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å–ª–æ–≤–∞ (w2v –∏–ª–∏ fasttext)\n",
    "* –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –≤ –≤–∏–¥–µ one-hot –≤–µ–∫—Ç–æ—Ä–æ–≤ —á–∞—Å—Ç—å —Ä–µ—á–∏ —Å–ª–æ–≤–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω–æ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç—å—é –∏–ª–∏ –Ω–µ—Ç, –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –≤ –≤–æ–ø—Ä–æ—Å–µ –∏–ª–∏ –Ω–µ—Ç \n",
    "* –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞, –ø–æ–ª—É—á–∞–µ–º–æ–≥–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º soft attention –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Å–ª–æ–≤ –∏–∑ –∞–±–∑–∞—Ü–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º –≤–æ–ø—Ä–æ—Å–∞.\n",
    "\n",
    "$f_{align}(p_i) = \\sum_jÙè∞Ç a_{i,j} E(q_j)$, –≥–¥–µ $E(q_j)$ ‚Äì —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞. –§–æ—Ä–º—É–ª–∞ –¥–ª—è $a_{i,j}$ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ. \n",
    "\n",
    "2. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –≤–æ–ø—Ä–æ—Å–∞ [question encoding] ‚Äì LSTM, –ø–æ–ª—É—á–∞—è—â–∞—è –Ω–∞ –≤—Ö–æ–¥ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞. –í—ã—Ö–æ–¥ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞: $q = Ùè∞Ç\\sum_jÙè∞Ç  b_j q_j$. –§–æ—Ä–º—É–ª–∞ –¥–ª—è $b_{j}$ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ. \n",
    "\n",
    "3. –°–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. \n",
    "\n",
    "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ, –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã–ª–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ DrQA, —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ, —á—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ‚Äì¬†—ç—Ç–æ –º–µ—Ç–∫–∞ yes / no, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ—â–µ, —á–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ø–∞–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è SQuAD.\n",
    "\n",
    "–û—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. \n",
    "\n",
    "[bonus] –ó–∞–º–µ–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞–º–∏, –Ω–∞ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏. –£–ª—É—á—à–∏—Ç –ª–∏ —ç—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json('train.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T15:06:48.858412Z",
     "start_time": "2023-05-12T15:06:48.849495Z"
    }
   },
   "outputs": [],
   "source": [
    "Sample = namedtuple(\"Sample\", \"question, passage, labels\")\n",
    "\n",
    "def get_dataset(path_file: list):\n",
    "    samples = []\n",
    "    file = pd.read_json(path_file, lines=True, orient='records')\n",
    "    question = file.question.tolist()\n",
    "    passage = file.passage.tolist()\n",
    "    \n",
    "    for i in tqdm(range(len(file))):\n",
    "        #question\n",
    "        text_i = []\n",
    "        for sent in sentenize(question[i]):\n",
    "            sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "            text_i.extend(sent_i)\n",
    "        question[i] = text_i\n",
    "        \n",
    "        #question\n",
    "        text_i = []\n",
    "        for sent in sentenize(passage[i]):\n",
    "            sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "            text_i.extend(sent_i)\n",
    "        passage[i] = text_i\n",
    "    \n",
    "        labels = file.answer.astype(int)\n",
    "    \n",
    "        sample = Sample(question, passage, labels)\n",
    "        samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len=500\n",
    "# max_char_seq_len=40\n",
    "# batch_size=32\n",
    "def get_next_gen_batch(samples, max_seq_len=512, batch_size=30):\n",
    "    indices = np.arange(len(samples))\n",
    "    np.random.shuffle(indices)\n",
    "    batch_begin = 0\n",
    "    with tqdm(total=len(samples)) as pbar:\n",
    "        while batch_begin < len(samples):\n",
    "            batch_indices = indices[batch_begin: batch_begin + batch_size]\n",
    "            batch_qst = []\n",
    "            batch_psg = []\n",
    "            batch_labels = []\n",
    "\n",
    "            for data_ind in batch_indices:\n",
    "                sample = samples[data_ind] #–±–µ—Ä—É –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
    "                words = torch.zeros(max_seq_len, dtype=torch.long).cuda()\n",
    "                inputs = torch.zeros((max_seq_len, max_char_seq_len), dtype=torch.long).cuda()\n",
    "                for token_num, token in enumerate(sample.tokens[:max_seq_len]): #—Ü–∏–∫–ª –ø–æ —Ç–æ–∫–µ–Ω–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ –¥–æ max_seq_len\n",
    "                    #—Å–ª–æ–≤–∞\n",
    "                    words[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                    #—Å–∏–º–≤–æ–ª—ã\n",
    "                    for char_num, char in enumerate(token[:max_char_seq_len]):\n",
    "                        inputs[token_num][char_num] = char_set.index(char) if char in char_set else char_set.index('<unk>')\n",
    "                labels = sample.labels[:max_seq_len]         #–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Å labels\n",
    "                masks = [1]*len(labels) + [0]*(max_seq_len - len(labels)) \n",
    "                labels += [0] * (max_seq_len - len(labels))  #–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Å labels\n",
    "                \n",
    "                batch_words.append(words)\n",
    "                batch_chars.append(inputs)\n",
    "                batch_labels.append(labels)\n",
    "                batch_masks.append(masks)\n",
    "              \n",
    "          batch_begin += batch_size\n",
    "          pbar.update(batch_size)\n",
    "          \n",
    "          batch_words = torch.stack(batch_words)\n",
    "          batch_chars = torch.stack(batch_chars)\n",
    "          labels = torch.cuda.LongTensor(batch_labels)\n",
    "          batch_masks = torch.tensor(batch_masks).cuda()>0\n",
    "          yield batch_indices, batch_words, batch_chars, labels, batch_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 4. [3 –±–∞–ª–ª–∞] BiDAF-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Å—Ç–∞—Ç—å–µ: Bidirectional Attention Flow for Machine Comprehension\n",
    "\n",
    "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "\n",
    "https://arxiv.org/abs/1611.01603\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ BiDAF –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ SQuAD, –Ω–æ –ª–µ–≥–∫–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫ —Ç–µ–∫—É—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –±–ª–æ–∫–æ–≤:\n",
    "1. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫  –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –¥–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞: —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ –∏ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –∏–∑ CNN –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ –∏ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã. \n",
    "2. –°–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è (–¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–æ –≤ —Å—Ç–∞—Ç—å–µ, —Å–º. –ø—É–Ω–∫—Ç Attention Flow Layer)\n",
    "3. –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ —Ç—Ä–µ—Ö —á–∞—Å—Ç–µ–π (–≤—ã—Ö–æ–¥ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞,   Query2Context (–æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä) –∏ Context2Query (–º–∞—Ç—Ä–∏—Ü–∞) –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è\n",
    "\n",
    "4. –°–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. \n",
    "\n",
    "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ, –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã–ª–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ BiDAF, —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ, —á—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ‚Äì¬†—ç—Ç–æ –º–µ—Ç–∫–∞ yes / no, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ—â–µ, —á–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ø–∞–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è SQuAD.\n",
    "\n",
    "–û—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. \n",
    "\n",
    "[bonus] –ó–∞–º–µ–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞–º–∏, –Ω–∞ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏. –£–ª—É—á—à–∏—Ç –ª–∏ —ç—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ DrQA –∏ BiDAF:\n",
    "    \n",
    "![](https://www.researchgate.net/profile/Felix_Wu6/publication/321069852/figure/fig1/AS:560800147881984@1510716582560/Schematic-layouts-of-the-BiDAF-left-and-DrQA-right-architectures-We-propose-to.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 5. [1 –±–∞–ª–ª] –ò—Ç–æ–≥–∏\n",
    "–ù–∞–ø–∏—à–∏—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ß—Ç–æ –ø–æ–º–æ–≥–ª–æ –≤–∞–º –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã, —á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–ª–æ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip', \n",
    "#                            'pre-trained-fasttext.zip')\n",
    "\n",
    "# from zipfile import ZipFile\n",
    "# with ZipFile('pre-trained-fasttext.zip', 'r') as f:\n",
    "#     f.extractall('pre-trained-fasttext')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

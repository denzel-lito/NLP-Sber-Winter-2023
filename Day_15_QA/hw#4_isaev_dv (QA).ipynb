{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:23:01.316099Z",
     "start_time": "2023-05-20T22:23:01.309268Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, namedtuple\n",
    "import urllib.request\n",
    "from razdel import tokenize, sentenize\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import itertools\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "## Yes/No Questions\n",
    "\n",
    "В этом домашнем задании вы будете работать с корпусом BoolQ. Корпус состоит из вопросов, предполагающих бинарный ответ (да / нет), абзацев из Википедии,  содержащих ответ на вопрос, заголовка статьи, из которой извлечен абзац и непосредственно ответа (true / false).\n",
    "\n",
    "Корпус описан в статье:\n",
    "\n",
    "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova\n",
    "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n",
    "\n",
    "https://arxiv.org/abs/1905.10044\n",
    "\n",
    "\n",
    "Корпус (train-dev split) доступен в репозитории проекта:  https://github.com/google-research-datasets/boolean-questions\n",
    "\n",
    "Используйте для обучения train часть корпуса, для валидации и тестирования – dev часть. \n",
    "\n",
    "Каждый бонус пункт оцениватся в 1 балл. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример вопроса: \n",
    "question: is batman and robin a sequel to batman forever\n",
    "\n",
    "title: Batman & Robin (film)\n",
    "\n",
    "answer: true\n",
    "\n",
    "passage: With the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. They hired director Joel Schumacher and writer Akiva Goldsman to reprise their duties the following August, and decided it was best to fast track production for a June 1997 target release date, which is a break from the usual 3-year gap between films. Schumacher wanted to homage both the broad camp style of the 1960s television series and the work of Dick Sprang. The storyline of Batman & Robin was conceived by Schumacher and Goldsman during pre-production on A Time to Kill. Portions of Mr. Freeze's back-story were based on the Batman: The Animated Series episode ''Heart of Ice'', written by Paul Dini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРАВИЛА\n",
    "1. Домашнее задание выполняется в группе до 2-х человек.\n",
    "2. Домашнее задание оформляется в виде отчета в ipython-тетрадке. \n",
    "3. Отчет должен содержать: нумерацию заданий и пунктов, которые вы выполнили, код решения, и понятное пошаговое описание того, что вы сделали. Отчет должен быть написан в академическом стиле, без излишнего использования сленга и с соблюдением норм русского языка.\n",
    "4. Не стоит копировать фрагменты лекций, статей и Википедии в ваш отчет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. [1 балл] Эксплоративный анализ\n",
    "1. Посчитайте долю yes и no классов в корпусе\n",
    "2. Оцените среднюю длину вопроса\n",
    "3. Оцените среднюю длину параграфа\n",
    "4. Предположите, по каким эвристикам были собраны вопросы (или найдите ответ в статье). Продемонстриуйте, как эти эвристики повлияли на структуру корпуса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T09:51:57.042008Z",
     "start_time": "2023-05-20T09:51:56.787343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>Persian language</td>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>Good Samaritan law</td>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is windows movie maker part of windows essentials</td>\n",
       "      <td>Windows Movie Maker</td>\n",
       "      <td>True</td>\n",
       "      <td>Windows Movie Maker (formerly known as Windows...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question                title  \\\n",
       "0    do iran and afghanistan speak the same language     Persian language   \n",
       "1  do good samaritan laws protect those who help ...   Good Samaritan law   \n",
       "2  is windows movie maker part of windows essentials  Windows Movie Maker   \n",
       "\n",
       "   answer                                            passage  \n",
       "0    True  Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...  \n",
       "1    True  Good Samaritan laws offer legal protection to ...  \n",
       "2    True  Windows Movie Maker (formerly known as Windows...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.read_json('train.jsonl', lines=True, orient='records')\n",
    "dev_data_df = pd.read_json('dev.jsonl', lines=True, orient='records')\n",
    "train_data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:27:18.453278Z",
     "start_time": "2023-05-11T15:27:18.441479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>cnt</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>5874</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>3553</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer   cnt    rt\n",
       "0    True  5874  0.62\n",
       "1   False  3553  0.38"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_1 = train_data_df.answer.value_counts()\\\n",
    "                   .reset_index()\\\n",
    "                   .rename(columns={'index':'answer', 'answer':'cnt'})\n",
    "a_1['rt'] = a_1['cnt']/a_1['cnt'].sum()\n",
    "a_1['rt'] = a_1['rt'].round(2)\n",
    "a_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:28:00.533994Z",
     "start_time": "2023-05-11T15:28:00.530310Z"
    }
   },
   "source": [
    "##### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:28:44.322252Z",
     "start_time": "2023-05-11T15:28:44.312053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.question.apply(len).mean().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:29:15.441166Z",
     "start_time": "2023-05-11T15:29:15.418927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.passage.apply(len).mean().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) <br>\n",
    "вопросы начинаются со слов-индикаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:50:30.469285Z",
     "start_time": "2023-05-11T15:50:30.444815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st_qst_word</th>\n",
       "      <th>cnt</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>4190</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can</td>\n",
       "      <td>1136</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does</td>\n",
       "      <td>952</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are</td>\n",
       "      <td>693</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do</td>\n",
       "      <td>664</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>did</td>\n",
       "      <td>461</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>was</td>\n",
       "      <td>335</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>has</td>\n",
       "      <td>302</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>will</td>\n",
       "      <td>181</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>91</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>have</td>\n",
       "      <td>70</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>in</td>\n",
       "      <td>35</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>were</td>\n",
       "      <td>25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>if</td>\n",
       "      <td>17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a</td>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>when</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>could</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1st_qst_word   cnt    rt\n",
       "0            is  4190  0.44\n",
       "1           can  1136  0.12\n",
       "2          does   952  0.10\n",
       "3           are   693  0.07\n",
       "4            do   664  0.07\n",
       "5           did   461  0.05\n",
       "6           was   335  0.04\n",
       "7           has   302  0.03\n",
       "8          will   181  0.02\n",
       "9           the    91  0.01\n",
       "10         have    70  0.01\n",
       "11           in    35  0.00\n",
       "12         were    25  0.00\n",
       "13           if    17  0.00\n",
       "14            a    16  0.00\n",
       "15         what    11  0.00\n",
       "16         when    10  0.00\n",
       "17        could     9  0.00"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_4 = train_data_df.question.apply(lambda x: x.split()[0])\\\n",
    "                            .value_counts()\\\n",
    "                            .reset_index()\\\n",
    "                            .rename(columns={'index':'1st_qst_word','question':'cnt'})\n",
    "a_4 = a_4[a_4.cnt>5]\n",
    "a_4['rt'] = (a_4.cnt / len(train_data_df)).round(2)\n",
    "a_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. [1 балл] Baseline\n",
    "1. Оцените accuracy точность совсем простого базового решения: присвоить каждой паре вопрос-ответ в dev части самый частый класс из train части\n",
    "2. Оцените accuracy чуть более сложного базового решения: fasttext на текстах, состоящих из склееных вопросов и абзацев (' '.join([question, passage]))\n",
    "\n",
    "Почему fasttext плохо справляется с этой задачей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:57:18.442466Z",
     "start_time": "2023-05-11T15:57:18.432946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6217125382262997"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frq_ans = Counter(train_data_df.answer).most_common(1)[0][0]\n",
    "(dev_data_df.answer == most_frq_ans).sum() / len(dev_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:57:07.670426Z",
     "start_time": "2023-05-11T15:57:07.662985Z"
    }
   },
   "source": [
    "##### 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/fasttext-for-text-classification-a4b38cbff27c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T15:22:52.414266Z",
     "start_time": "2023-05-12T15:22:40.127939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9427/9427 [00:08<00:00, 1099.13it/s]\n",
      "100%|█████████████████████████████████████| 3270/3270 [00:02<00:00, 1123.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_text = [  ' '.join([question, passage])\n",
    "                for question, passage \n",
    "                in zip(train_data_df.question.tolist(), train_data_df.passage.tolist())]\n",
    "\n",
    "for i in tqdm(range(len(train_text))):\n",
    "    text_i = []\n",
    "    for sent in sentenize(train_text[i]):\n",
    "        sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "        text_i.extend(sent_i)\n",
    "    train_text[i] = text_i\n",
    "    \n",
    "# dev\n",
    "dev_text = [  ' '.join([question, passage])\n",
    "                for question, passage \n",
    "                in zip(dev_data_df.question.tolist(), dev_data_df.passage.tolist())]\n",
    "\n",
    "for i in tqdm(range(len(dev_text))):\n",
    "    text_i = []\n",
    "    for sent in sentenize(dev_text[i]):\n",
    "        sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "        text_i.extend(sent_i)\n",
    "    dev_text[i] = text_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving txt-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:13.538012Z",
     "start_time": "2023-05-12T13:15:13.265219Z"
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_dataset = pd.DataFrame({'text': train_text, 'target': train_data_df.answer.tolist()})\n",
    "train_dataset['text'] = train_dataset['text'].apply(lambda x: ' '.join(x))\n",
    "train_dataset['target'] = train_dataset['target'].apply(lambda x: '__label__' + str(x))\n",
    "train_dataset[['target', 'text']].to_csv('train.txt', \n",
    "                                         index = False, \n",
    "                                         sep = ' ',\n",
    "                                         header = None, \n",
    "                                         quoting = csv.QUOTE_NONE, \n",
    "                                         quotechar = \"\", \n",
    "                                         escapechar = \" \")\n",
    "# dev\n",
    "dev_dataset = pd.DataFrame({'text': dev_text, 'target': dev_data_df.answer.tolist()})\n",
    "dev_dataset['text'] = dev_dataset['text'].apply(lambda x: ' '.join(x))\n",
    "dev_dataset['target'] = dev_dataset['target'].apply(lambda x: '__label__' + str(x))\n",
    "dev_dataset[['target', 'text']].to_csv('dev.txt', \n",
    "                                         index = False, \n",
    "                                         sep = ' ',\n",
    "                                         header = None, \n",
    "                                         quoting = csv.QUOTE_NONE, \n",
    "                                         quotechar = \"\", \n",
    "                                         escapechar = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:18.205834Z",
     "start_time": "2023-05-12T13:15:16.145093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  47170\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  995967 lr:  0.000000 avg.loss:  0.654642 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised('train.txt', wordNgrams = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:24.690909Z",
     "start_time": "2023-05-12T13:15:24.541792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3270, 0.6290519877675841, 0.6290519877675841)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('dev.txt')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:16:02.331851Z",
     "start_time": "2023-05-12T13:16:02.324819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__label__True',)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(dev_dataset['text'].iloc[0])[0]\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. [1 балл] Используем эмбеддинги предложений\n",
    "1. Постройте BERT эмбеддинги вопроса и абзаца. Обучите логистическую регрессию на конкатенированных эмбеддингах вопроса и абзаца и оцените accuracy этого решения. \n",
    "\n",
    "[bonus] Используйте другие модели эмбеддингов, доступные, например, в библиотеке 🤗 Transformers. Какая модель эмбеддингов даст лучшие результаты?\n",
    "\n",
    "[bonus] Предложите метод аугментации данных и продемонстрируйте его эффективность. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:12:35.925154Z",
     "start_time": "2023-05-12T14:12:28.182051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# train\n",
    "train_dataset = train_data_df[['question', 'passage', 'answer']].copy()\n",
    "train_dataset['question'] = train_dataset.question.apply(lambda x: tokenizer(x)['input_ids']) #tokenizer.tokenize(x))\n",
    "train_dataset['question'] = train_dataset.question.apply(lambda x: x[:30] + [0]*(30-len(x[:30]))) \n",
    "train_dataset['passage'] = train_dataset.passage.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "train_dataset['passage'] = train_dataset.passage.apply(lambda x: x[:1000] + [0]*(1000-len(x[:1000])))\n",
    "train_dataset['q&p'] = train_dataset[['question', 'passage']].apply(lambda x: x[0]+x[1], axis=1)\n",
    "\n",
    "# dev\n",
    "dev_dataset = dev_data_df[['question', 'passage', 'answer']].copy()\n",
    "dev_dataset['question'] = dev_dataset.question.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "dev_dataset['question'] = dev_dataset.question.apply(lambda x: x[:30] + [0]*(30-len(x[:30]))) \n",
    "dev_dataset['passage'] = dev_dataset.passage.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "dev_dataset['passage'] = dev_dataset.passage.apply(lambda x: x[:1000] + [0]*(1000-len(x[:1000])))\n",
    "dev_dataset['q&p'] = dev_dataset[['question', 'passage']].apply(lambda x: x[0]+x[1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:32:01.422386Z",
     "start_time": "2023-05-12T14:32:00.207439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.6375304975071603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/16971921/.conda/envs/py38_venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = np.array(train_dataset['q&p'].tolist()), train_dataset['answer'].astype(int)\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "predict = clf.predict(X_train)\n",
    "print('train accuracy:', (predict == y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:32:52.180859Z",
     "start_time": "2023-05-12T14:32:51.890150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.6067278287461774\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = np.array(dev_dataset['q&p'].tolist()), dev_dataset['answer'].astype(int)\n",
    "predict = clf.predict(X_test)\n",
    "print('test accuracy:', (predict == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. [3 балла] DrQA-подобная архитектура\n",
    "\n",
    "Основана на статье: Reading Wikipedia to Answer Open-Domain Questions\n",
    "\n",
    "Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes\n",
    "\n",
    "https://arxiv.org/abs/1704.00051\n",
    "\n",
    "Архитектура DrQA предложена для задачи SQuAD, но легко может быть адаптирована к текущему заданию. Модель состоит из следующих блоков:\n",
    "1. Кодировщик абзаца [paragraph encoding] – LSTM, получаящая на вход вектора слов, состоящие из: \n",
    "* эмбеддинга слова (w2v или fasttext)\n",
    "* дополнительных признаков-индикаторов, кодирующих в виде one-hot векторов часть речи слова, является ли оно именованной сущностью или нет, встречается ли слово в вопросе или нет \n",
    "* выровненного эмбеддинга вопроса, получаемого с использованием soft attention между эмбеддингами слов из абзаца и эмбеддингом вопроса.\n",
    "\n",
    "$f_{align}(p_i) = \\sum_j􏰂 a_{i,j} E(q_j)$, где $E(q_j)$ – эмбеддинг слова из вопроса. Формула для $a_{i,j}$ приведена в статье. \n",
    "\n",
    "2. Кодировщик вопроса [question encoding] – LSTM, получаящая на вход эмбеддинги слов из вопроса. Выход кодировщика: $q = 􏰂\\sum_j􏰂  b_j q_j$. Формула для $b_{j}$ приведена в статье. \n",
    "\n",
    "3. Слой предсказания. \n",
    "\n",
    "Предложите, как можно было модифицировать последний слой предсказания в архитектуре DrQA, с учетом того, что итоговое предсказание – это метка yes / no, предсказание которой проще, чем предсказание спана ответа для SQuAD.\n",
    "\n",
    "Оцените качество этой модели для решения задачи. \n",
    "\n",
    "[bonus] Замените входные эмбеддинги и все дополнительные признаки, используемые кодировщиками, на BERT эмбеддинги. Улучшит ли это качество результатов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get dataset + tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T09:52:22.002633Z",
     "start_time": "2023-05-20T09:52:21.994254Z"
    }
   },
   "outputs": [],
   "source": [
    "Sample = namedtuple(\"Sample\", \"question, passage, labels\")\n",
    "\n",
    "def get_dataset(path_file: list):\n",
    "    samples = []\n",
    "    file = pd.read_json(path_file, lines=True, orient='records')\n",
    "    question = file.question.tolist()\n",
    "    passage = file.passage.tolist()\n",
    "    labels = file.answer.astype(int)\n",
    "    \n",
    "    for i in tqdm(range(len(file))):\n",
    "        #question\n",
    "        question_i = []\n",
    "        for sent in sentenize(question[i]):\n",
    "            sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "            question_i.extend(sent_i)\n",
    "        \n",
    "        #question\n",
    "        passage_i = []\n",
    "        for sent in sentenize(passage[i]):\n",
    "            sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "            passage_i.extend(sent_i)\n",
    "    \n",
    "        sample = Sample(question_i, passage_i, labels[i])\n",
    "        samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T09:52:39.926297Z",
     "start_time": "2023-05-20T09:52:27.852531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9427/9427 [00:08<00:00, 1052.38it/s]\n",
      "100%|█████████████████████████████████████| 3270/3270 [00:02<00:00, 1124.04it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_dataset('train.jsonl')\n",
    "random.shuffle(data)\n",
    "\n",
    "train_size = int(len(data)*0.8)\n",
    "train = data[:train_size]\n",
    "val = data[train_size:]\n",
    "test = get_dataset('dev.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T09:52:42.462346Z",
     "start_time": "2023-05-20T09:52:42.331767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42113"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = list( {token for sample in train for token in sample.question} |\n",
    "                 {token for sample in train for token in sample.passage}\n",
    "               )\n",
    "word_set.insert(0, '<unk>'), word_set.insert(0, '<pad>')\n",
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T12:15:21.819603Z",
     "start_time": "2023-05-20T12:15:21.813212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  7, 10]),)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [4,23,46,21367,7,234,7,4,1,34,4]\n",
    "np.where(np.array(a) == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:37:54.791631Z",
     "start_time": "2023-05-20T19:37:54.773430Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_gen_batch(samples, max_qst_len=50, max_psg_len=512, batch_size=32):\n",
    "    indices = np.arange(len(samples))\n",
    "    np.random.shuffle(indices)\n",
    "    batch_begin = 0\n",
    "    with tqdm(total=len(samples)) as pbar:\n",
    "        while batch_begin < len(samples):\n",
    "            batch_indices = indices[batch_begin: batch_begin + 32]\n",
    "            batch_qst = []\n",
    "            batch_psg = []\n",
    "            batch_psg_fts = []\n",
    "            batch_labels = []\n",
    "            batch_qst_mask = torch.ByteTensor(len(batch_indices), max_qst_len).fill_(1)\n",
    "            batch_psg_mask = torch.ByteTensor(len(batch_indices), max_psg_len).fill_(1)\n",
    "            for data_ind in batch_indices:\n",
    "                ind = list(batch_indices).index(data_ind)\n",
    "                \n",
    "                sample = samples[data_ind]\n",
    "                #вопрос\n",
    "                question = torch.zeros(max_qst_len, dtype=torch.long) #.cuda()\n",
    "                for token_num, token in enumerate(sample.question[:max_qst_len]):\n",
    "                    question[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                question_len = len(sample.question[:max_qst_len])\n",
    "                batch_qst_mask[ind, :question_len].fill_(0)\n",
    "                #параграф\n",
    "                passage = torch.zeros(max_psg_len, dtype=torch.long) #.cuda()\n",
    "                passage_features = torch.zeros(max_psg_len, max_qst_len, dtype=torch.long)\n",
    "                for token_num, token in enumerate(sample.passage[:max_psg_len]):\n",
    "                    passage[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                    q_inds = np.where(np.array(sample.question[:max_qst_len]) == token)\n",
    "                    for q_ind in q_inds:\n",
    "                        passage_features[token_num, q_ind] = 1\n",
    "                passage_len = len(sample.passage[:max_psg_len])\n",
    "                batch_psg_mask[ind, :passage_len].fill_(0)\n",
    "                    \n",
    "                labels = sample.labels\n",
    "\n",
    "                batch_qst.append(question)\n",
    "                batch_psg.append(passage)\n",
    "                batch_psg_fts.append(passage_features)\n",
    "                batch_labels.append(labels)\n",
    "              \n",
    "            batch_begin += batch_size\n",
    "            pbar.update(batch_size)\n",
    "          \n",
    "            batch_qst = torch.stack(batch_qst)\n",
    "            batch_psg = torch.stack(batch_psg)\n",
    "            batch_psg_fts = torch.stack(batch_psg_fts)\n",
    "            batch_labels = torch.LongTensor(batch_labels)\n",
    "            yield batch_indices, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T12:16:48.890907Z",
     "start_time": "2023-05-20T12:16:43.605426Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                        | 32/7541 [00:05<20:38,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch_qst            torch.Size([32, 50])\n",
      "batch_qst_mask       torch.Size([32, 50])\n",
      "batch_psg            torch.Size([32, 512])\n",
      "batch_psg_fts        torch.Size([32, 512, 50])\n",
      "passage_mask         torch.Size([32, 512])\n",
      "batch_labels         torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels in get_next_gen_batch(train):\n",
    "    1+1\n",
    "    break\n",
    "print()\n",
    "print( 'batch_qst'.ljust(20), batch_qst.size() )\n",
    "print( 'batch_qst_mask'.ljust(20), batch_qst_mask.size() )\n",
    "print( 'batch_psg'.ljust(20), batch_psg.size() )\n",
    "print( 'batch_psg_fts'.ljust(20), batch_psg_fts.size() )\n",
    "print( 'passage_mask'.ljust(20), batch_psg_mask.size() )\n",
    "print( 'batch_labels'.ljust(20), batch_labels.size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:21:55.077211Z",
     "start_time": "2023-05-20T22:21:55.050971Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/DrQA/tree/main\n",
    "class SeqAttnMatch(nn.Module):\n",
    "    \"\"\"Given sequences X and Y, match sequence Y to each element in X.\n",
    "\n",
    "    * o_i = sum(alpha_j * y_j) for i in X\n",
    "    * alpha_j = softmax(y_j * x_i)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, identity=False):\n",
    "        super(SeqAttnMatch, self).__init__()\n",
    "        if not identity:\n",
    "            self.linear = nn.Linear(input_size, input_size)\n",
    "        else:\n",
    "            self.linear = None\n",
    "\n",
    "    def forward(self, x, y, y_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch * len1 * hdim\n",
    "            y: batch * len2 * hdim\n",
    "            y_mask: batch * len2 (1 for padding, 0 for true)\n",
    "        Output:\n",
    "            matched_seq: batch * len1 * hdim\n",
    "        \"\"\"\n",
    "        # Project vectors\n",
    "        if self.linear:\n",
    "            x_proj = self.linear(x.view(-1, x.size(2))).view(x.size())\n",
    "            x_proj = F.relu(x_proj)\n",
    "            y_proj = self.linear(y.view(-1, y.size(2))).view(y.size())\n",
    "            y_proj = F.relu(y_proj)\n",
    "        else:\n",
    "            x_proj = x\n",
    "            y_proj = y\n",
    "\n",
    "        # Compute scores\n",
    "        scores = x_proj.bmm(y_proj.transpose(2, 1))\n",
    "\n",
    "        # Mask padding\n",
    "        y_mask = y_mask.unsqueeze(1).expand(scores.size())\n",
    "        scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
    "\n",
    "        # Normalize with softmax\n",
    "        alpha_flat = F.softmax(scores.view(-1, y.size(1)), dim=-1)\n",
    "        alpha = alpha_flat.view(-1, x.size(1), y.size(1))\n",
    "\n",
    "        # Take weighted average\n",
    "        matched_seq = alpha.bmm(y)\n",
    "        return matched_seq\n",
    "\n",
    "class LinearSeqAttn(nn.Module):\n",
    "    \"\"\"Self attention over a sequence:\n",
    "\n",
    "    * o_i = softmax(Wx_i) for x_i in X.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearSeqAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch * len * hdim\n",
    "            x_mask: batch * len (1 for padding, 0 for true)\n",
    "        Output:\n",
    "            alpha: batch * len\n",
    "        \"\"\"\n",
    "        x_flat = x.view(-1, x.size(-1))\n",
    "        scores = self.linear(x_flat).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, dim=-1)\n",
    "        return alpha\n",
    "    \n",
    "#------------------------------------------------------------------------------------------\n",
    "    \n",
    "class DrQA_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 word_set_size,\n",
    "                 question_max_size = 50,\n",
    "                 passage_max_size = 512,\n",
    "                 word_embedding_dim=128,\n",
    "                 lstm_embedding_dim = 24,\n",
    "                 classes_count=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qemb_match = SeqAttnMatch(word_embedding_dim)\n",
    "        self.self_attn = LinearSeqAttn(2*lstm_embedding_dim)\n",
    "        self.embedding = nn.Embedding(word_set_size, word_embedding_dim)\n",
    "        self.psg_rnn = nn.LSTM(2*word_embedding_dim+question_max_size, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.qst_rnn = nn.LSTM(word_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.lin = nn.Linear(passage_max_size+1, classes_count)\n",
    "\n",
    "    def forward(self, questions, question_mask, passages, passage_features, passage_mask):\n",
    "        #эмбеддинги слов w2v\n",
    "        q_emb = self.embedding(questions)\n",
    "        p_emb = self.embedding(passages)\n",
    "        # выровненный эмбеддинг вопроса, получаемый с использованием \n",
    "        # soft attention между эмбеддингами слов из абзаца и эмбеддингом вопроса\n",
    "        q_weighted_emb = self.qemb_match(p_emb, q_emb, question_mask)\n",
    "        # дополнительных признаков-индикаторов, кодирующих в виде one-hot \n",
    "        # векторов встречается ли слово в вопросе или нет\n",
    "        drnn_input = [p_emb]\n",
    "        drnn_input.append(q_weighted_emb)\n",
    "        drnn_input.append(passage_features)\n",
    "        drnn_input = torch.cat(drnn_input, 2)\n",
    "        \n",
    "        # 1) paragraph encoding (LSTM)\n",
    "        #    packs a Tensor containing padded sequences of variable length.\n",
    "        #    compute sorted sequence lengths\n",
    "        lengths = passage_mask.data.eq(0).long().sum(1)\n",
    "        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        lengths = list(lengths[idx_sort])\n",
    "        #    ---\n",
    "        p_rnn_input = drnn_input.index_select(0, idx_sort) # sort x\n",
    "        p_rnn_input = nn.utils.rnn.pack_padded_sequence(p_rnn_input, lengths, batch_first=True) # pack it up\n",
    "        #    ---\n",
    "        p_hidden, (final_hidden_state, final_cell_state) = self.psg_rnn(p_rnn_input) #LSTM\n",
    "        p_output = nn.utils.rnn.pad_packed_sequence(p_hidden, batch_first=True)[0] # unpack \n",
    "        p_output = p_output.index_select(0, idx_unsort) # unsort\n",
    "        #    pad up to original batch sequence length\n",
    "        if p_output.size(1) != passage_mask.size(1):\n",
    "            padding = torch.zeros(p_output.size(0), \n",
    "                                  passage_mask.size(1) - p_output.size(1),\n",
    "                                  p_output.size(2)).type(p_output.data.type())\n",
    "            p_output = torch.cat([p_output, padding], 1)\n",
    "        \n",
    "        # 2) question encoding (LSTM)\n",
    "        lengths = question_mask.data.eq(0).long().sum(1)\n",
    "        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        lengths = list(lengths[idx_sort])\n",
    "        #    ---\n",
    "        q_rnn_input = q_emb.index_select(0, idx_sort) # sort x\n",
    "        q_rnn_input = nn.utils.rnn.pack_padded_sequence(q_rnn_input, lengths, batch_first=True) # pack it up\n",
    "        #    ---\n",
    "        q_hidden, (final_hidden_state, final_cell_state) = self.qst_rnn(q_rnn_input) #LSTM\n",
    "        q_hidden = nn.utils.rnn.pad_packed_sequence(q_hidden, batch_first=True)[0]   # unpack \n",
    "        q_hidden = q_hidden.index_select(0, idx_unsort) # unsort\n",
    "        #    pad up to original batch sequence length\n",
    "        if q_hidden.size(1) != question_mask.size(1):\n",
    "            padding = torch.zeros(q_hidden.size(0), \n",
    "                                  question_mask.size(1) - q_hidden.size(1),\n",
    "                                  q_hidden.size(2)).type(q_hidden.data.type())\n",
    "            q_hidden = torch.cat([q_hidden, padding], 1)\n",
    "        #    взвешенное представление вопроса\n",
    "        q_weights = self.self_attn(q_hidden, question_mask)\n",
    "        #    return a weighted average of x (a sequence of vectors)\n",
    "        q_output = q_weights.unsqueeze(1).bmm(q_hidden).squeeze(1)\n",
    "\n",
    "        # 3) concat + linear\n",
    "        q_output = q_output.unsqueeze(1)\n",
    "        output = torch.cat([p_output, q_output], 1)\n",
    "        output = output.mean(-1)\n",
    "        output = self.lin(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:21:58.860711Z",
     "start_time": "2023-05-20T22:21:58.389682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:39: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:69: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(x_mask.data, -float('inf'))\n"
     ]
    }
   ],
   "source": [
    "model = DrQA_model(len(word_set))\n",
    "\n",
    "lr=0.01\n",
    "device_name=\"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "\n",
    "logits = model(batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask) # Прямой проход\n",
    "loss = loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "loss.backward() # Подсчёт градиентов dL/dw\n",
    "optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:28:10.961328Z",
     "start_time": "2023-05-20T22:28:10.948109Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n",
    "                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cpu\",\n",
    "                    early_stopping=True):\n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss()#.cuda()\n",
    "    prev_avg_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        model.train()\n",
    "        for step, (_, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels) in enumerate(get_next_gen_batch(train)):\n",
    "            logits = model(batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask) # Прямой проход\n",
    "            loss = loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            loss.backward() # Подсчёт градиентов dL/dw\n",
    "            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n",
    "            total_loss += loss.item()\n",
    "        val_total_loss = 0\n",
    "        val_batch_count = 0\n",
    "        model.eval()\n",
    "        for _, (_, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels) in enumerate(get_next_gen_batch(val)):\n",
    "            logits = model(batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask) # Прямой проход\n",
    "            val_total_loss += loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            val_batch_count += 1\n",
    "        avg_val_loss = val_total_loss/val_batch_count\n",
    "        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.eval()\n",
    "            break\n",
    "        prev_avg_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:57:01.944642Z",
     "start_time": "2023-05-20T22:28:11.838806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 5501365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                        | 32/7541 [00:04<16:32,  7.57it/s]/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:39: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:69: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
      "7552it [23:03,  5.46it/s]                                                       \n",
      "1888it [05:45,  5.46it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Avg Train Loss = 1.5385, Avg val loss = 0.6367, Time = 1728.65s\n"
     ]
    }
   ],
   "source": [
    "model = DrQA_model(len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T23:03:19.522177Z",
     "start_time": "2023-05-20T23:03:19.510445Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_precision_and_recall(true_positive, false_positive, false_negative):\n",
    "    \"\"\"\n",
    "    Вычисляем точность и полноту по TP, FP и FN\n",
    "    \"\"\"\n",
    "    if false_positive + true_positive > 0:\n",
    "        precision = float(true_positive) / (true_positive + false_positive)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if false_negative + true_positive > 0:\n",
    "        recall = float(true_positive) / (true_positive + false_negative)\n",
    "    else:\n",
    "        recall = 0\n",
    "    return recall, precision\n",
    "\n",
    "def predict(model, samples):\n",
    "    model.eval()\n",
    "    tp, fp, fn = 0,0,0\n",
    "    for _, (indices, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels) in enumerate(get_next_gen_batch(samples)):\n",
    "        logits = model(batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask)\n",
    "        plabels = logits.max(dim=1)[1]\n",
    "        tp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j!=0) and (i==j)])\n",
    "        fp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j!=0) and (i!=j)])\n",
    "        fn += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j==0) and (i!=j)])\n",
    "\n",
    "    recall, precision = compute_precision_and_recall(tp, fp, fn)\n",
    "    try:\n",
    "        f_measure = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f_measure = 'None'\n",
    "    print('precision:', precision)\n",
    "    print('recall:   ', recall)\n",
    "    print('f1:       ', f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T23:38:22.418224Z",
     "start_time": "2023-05-20T23:03:20.887553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                        | 32/7541 [00:05<20:47,  6.02it/s]/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:39: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:69: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
      "7552it [22:20,  5.63it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7421614409606404\n",
      "recall:    0.9502455690796497\n",
      "f1:        0.8334113681056279\n",
      "\n",
      " -------on val dataset:--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888it [04:41,  6.70it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6569435637285986\n",
      "recall:    0.869857262804366\n",
      "f1:        0.7485549132947977\n",
      "\n",
      " -------on test dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3296it [07:59,  6.88it/s]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6549426138467235\n",
      "recall:    0.8701426463354648\n",
      "f1:        0.7473595268272074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. [3 балла] BiDAF-подобная архитектура\n",
    "\n",
    "Основана на статье: Bidirectional Attention Flow for Machine Comprehension\n",
    "\n",
    "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "\n",
    "https://arxiv.org/abs/1611.01603\n",
    "\n",
    "Архитектура BiDAF предложена для задачи SQuAD, но легко может быть адаптирована к текущему заданию. Модель состоит из следующих блоков:\n",
    "1. Кодировщик  получает на вход два представления слова: эмбеддинг слова и полученное из CNN посимвольное представление слова. Кодировщики для вопроса и для параграфа одинаковы. \n",
    "2. Слой внимания (детальное описание приведено в статье, см. пункт Attention Flow Layer)\n",
    "3. Промежуточный слой, который получает на вход контекстуализированные эмбеддинги слов из параграфа, состоящие из трех частей (выход кодировщика параграфа,   Query2Context (один вектор) и Context2Query (матрица) выравнивания\n",
    "\n",
    "4. Слой предсказания. \n",
    "\n",
    "Предложите, как можно было модифицировать последний слой предсказания в архитектуре BiDAF, с учетом того, что итоговое предсказание – это метка yes / no, предсказание которой проще, чем предсказание спана ответа для SQuAD.\n",
    "\n",
    "Оцените качество этой модели для решения задачи. \n",
    "\n",
    "[bonus] Замените входные эмбеддинги и все дополнительные признаки, используемые кодировщиками, на BERT эмбеддинги. Улучшит ли это качество результатов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T07:16:08.315087Z",
     "start_time": "2023-05-21T07:16:08.032794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = list( {ch for sample in train for token in sample.question for ch in token} |\n",
    "                 {ch for sample in train for token in sample.passage for ch in token}\n",
    "               )\n",
    "char_set.insert(0, '<unk>'), char_set.insert(0, '<pad>')\n",
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:23:43.530297Z",
     "start_time": "2023-05-21T08:23:43.515153Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_gen_batch(samples, max_qst_len=50, max_psg_len=512, max_char_seq_len=40, batch_size=32):\n",
    "    indices = np.arange(len(samples))\n",
    "    np.random.shuffle(indices)\n",
    "    batch_begin = 0\n",
    "    with tqdm(total=len(samples)) as pbar:\n",
    "        while batch_begin < len(samples):\n",
    "            batch_indices = indices[batch_begin: batch_begin + 32]\n",
    "            batch_qst = []\n",
    "            batch_qst_ch = []\n",
    "            batch_psg = []\n",
    "            batch_psg_ch = []\n",
    "            batch_labels = []\n",
    "            for data_ind in batch_indices:\n",
    "                ind = list(batch_indices).index(data_ind)\n",
    "                sample = samples[data_ind]\n",
    "                \n",
    "                #вопрос\n",
    "                question = torch.zeros(max_qst_len, dtype=torch.long) #.cuda()\n",
    "                qst_chars = torch.zeros((max_qst_len, max_char_seq_len), dtype=torch.long)#.cuda()\n",
    "                for token_num, token in enumerate(sample.question[:max_qst_len]):\n",
    "                    #слова\n",
    "                    question[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                    #символы\n",
    "                    for char_num, char in enumerate(token[:max_char_seq_len]):\n",
    "                        qst_chars[token_num][char_num] = char_set.index(char) if char in char_set else char_set.index('<unk>')\n",
    "                \n",
    "                #параграф\n",
    "                passage = torch.zeros(max_psg_len, dtype=torch.long) #.cuda()\n",
    "                psg_chars = torch.zeros((max_psg_len, max_char_seq_len), dtype=torch.long)#.cuda()\n",
    "                for token_num, token in enumerate(sample.passage[:max_psg_len]):\n",
    "                    #слова\n",
    "                    passage[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                    #символы\n",
    "                    for char_num, char in enumerate(token[:max_char_seq_len]):\n",
    "                        psg_chars[token_num][char_num] = char_set.index(char) if char in char_set else char_set.index('<unk>')\n",
    "                \n",
    "                labels = sample.labels\n",
    "\n",
    "                batch_qst.append(question)\n",
    "                batch_qst_ch.append(qst_chars)\n",
    "                batch_psg.append(passage)\n",
    "                batch_psg_ch.append(psg_chars)\n",
    "                batch_labels.append(labels)\n",
    "              \n",
    "            batch_begin += batch_size\n",
    "            pbar.update(batch_size)\n",
    "          \n",
    "            batch_qst = torch.stack(batch_qst)\n",
    "            batch_psg = torch.stack(batch_psg)\n",
    "            batch_qst_ch = torch.stack(batch_qst_ch)\n",
    "            batch_psg_ch = torch.stack(batch_psg_ch)\n",
    "            batch_labels = torch.LongTensor(batch_labels)\n",
    "            yield batch_indices, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:23:48.648689Z",
     "start_time": "2023-05-21T08:23:44.201330Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                        | 32/7541 [00:04<17:20,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch_qst            torch.Size([32, 50])\n",
      "batch_qst_mask       torch.Size([32, 50, 40])\n",
      "batch_psg            torch.Size([32, 512])\n",
      "batch_psg_fts        torch.Size([32, 512, 40])\n",
      "batch_labels         torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels in get_next_gen_batch(train):\n",
    "    1+1\n",
    "    break\n",
    "print()\n",
    "print( 'batch_qst'.ljust(20), batch_qst.size() )\n",
    "print( 'batch_qst_mask'.ljust(20), batch_qst_ch.size() )\n",
    "print( 'batch_psg'.ljust(20), batch_psg.size() )\n",
    "print( 'batch_psg_fts'.ljust(20), batch_psg_ch.size() )\n",
    "print( 'batch_labels'.ljust(20), batch_labels.size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T09:49:53.957944Z",
     "start_time": "2023-05-21T09:49:53.939007Z"
    },
    "code_folding": [
     27
    ]
   },
   "outputs": [],
   "source": [
    "class BiDAF_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 char_set_size, word_set_size,\n",
    "                 question_max_size = 50, passage_max_size = 512,\n",
    "                 char_embedding_dim=8, word_embedding_dim=64,\n",
    "                 lstm_embedding_dim = 24, \n",
    "                 char_max_seq_len=40, kernel_size=3,\n",
    "                 classes_count=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        self.char_max_seq_len = char_max_seq_len\n",
    "        self.char_embedding = nn.Embedding(char_set_size, char_embedding_dim)\n",
    "        self.char_cnn = nn.Conv1d(in_channels=char_embedding_dim, out_channels=1, kernel_size=kernel_size)\n",
    "        self.embedding = nn.Embedding(word_set_size, word_embedding_dim)\n",
    "        self.context_LSTM = nn.LSTM(word_embedding_dim+char_max_seq_len-(kernel_size-1), lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.modeling_LSTM1 = nn.LSTM(8*lstm_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.modeling_LSTM2 = nn.LSTM(2*lstm_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.lin = nn.Linear(passage_max_size, classes_count)\n",
    "\n",
    "        # Attention Flow Layer\n",
    "        self.att_weight_c = nn.Linear(2*lstm_embedding_dim, 1)\n",
    "        self.att_weight_q = nn.Linear(2*lstm_embedding_dim, 1)\n",
    "        self.att_weight_cq = nn.Linear(2*lstm_embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, questions, question_chars, passages, passage_chars):\n",
    "        # https://github.com/galsang/BiDAF-pytorch/tree/master\n",
    "        def att_flow_layer(c, q):\n",
    "            \"\"\"\n",
    "            :param c: (batch, c_len, hidden_size * 2)\n",
    "            :param q: (batch, q_len, hidden_size * 2)\n",
    "            :return: (batch, c_len, q_len)\n",
    "            \"\"\"\n",
    "            c_len = c.size(1)\n",
    "            q_len = q.size(1)\n",
    "\n",
    "            cq = []\n",
    "            for i in range(q_len):\n",
    "                #(batch, 1, hidden_size * 2)\n",
    "                qi = q.select(1, i).unsqueeze(1)\n",
    "                #(batch, c_len, 1)\n",
    "                ci = self.att_weight_cq(c * qi).squeeze()\n",
    "                cq.append(ci)\n",
    "            # (batch, c_len, q_len)\n",
    "            cq = torch.stack(cq, dim=-1)\n",
    "\n",
    "            # (batch, c_len, q_len)\n",
    "            s = self.att_weight_c(c).expand(-1, -1, q_len) + \\\n",
    "                self.att_weight_q(q).permute(0, 2, 1).expand(-1, c_len, -1) + \\\n",
    "                cq\n",
    "\n",
    "            # (batch, c_len, q_len)\n",
    "            a = F.softmax(s, dim=2)\n",
    "            # (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -> (batch, c_len, hidden_size * 2)\n",
    "            c2q_att = torch.bmm(a, q)\n",
    "            # (batch, 1, c_len)\n",
    "            b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)\n",
    "            # (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -> (batch, hidden_size * 2)\n",
    "            q2c_att = torch.bmm(b, c).squeeze()\n",
    "            # (batch, c_len, hidden_size * 2) (tiled)\n",
    "            q2c_att = q2c_att.unsqueeze(1).expand(-1, c_len, -1)\n",
    "            # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
    "\n",
    "            # (batch, c_len, hidden_size * 8)\n",
    "            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=-1)\n",
    "            return x\n",
    "        \n",
    "        # эмбеддинги слов w2v\n",
    "        q_emb = self.embedding(questions)\n",
    "        p_emb = self.embedding(passages)\n",
    "        # полученное из CNN посимвольное представление слова\n",
    "        q_c_emb = self.char_embedding(question_chars)\n",
    "        q_c_emb = q_c_emb.reshape(q_c_emb.size(0)*q_c_emb.size(1), self.char_max_seq_len, self.char_embedding_dim)\n",
    "        q_c_emb = q_c_emb.permute(0,2,1)\n",
    "        q_c_cnn = self.char_cnn(q_c_emb)\n",
    "        q_c_cnn = q_c_cnn.reshape(question_chars.size(0), question_chars.size(1), -1)\n",
    "        #----\n",
    "        p_c_emb = self.char_embedding(passage_chars)\n",
    "        p_c_emb = p_c_emb.reshape(p_c_emb.size(0)*p_c_emb.size(1), self.char_max_seq_len, self.char_embedding_dim)\n",
    "        p_c_emb = p_c_emb.permute(0,2,1)\n",
    "        p_c_cnn = self.char_cnn(p_c_emb)\n",
    "        p_c_cnn = p_c_cnn.reshape(passage_chars.size(0), passage_chars.size(1), -1)\n",
    "        # объединение ембедингов слов и символов\n",
    "        q = torch.cat([q_c_cnn, q_emb], dim=-1)\n",
    "        p = torch.cat([p_c_cnn, p_emb], dim=-1)\n",
    "        # контекстная рекуррентная сеть\n",
    "        q = self.context_LSTM(q)[0]\n",
    "        p = self.context_LSTM(p)[0]\n",
    "        # Attention Flow Layer\n",
    "        g = att_flow_layer(p, q)\n",
    "        # LSTM modeling\n",
    "        rnn1 = self.modeling_LSTM1(g)[0]\n",
    "        rnn2 = self.modeling_LSTM2(rnn1)[0]\n",
    "        # output\n",
    "        output = rnn2.mean(-1)\n",
    "        output = self.lin(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:23:51.933150Z",
     "start_time": "2023-05-21T08:23:50.578087Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BiDAF_model(len(char_set), len(word_set))\n",
    "\n",
    "lr=0.01\n",
    "device_name=\"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "\n",
    "logits = model(batch_qst, batch_qst_ch, batch_psg, batch_psg_ch) # Прямой проход\n",
    "loss = loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "loss.backward() # Подсчёт градиентов dL/dw\n",
    "optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:24:10.392627Z",
     "start_time": "2023-05-21T08:24:10.382964Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n",
    "                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cpu\",\n",
    "                    early_stopping=True):\n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss()#.cuda()\n",
    "    prev_avg_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        model.train()\n",
    "        for step, (_, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels) in enumerate(get_next_gen_batch(train)):\n",
    "            logits = model(batch_qst, batch_qst_ch, batch_psg, batch_psg_ch) # Прямой проход\n",
    "            loss = loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            loss.backward() # Подсчёт градиентов dL/dw\n",
    "            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n",
    "            total_loss += loss.item()\n",
    "        val_total_loss = 0\n",
    "        val_batch_count = 0\n",
    "        model.eval()\n",
    "        for _, (_, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels) in enumerate(get_next_gen_batch(val)):\n",
    "            logits = model(batch_qst, batch_qst_ch, batch_psg, batch_psg_ch) # Прямой проход\n",
    "            val_total_loss += loss_function(logits, batch_labels) # Подсчёт ошибки\n",
    "            val_batch_count += 1\n",
    "        avg_val_loss = val_total_loss/val_batch_count\n",
    "        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.eval()\n",
    "            break\n",
    "        prev_avg_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:56:28.020787Z",
     "start_time": "2023-05-21T08:24:11.149426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 2782102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [25:51,  4.87it/s]                                                       \n",
      "1888it [05:49,  5.41it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Avg Train Loss = 1.6096, Avg val loss = 0.6590, Time = 1900.84s\n"
     ]
    }
   ],
   "source": [
    "model = BiDAF_model(len(char_set), len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T09:02:43.843548Z",
     "start_time": "2023-05-21T09:02:43.835714Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, samples):\n",
    "    model.eval()\n",
    "    tp, fp, fn = 0,0,0\n",
    "    for _, (indices, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels) in enumerate(get_next_gen_batch(samples)):\n",
    "        logits = model(batch_qst, batch_qst_ch, batch_psg, batch_psg_ch)\n",
    "        plabels = logits.max(dim=1)[1]\n",
    "        tp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j!=0) and (i==j)])\n",
    "        fp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j!=0) and (i!=j)])\n",
    "        fn += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j==0) and (i!=j)])\n",
    "\n",
    "    recall, precision = compute_precision_and_recall(tp, fp, fn)\n",
    "    try:\n",
    "        f_measure = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f_measure = 'None'\n",
    "    print('precision:', precision)\n",
    "    print('recall:   ', recall)\n",
    "    print('f1:       ', f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T09:36:14.950363Z",
     "start_time": "2023-05-21T09:02:44.802404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [19:54,  6.32it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6210051717278875\n",
      "recall:    1.0\n",
      "f1:        0.7661976439790575\n",
      "\n",
      " -------on val dataset:--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888it [04:55,  6.38it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6314952279957582\n",
      "recall:    1.0\n",
      "f1:        0.7741306467338316\n",
      "\n",
      " -------on test dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3296it [08:39,  6.34it/s]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6217125382262997\n",
      "recall:    1.0\n",
      "f1:        0.7667358099189139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*модель не обучилась - всегда один ответ*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### попытка №2 (BiDAF) -> smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T10:21:11.234821Z",
     "start_time": "2023-05-21T09:51:20.373629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 2782102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [23:15,  5.41it/s]                                                       \n",
      "1888it [06:05,  5.16it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Avg Train Loss = 1.5674, Avg val loss = 0.6579, Time = 1760.89s\n"
     ]
    }
   ],
   "source": [
    "model = BiDAF_model(len(char_set), len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T10:56:04.826082Z",
     "start_time": "2023-05-21T10:21:11.248213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [20:37,  6.10it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6219090667375698\n",
      "recall:    0.9989323083493488\n",
      "f1:        0.7665710774272839\n",
      "\n",
      " -------on val dataset:--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888it [05:04,  6.21it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6312997347480106\n",
      "recall:    0.9991603694374476\n",
      "f1:        0.7737321196358907\n",
      "\n",
      " -------on test dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3296it [09:11,  5.97it/s]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6215554194733619\n",
      "recall:    0.9985243482538121\n",
      "f1:        0.7661822985468958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### увы не помогло ☝️😔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение DrQA и BiDAF:\n",
    "    \n",
    "![](https://www.researchgate.net/profile/Felix_Wu6/publication/321069852/figure/fig1/AS:560800147881984@1510716582560/Schematic-layouts-of-the-BiDAF-left-and-DrQA-right-architectures-We-propose-to.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 5. [1 балл] Итоги\n",
    "Напишите краткое резюме проделанной работы. Сравните результаты всех разработанных моделей. Что помогло вам в выполнении работы, чего не хватало?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** baseline = 0.6291 (fasttext на текстах, состоящих из склееных вопросов и абзацев) <br>\n",
    "   низкое качество, поскольку архитектура в силу своей простоты не может выявить  <br>\n",
    "   закономерности выборки  <br>\n",
    "**2)** bert = 0.6067 <br>\n",
    "   я бы сказал, что проблема та же - конкатенация эмбедингов абзаца и вопроса не  <br>\n",
    "   позволяет учесть взаимосвязь между ними  <br>\n",
    "**3)** DrQA = 0.6549 <br>\n",
    "   возможно можно добавить механизм внимания выходами рекуррентных сетей над <br>\n",
    "   вопросом и абзацем <br>\n",
    "**4)** BiDAF = 0.6217 <br>\n",
    "   модель не обучилась - в качестве предсказания выдает самый частый ответ выборки\n",
    "   возможно можно обойтись одной рекуррентной сетью перед финальным линейным слоем\n",
    "<br>\n",
    "<br>\n",
    "модель DrQA показала, что механизм внимания между абзацем и <br>\n",
    "вопросом позвоялет лучше обучаться сети на предсказние ответа, <br>\n",
    "однако в случае с BiDAF обучить сеть не удалось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip', \n",
    "#                            'pre-trained-fasttext.zip')\n",
    "\n",
    "# from zipfile import ZipFile\n",
    "# with ZipFile('pre-trained-fasttext.zip', 'r') as f:\n",
    "#     f.extractall('pre-trained-fasttext')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

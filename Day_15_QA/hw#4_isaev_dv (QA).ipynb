{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:23:01.316099Z",
     "start_time": "2023-05-20T22:23:01.309268Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, namedtuple\n",
    "import urllib.request\n",
    "from razdel import tokenize, sentenize\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import itertools\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ\n",
    "## Yes/No Questions\n",
    "\n",
    "–í —ç—Ç–æ–º –¥–æ–º–∞—à–Ω–µ–º –∑–∞–¥–∞–Ω–∏–∏ –≤—ã –±—É–¥–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∫–æ—Ä–ø—É—Å–æ–º BoolQ. –ö–æ—Ä–ø—É—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–æ–ø—Ä–æ—Å–æ–≤, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—â–∏—Ö –±–∏–Ω–∞—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç (–¥–∞ / –Ω–µ—Ç), –∞–±–∑–∞—Ü–µ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏,  —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç–∞—Ç—å–∏, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –∏–∑–≤–ª–µ—á–µ–Ω –∞–±–∑–∞—Ü –∏ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–≤–µ—Ç–∞ (true / false).\n",
    "\n",
    "–ö–æ—Ä–ø—É—Å –æ–ø–∏—Å–∞–Ω –≤ —Å—Ç–∞—Ç—å–µ:\n",
    "\n",
    "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova\n",
    "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n",
    "\n",
    "https://arxiv.org/abs/1905.10044\n",
    "\n",
    "\n",
    "–ö–æ—Ä–ø—É—Å (train-dev split) –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞:  https://github.com/google-research-datasets/boolean-questions\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è train —á–∞—Å—Ç—å –∫–æ—Ä–ø—É—Å–∞, –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äì dev —á–∞—Å—Ç—å. \n",
    "\n",
    "–ö–∞–∂–¥—ã–π –±–æ–Ω—É—Å –ø—É–Ω–∫—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—Å—è –≤ 1 –±–∞–ª–ª. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∏–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞: \n",
    "question: is batman and robin a sequel to batman forever\n",
    "\n",
    "title: Batman & Robin (film)\n",
    "\n",
    "answer: true\n",
    "\n",
    "passage: With the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. They hired director Joel Schumacher and writer Akiva Goldsman to reprise their duties the following August, and decided it was best to fast track production for a June 1997 target release date, which is a break from the usual 3-year gap between films. Schumacher wanted to homage both the broad camp style of the 1960s television series and the work of Dick Sprang. The storyline of Batman & Robin was conceived by Schumacher and Goldsman during pre-production on A Time to Kill. Portions of Mr. Freeze's back-story were based on the Batman: The Animated Series episode ''Heart of Ice'', written by Paul Dini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–†–ê–í–ò–õ–ê\n",
    "1. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –≥—Ä—É–ø–ø–µ –¥–æ 2-—Ö —á–µ–ª–æ–≤–µ–∫.\n",
    "2. –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –æ—Ñ–æ—Ä–º–ª—è–µ—Ç—Å—è –≤ –≤–∏–¥–µ –æ—Ç—á–µ—Ç–∞ –≤ ipython-—Ç–µ—Ç—Ä–∞–¥–∫–µ. \n",
    "3. –û—Ç—á–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å: –Ω—É–º–µ—Ä–∞—Ü–∏—é –∑–∞–¥–∞–Ω–∏–π –∏ –ø—É–Ω–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –≤—ã–ø–æ–ª–Ω–∏–ª–∏, –∫–æ–¥ —Ä–µ—à–µ–Ω–∏—è, –∏ –ø–æ–Ω—è—Ç–Ω–æ–µ –ø–æ—à–∞–≥–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –≤—ã —Å–¥–µ–ª–∞–ª–∏. –û—Ç—á–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–º —Å—Ç–∏–ª–µ, –±–µ–∑ –∏–∑–ª–∏—à–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–µ–Ω–≥–∞ –∏ —Å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ–º –Ω–æ—Ä–º —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\n",
    "4. –ù–µ —Å—Ç–æ–∏—Ç –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ª–µ–∫—Ü–∏–π, —Å—Ç–∞—Ç–µ–π –∏ –í–∏–∫–∏–ø–µ–¥–∏–∏ –≤ –≤–∞—à –æ—Ç—á–µ—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 1. [1 –±–∞–ª–ª] –≠–∫—Å–ø–ª–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "1. –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –¥–æ–ª—é yes –∏ no –∫–ª–∞—Å—Å–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ\n",
    "2. –û—Ü–µ–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –≤–æ–ø—Ä–æ—Å–∞\n",
    "3. –û—Ü–µ–Ω–∏—Ç–µ —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞\n",
    "4. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ, –ø–æ –∫–∞–∫–∏–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º –±—ã–ª–∏ —Å–æ–±—Ä–∞–Ω—ã –≤–æ–ø—Ä–æ—Å—ã (–∏–ª–∏ –Ω–∞–π–¥–∏—Ç–µ –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∞—Ç—å–µ). –ü—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—É–π—Ç–µ, –∫–∞–∫ —ç—Ç–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ–≤–ª–∏—è–ª–∏ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ—Ä–ø—É—Å–∞. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T09:51:57.042008Z",
     "start_time": "2023-05-20T09:51:56.787343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>title</th>\n",
       "      <th>answer</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do iran and afghanistan speak the same language</td>\n",
       "      <td>Persian language</td>\n",
       "      <td>True</td>\n",
       "      <td>Persian (/Ààp…úÀêr í…ôn, - É…ôn/), also known by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do good samaritan laws protect those who help ...</td>\n",
       "      <td>Good Samaritan law</td>\n",
       "      <td>True</td>\n",
       "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is windows movie maker part of windows essentials</td>\n",
       "      <td>Windows Movie Maker</td>\n",
       "      <td>True</td>\n",
       "      <td>Windows Movie Maker (formerly known as Windows...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question                title  \\\n",
       "0    do iran and afghanistan speak the same language     Persian language   \n",
       "1  do good samaritan laws protect those who help ...   Good Samaritan law   \n",
       "2  is windows movie maker part of windows essentials  Windows Movie Maker   \n",
       "\n",
       "   answer                                            passage  \n",
       "0    True  Persian (/Ààp…úÀêr í…ôn, - É…ôn/), also known by its ...  \n",
       "1    True  Good Samaritan laws offer legal protection to ...  \n",
       "2    True  Windows Movie Maker (formerly known as Windows...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.read_json('train.jsonl', lines=True, orient='records')\n",
    "dev_data_df = pd.read_json('dev.jsonl', lines=True, orient='records')\n",
    "train_data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:27:18.453278Z",
     "start_time": "2023-05-11T15:27:18.441479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>cnt</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>5874</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>3553</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer   cnt    rt\n",
       "0    True  5874  0.62\n",
       "1   False  3553  0.38"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_1 = train_data_df.answer.value_counts()\\\n",
    "                   .reset_index()\\\n",
    "                   .rename(columns={'index':'answer', 'answer':'cnt'})\n",
    "a_1['rt'] = a_1['cnt']/a_1['cnt'].sum()\n",
    "a_1['rt'] = a_1['rt'].round(2)\n",
    "a_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:28:00.533994Z",
     "start_time": "2023-05-11T15:28:00.530310Z"
    }
   },
   "source": [
    "##### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:28:44.322252Z",
     "start_time": "2023-05-11T15:28:44.312053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.question.apply(len).mean().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:29:15.441166Z",
     "start_time": "2023-05-11T15:29:15.418927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.passage.apply(len).mean().round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) <br>\n",
    "–≤–æ–ø—Ä–æ—Å—ã –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å–æ —Å–ª–æ–≤-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:50:30.469285Z",
     "start_time": "2023-05-11T15:50:30.444815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st_qst_word</th>\n",
       "      <th>cnt</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is</td>\n",
       "      <td>4190</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can</td>\n",
       "      <td>1136</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>does</td>\n",
       "      <td>952</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>are</td>\n",
       "      <td>693</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do</td>\n",
       "      <td>664</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>did</td>\n",
       "      <td>461</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>was</td>\n",
       "      <td>335</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>has</td>\n",
       "      <td>302</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>will</td>\n",
       "      <td>181</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>91</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>have</td>\n",
       "      <td>70</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>in</td>\n",
       "      <td>35</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>were</td>\n",
       "      <td>25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>if</td>\n",
       "      <td>17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>a</td>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>when</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>could</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1st_qst_word   cnt    rt\n",
       "0            is  4190  0.44\n",
       "1           can  1136  0.12\n",
       "2          does   952  0.10\n",
       "3           are   693  0.07\n",
       "4            do   664  0.07\n",
       "5           did   461  0.05\n",
       "6           was   335  0.04\n",
       "7           has   302  0.03\n",
       "8          will   181  0.02\n",
       "9           the    91  0.01\n",
       "10         have    70  0.01\n",
       "11           in    35  0.00\n",
       "12         were    25  0.00\n",
       "13           if    17  0.00\n",
       "14            a    16  0.00\n",
       "15         what    11  0.00\n",
       "16         when    10  0.00\n",
       "17        could     9  0.00"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_4 = train_data_df.question.apply(lambda x: x.split()[0])\\\n",
    "                            .value_counts()\\\n",
    "                            .reset_index()\\\n",
    "                            .rename(columns={'index':'1st_qst_word','question':'cnt'})\n",
    "a_4 = a_4[a_4.cnt>5]\n",
    "a_4['rt'] = (a_4.cnt / len(train_data_df)).round(2)\n",
    "a_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 2. [1 –±–∞–ª–ª] Baseline\n",
    "1. –û—Ü–µ–Ω–∏—Ç–µ accuracy —Ç–æ—á–Ω–æ—Å—Ç—å —Å–æ–≤—Å–µ–º –ø—Ä–æ—Å—Ç–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: –ø—Ä–∏—Å–≤–æ–∏—Ç—å –∫–∞–∂–¥–æ–π –ø–∞—Ä–µ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –≤ dev —á–∞—Å—Ç–∏ —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å –∏–∑ train —á–∞—Å—Ç–∏\n",
    "2. –û—Ü–µ–Ω–∏—Ç–µ accuracy —á—É—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: fasttext –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ —Å–∫–ª–µ–µ–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∞–±–∑–∞—Ü–µ–≤ (' '.join([question, passage]))\n",
    "\n",
    "–ü–æ—á–µ–º—É fasttext –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —ç—Ç–æ–π –∑–∞–¥–∞—á–µ–π?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:57:18.442466Z",
     "start_time": "2023-05-11T15:57:18.432946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6217125382262997"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frq_ans = Counter(train_data_df.answer).most_common(1)[0][0]\n",
    "(dev_data_df.answer == most_frq_ans).sum() / len(dev_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T15:57:07.670426Z",
     "start_time": "2023-05-11T15:57:07.662985Z"
    }
   },
   "source": [
    "##### 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/fasttext-for-text-classification-a4b38cbff27c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T15:22:52.414266Z",
     "start_time": "2023-05-12T15:22:40.127939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9427/9427 [00:08<00:00, 1099.13it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3270/3270 [00:02<00:00, 1123.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "train_text = [  ' '.join([question, passage])\n",
    "                for question, passage \n",
    "                in zip(train_data_df.question.tolist(), train_data_df.passage.tolist())]\n",
    "\n",
    "for i in tqdm(range(len(train_text))):\n",
    "    text_i = []\n",
    "    for sent in sentenize(train_text[i]):\n",
    "        sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "        text_i.extend(sent_i)\n",
    "    train_text[i] = text_i\n",
    "    \n",
    "# dev\n",
    "dev_text = [  ' '.join([question, passage])\n",
    "                for question, passage \n",
    "                in zip(dev_data_df.question.tolist(), dev_data_df.passage.tolist())]\n",
    "\n",
    "for i in tqdm(range(len(dev_text))):\n",
    "    text_i = []\n",
    "    for sent in sentenize(dev_text[i]):\n",
    "        sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "        text_i.extend(sent_i)\n",
    "    dev_text[i] = text_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving txt-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:13.538012Z",
     "start_time": "2023-05-12T13:15:13.265219Z"
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_dataset = pd.DataFrame({'text': train_text, 'target': train_data_df.answer.tolist()})\n",
    "train_dataset['text'] = train_dataset['text'].apply(lambda x: ' '.join(x))\n",
    "train_dataset['target'] = train_dataset['target'].apply(lambda x: '__label__' + str(x))\n",
    "train_dataset[['target', 'text']].to_csv('train.txt', \n",
    "                                         index = False, \n",
    "                                         sep = ' ',\n",
    "                                         header = None, \n",
    "                                         quoting = csv.QUOTE_NONE, \n",
    "                                         quotechar = \"\", \n",
    "                                         escapechar = \" \")\n",
    "# dev\n",
    "dev_dataset = pd.DataFrame({'text': dev_text, 'target': dev_data_df.answer.tolist()})\n",
    "dev_dataset['text'] = dev_dataset['text'].apply(lambda x: ' '.join(x))\n",
    "dev_dataset['target'] = dev_dataset['target'].apply(lambda x: '__label__' + str(x))\n",
    "dev_dataset[['target', 'text']].to_csv('dev.txt', \n",
    "                                         index = False, \n",
    "                                         sep = ' ',\n",
    "                                         header = None, \n",
    "                                         quoting = csv.QUOTE_NONE, \n",
    "                                         quotechar = \"\", \n",
    "                                         escapechar = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:18.205834Z",
     "start_time": "2023-05-12T13:15:16.145093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  47170\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  995967 lr:  0.000000 avg.loss:  0.654642 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised('train.txt', wordNgrams = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:15:24.690909Z",
     "start_time": "2023-05-12T13:15:24.541792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3270, 0.6290519877675841, 0.6290519877675841)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('dev.txt')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T13:16:02.331851Z",
     "start_time": "2023-05-12T13:16:02.324819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('__label__True',)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(dev_dataset['text'].iloc[0])[0]\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 3. [1 –±–∞–ª–ª] –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "1. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–æ–ø—Ä–æ—Å–∞ –∏ –∞–±–∑–∞—Ü–∞. –û–±—É—á–∏—Ç–µ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –Ω–∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö –≤–æ–ø—Ä–æ—Å–∞ –∏ –∞–±–∑–∞—Ü–∞ –∏ –æ—Ü–µ–Ω–∏—Ç–µ accuracy —ç—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. \n",
    "\n",
    "[bonus] –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –¥–æ—Å—Ç—É–ø–Ω—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ ü§ó Transformers. –ö–∞–∫–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–∞—Å—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã?\n",
    "\n",
    "[bonus] –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –º–µ—Ç–æ–¥ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–π—Ç–µ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) BERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:12:35.925154Z",
     "start_time": "2023-05-12T14:12:28.182051Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# train\n",
    "train_dataset = train_data_df[['question', 'passage', 'answer']].copy()\n",
    "train_dataset['question'] = train_dataset.question.apply(lambda x: tokenizer(x)['input_ids']) #tokenizer.tokenize(x))\n",
    "train_dataset['question'] = train_dataset.question.apply(lambda x: x[:30] + [0]*(30-len(x[:30]))) \n",
    "train_dataset['passage'] = train_dataset.passage.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "train_dataset['passage'] = train_dataset.passage.apply(lambda x: x[:1000] + [0]*(1000-len(x[:1000])))\n",
    "train_dataset['q&p'] = train_dataset[['question', 'passage']].apply(lambda x: x[0]+x[1], axis=1)\n",
    "\n",
    "# dev\n",
    "dev_dataset = dev_data_df[['question', 'passage', 'answer']].copy()\n",
    "dev_dataset['question'] = dev_dataset.question.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "dev_dataset['question'] = dev_dataset.question.apply(lambda x: x[:30] + [0]*(30-len(x[:30]))) \n",
    "dev_dataset['passage'] = dev_dataset.passage.apply(lambda x: tokenizer(x)['input_ids'])\n",
    "dev_dataset['passage'] = dev_dataset.passage.apply(lambda x: x[:1000] + [0]*(1000-len(x[:1000])))\n",
    "dev_dataset['q&p'] = dev_dataset[['question', 'passage']].apply(lambda x: x[0]+x[1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:32:01.422386Z",
     "start_time": "2023-05-12T14:32:00.207439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.6375304975071603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/16971921/.conda/envs/py38_venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = np.array(train_dataset['q&p'].tolist()), train_dataset['answer'].astype(int)\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "predict = clf.predict(X_train)\n",
    "print('train accuracy:', (predict == y_train).sum() / len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T14:32:52.180859Z",
     "start_time": "2023-05-12T14:32:51.890150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.6067278287461774\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = np.array(dev_dataset['q&p'].tolist()), dev_dataset['answer'].astype(int)\n",
    "predict = clf.predict(X_test)\n",
    "print('test accuracy:', (predict == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 3. [3 –±–∞–ª–ª–∞] DrQA-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Å—Ç–∞—Ç—å–µ: Reading Wikipedia to Answer Open-Domain Questions\n",
    "\n",
    "Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes\n",
    "\n",
    "https://arxiv.org/abs/1704.00051\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DrQA –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ SQuAD, –Ω–æ –ª–µ–≥–∫–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫ —Ç–µ–∫—É—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –±–ª–æ–∫–æ–≤:\n",
    "1. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –∞–±–∑–∞—Ü–∞ [paragraph encoding] ‚Äì LSTM, –ø–æ–ª—É—á–∞—è—â–∞—è –Ω–∞ –≤—Ö–æ–¥ –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑: \n",
    "* —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å–ª–æ–≤–∞ (w2v –∏–ª–∏ fasttext)\n",
    "* –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –≤ –≤–∏–¥–µ one-hot –≤–µ–∫—Ç–æ—Ä–æ–≤ —á–∞—Å—Ç—å —Ä–µ—á–∏ —Å–ª–æ–≤–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω–æ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç—å—é –∏–ª–∏ –Ω–µ—Ç, –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –≤ –≤–æ–ø—Ä–æ—Å–µ –∏–ª–∏ –Ω–µ—Ç \n",
    "* –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞, –ø–æ–ª—É—á–∞–µ–º–æ–≥–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º soft attention –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Å–ª–æ–≤ –∏–∑ –∞–±–∑–∞—Ü–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º –≤–æ–ø—Ä–æ—Å–∞.\n",
    "\n",
    "$f_{align}(p_i) = \\sum_jÙè∞Ç a_{i,j} E(q_j)$, –≥–¥–µ $E(q_j)$ ‚Äì —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞. –§–æ—Ä–º—É–ª–∞ –¥–ª—è $a_{i,j}$ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ. \n",
    "\n",
    "2. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –≤–æ–ø—Ä–æ—Å–∞ [question encoding] ‚Äì LSTM, –ø–æ–ª—É—á–∞—è—â–∞—è –Ω–∞ –≤—Ö–æ–¥ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞. –í—ã—Ö–æ–¥ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞: $q = Ùè∞Ç\\sum_jÙè∞Ç  b_j q_j$. –§–æ—Ä–º—É–ª–∞ –¥–ª—è $b_{j}$ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Å—Ç–∞—Ç—å–µ. \n",
    "\n",
    "3. –°–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. \n",
    "\n",
    "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ, –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã–ª–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ DrQA, —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ, —á—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ‚Äì¬†—ç—Ç–æ –º–µ—Ç–∫–∞ yes / no, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ—â–µ, —á–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ø–∞–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è SQuAD.\n",
    "\n",
    "–û—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. \n",
    "\n",
    "[bonus] –ó–∞–º–µ–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞–º–∏, –Ω–∞ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏. –£–ª—É—á—à–∏—Ç –ª–∏ —ç—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get dataset + tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T09:52:22.002633Z",
     "start_time": "2023-05-20T09:52:21.994254Z"
    }
   },
   "outputs": [],
   "source": [
    "Sample = namedtuple(\"Sample\", \"question, passage, labels\")\n",
    "\n",
    "def get_dataset(path_file: list):\n",
    "    samples = []\n",
    "    file = pd.read_json(path_file, lines=True, orient='records')\n",
    "    question = file.question.tolist()\n",
    "    passage = file.passage.tolist()\n",
    "    labels = file.answer.astype(int)\n",
    "    \n",
    "    for i in tqdm(range(len(file))):\n",
    "        #question\n",
    "        question_i = []\n",
    "        for sent in sentenize(question[i]):\n",
    "            sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "            question_i.extend(sent_i)\n",
    "        \n",
    "        #question\n",
    "        passage_i = []\n",
    "        for sent in sentenize(passage[i]):\n",
    "            sent_i = [ j.text.lower() for j in tokenize(sent.text) if j.text not in list(punctuation)]\n",
    "            passage_i.extend(sent_i)\n",
    "    \n",
    "        sample = Sample(question_i, passage_i, labels[i])\n",
    "        samples.append(sample)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T09:52:39.926297Z",
     "start_time": "2023-05-20T09:52:27.852531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9427/9427 [00:08<00:00, 1052.38it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3270/3270 [00:02<00:00, 1124.04it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_dataset('train.jsonl')\n",
    "random.shuffle(data)\n",
    "\n",
    "train_size = int(len(data)*0.8)\n",
    "train = data[:train_size]\n",
    "val = data[train_size:]\n",
    "test = get_dataset('dev.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T09:52:42.462346Z",
     "start_time": "2023-05-20T09:52:42.331767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42113"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_set = list( {token for sample in train for token in sample.question} |\n",
    "                 {token for sample in train for token in sample.passage}\n",
    "               )\n",
    "word_set.insert(0, '<unk>'), word_set.insert(0, '<pad>')\n",
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T12:15:21.819603Z",
     "start_time": "2023-05-20T12:15:21.813212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  7, 10]),)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [4,23,46,21367,7,234,7,4,1,34,4]\n",
    "np.where(np.array(a) == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T19:37:54.791631Z",
     "start_time": "2023-05-20T19:37:54.773430Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_gen_batch(samples, max_qst_len=50, max_psg_len=512, batch_size=32):\n",
    "    indices = np.arange(len(samples))\n",
    "    np.random.shuffle(indices)\n",
    "    batch_begin = 0\n",
    "    with tqdm(total=len(samples)) as pbar:\n",
    "        while batch_begin < len(samples):\n",
    "            batch_indices = indices[batch_begin: batch_begin + 32]\n",
    "            batch_qst = []\n",
    "            batch_psg = []\n",
    "            batch_psg_fts = []\n",
    "            batch_labels = []\n",
    "            batch_qst_mask = torch.ByteTensor(len(batch_indices), max_qst_len).fill_(1)\n",
    "            batch_psg_mask = torch.ByteTensor(len(batch_indices), max_psg_len).fill_(1)\n",
    "            for data_ind in batch_indices:\n",
    "                ind = list(batch_indices).index(data_ind)\n",
    "                \n",
    "                sample = samples[data_ind]\n",
    "                #–≤–æ–ø—Ä–æ—Å\n",
    "                question = torch.zeros(max_qst_len, dtype=torch.long) #.cuda()\n",
    "                for token_num, token in enumerate(sample.question[:max_qst_len]):\n",
    "                    question[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                question_len = len(sample.question[:max_qst_len])\n",
    "                batch_qst_mask[ind, :question_len].fill_(0)\n",
    "                #–ø–∞—Ä–∞–≥—Ä–∞—Ñ\n",
    "                passage = torch.zeros(max_psg_len, dtype=torch.long) #.cuda()\n",
    "                passage_features = torch.zeros(max_psg_len, max_qst_len, dtype=torch.long)\n",
    "                for token_num, token in enumerate(sample.passage[:max_psg_len]):\n",
    "                    passage[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                    q_inds = np.where(np.array(sample.question[:max_qst_len]) == token)\n",
    "                    for q_ind in q_inds:\n",
    "                        passage_features[token_num, q_ind] = 1\n",
    "                passage_len = len(sample.passage[:max_psg_len])\n",
    "                batch_psg_mask[ind, :passage_len].fill_(0)\n",
    "                    \n",
    "                labels = sample.labels\n",
    "\n",
    "                batch_qst.append(question)\n",
    "                batch_psg.append(passage)\n",
    "                batch_psg_fts.append(passage_features)\n",
    "                batch_labels.append(labels)\n",
    "              \n",
    "            batch_begin += batch_size\n",
    "            pbar.update(batch_size)\n",
    "          \n",
    "            batch_qst = torch.stack(batch_qst)\n",
    "            batch_psg = torch.stack(batch_psg)\n",
    "            batch_psg_fts = torch.stack(batch_psg_fts)\n",
    "            batch_labels = torch.LongTensor(batch_labels)\n",
    "            yield batch_indices, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T12:16:48.890907Z",
     "start_time": "2023-05-20T12:16:43.605426Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñè                                        | 32/7541 [00:05<20:38,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch_qst            torch.Size([32, 50])\n",
      "batch_qst_mask       torch.Size([32, 50])\n",
      "batch_psg            torch.Size([32, 512])\n",
      "batch_psg_fts        torch.Size([32, 512, 50])\n",
      "passage_mask         torch.Size([32, 512])\n",
      "batch_labels         torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels in get_next_gen_batch(train):\n",
    "    1+1\n",
    "    break\n",
    "print()\n",
    "print( 'batch_qst'.ljust(20), batch_qst.size() )\n",
    "print( 'batch_qst_mask'.ljust(20), batch_qst_mask.size() )\n",
    "print( 'batch_psg'.ljust(20), batch_psg.size() )\n",
    "print( 'batch_psg_fts'.ljust(20), batch_psg_fts.size() )\n",
    "print( 'passage_mask'.ljust(20), batch_psg_mask.size() )\n",
    "print( 'batch_labels'.ljust(20), batch_labels.size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:21:55.077211Z",
     "start_time": "2023-05-20T22:21:55.050971Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/DrQA/tree/main\n",
    "class SeqAttnMatch(nn.Module):\n",
    "    \"\"\"Given sequences X and Y, match sequence Y to each element in X.\n",
    "\n",
    "    * o_i = sum(alpha_j * y_j) for i in X\n",
    "    * alpha_j = softmax(y_j * x_i)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, identity=False):\n",
    "        super(SeqAttnMatch, self).__init__()\n",
    "        if not identity:\n",
    "            self.linear = nn.Linear(input_size, input_size)\n",
    "        else:\n",
    "            self.linear = None\n",
    "\n",
    "    def forward(self, x, y, y_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch * len1 * hdim\n",
    "            y: batch * len2 * hdim\n",
    "            y_mask: batch * len2 (1 for padding, 0 for true)\n",
    "        Output:\n",
    "            matched_seq: batch * len1 * hdim\n",
    "        \"\"\"\n",
    "        # Project vectors\n",
    "        if self.linear:\n",
    "            x_proj = self.linear(x.view(-1, x.size(2))).view(x.size())\n",
    "            x_proj = F.relu(x_proj)\n",
    "            y_proj = self.linear(y.view(-1, y.size(2))).view(y.size())\n",
    "            y_proj = F.relu(y_proj)\n",
    "        else:\n",
    "            x_proj = x\n",
    "            y_proj = y\n",
    "\n",
    "        # Compute scores\n",
    "        scores = x_proj.bmm(y_proj.transpose(2, 1))\n",
    "\n",
    "        # Mask padding\n",
    "        y_mask = y_mask.unsqueeze(1).expand(scores.size())\n",
    "        scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
    "\n",
    "        # Normalize with softmax\n",
    "        alpha_flat = F.softmax(scores.view(-1, y.size(1)), dim=-1)\n",
    "        alpha = alpha_flat.view(-1, x.size(1), y.size(1))\n",
    "\n",
    "        # Take weighted average\n",
    "        matched_seq = alpha.bmm(y)\n",
    "        return matched_seq\n",
    "\n",
    "class LinearSeqAttn(nn.Module):\n",
    "    \"\"\"Self attention over a sequence:\n",
    "\n",
    "    * o_i = softmax(Wx_i) for x_i in X.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearSeqAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: batch * len * hdim\n",
    "            x_mask: batch * len (1 for padding, 0 for true)\n",
    "        Output:\n",
    "            alpha: batch * len\n",
    "        \"\"\"\n",
    "        x_flat = x.view(-1, x.size(-1))\n",
    "        scores = self.linear(x_flat).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, dim=-1)\n",
    "        return alpha\n",
    "    \n",
    "#------------------------------------------------------------------------------------------\n",
    "    \n",
    "class DrQA_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 word_set_size,\n",
    "                 question_max_size = 50,\n",
    "                 passage_max_size = 512,\n",
    "                 word_embedding_dim=128,\n",
    "                 lstm_embedding_dim = 24,\n",
    "                 classes_count=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.qemb_match = SeqAttnMatch(word_embedding_dim)\n",
    "        self.self_attn = LinearSeqAttn(2*lstm_embedding_dim)\n",
    "        self.embedding = nn.Embedding(word_set_size, word_embedding_dim)\n",
    "        self.psg_rnn = nn.LSTM(2*word_embedding_dim+question_max_size, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.qst_rnn = nn.LSTM(word_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.lin = nn.Linear(passage_max_size+1, classes_count)\n",
    "\n",
    "    def forward(self, questions, question_mask, passages, passage_features, passage_mask):\n",
    "        #—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ w2v\n",
    "        q_emb = self.embedding(questions)\n",
    "        p_emb = self.embedding(passages)\n",
    "        # –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å–∞, –ø–æ–ª—É—á–∞–µ–º—ã–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º \n",
    "        # soft attention –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Å–ª–æ–≤ –∏–∑ –∞–±–∑–∞—Ü–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º –≤–æ–ø—Ä–æ—Å–∞\n",
    "        q_weighted_emb = self.qemb_match(p_emb, q_emb, question_mask)\n",
    "        # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –≤ –≤–∏–¥–µ one-hot \n",
    "        # –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –≤ –≤–æ–ø—Ä–æ—Å–µ –∏–ª–∏ –Ω–µ—Ç\n",
    "        drnn_input = [p_emb]\n",
    "        drnn_input.append(q_weighted_emb)\n",
    "        drnn_input.append(passage_features)\n",
    "        drnn_input = torch.cat(drnn_input, 2)\n",
    "        \n",
    "        # 1) paragraph encoding (LSTM)\n",
    "        #    packs a Tensor containing padded sequences of variable length.\n",
    "        #    compute sorted sequence lengths\n",
    "        lengths = passage_mask.data.eq(0).long().sum(1)\n",
    "        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        lengths = list(lengths[idx_sort])\n",
    "        #    ---\n",
    "        p_rnn_input = drnn_input.index_select(0, idx_sort) # sort x\n",
    "        p_rnn_input = nn.utils.rnn.pack_padded_sequence(p_rnn_input, lengths, batch_first=True) # pack it up\n",
    "        #    ---\n",
    "        p_hidden, (final_hidden_state, final_cell_state) = self.psg_rnn(p_rnn_input) #LSTM\n",
    "        p_output = nn.utils.rnn.pad_packed_sequence(p_hidden, batch_first=True)[0] # unpack \n",
    "        p_output = p_output.index_select(0, idx_unsort) # unsort\n",
    "        #    pad up to original batch sequence length\n",
    "        if p_output.size(1) != passage_mask.size(1):\n",
    "            padding = torch.zeros(p_output.size(0), \n",
    "                                  passage_mask.size(1) - p_output.size(1),\n",
    "                                  p_output.size(2)).type(p_output.data.type())\n",
    "            p_output = torch.cat([p_output, padding], 1)\n",
    "        \n",
    "        # 2) question encoding (LSTM)\n",
    "        lengths = question_mask.data.eq(0).long().sum(1)\n",
    "        _, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        lengths = list(lengths[idx_sort])\n",
    "        #    ---\n",
    "        q_rnn_input = q_emb.index_select(0, idx_sort) # sort x\n",
    "        q_rnn_input = nn.utils.rnn.pack_padded_sequence(q_rnn_input, lengths, batch_first=True) # pack it up\n",
    "        #    ---\n",
    "        q_hidden, (final_hidden_state, final_cell_state) = self.qst_rnn(q_rnn_input) #LSTM\n",
    "        q_hidden = nn.utils.rnn.pad_packed_sequence(q_hidden, batch_first=True)[0]   # unpack \n",
    "        q_hidden = q_hidden.index_select(0, idx_unsort) # unsort\n",
    "        #    pad up to original batch sequence length\n",
    "        if q_hidden.size(1) != question_mask.size(1):\n",
    "            padding = torch.zeros(q_hidden.size(0), \n",
    "                                  question_mask.size(1) - q_hidden.size(1),\n",
    "                                  q_hidden.size(2)).type(q_hidden.data.type())\n",
    "            q_hidden = torch.cat([q_hidden, padding], 1)\n",
    "        #    –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞\n",
    "        q_weights = self.self_attn(q_hidden, question_mask)\n",
    "        #    return a weighted average of x (a sequence of vectors)\n",
    "        q_output = q_weights.unsqueeze(1).bmm(q_hidden).squeeze(1)\n",
    "\n",
    "        # 3) concat + linear\n",
    "        q_output = q_output.unsqueeze(1)\n",
    "        output = torch.cat([p_output, q_output], 1)\n",
    "        output = output.mean(-1)\n",
    "        output = self.lin(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:21:58.860711Z",
     "start_time": "2023-05-20T22:21:58.389682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:39: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:69: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(x_mask.data, -float('inf'))\n"
     ]
    }
   ],
   "source": [
    "model = DrQA_model(len(word_set))\n",
    "\n",
    "lr=0.01\n",
    "device_name=\"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "\n",
    "logits = model(batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask) # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "loss = loss_function(logits, batch_labels) # –ü–æ–¥—Å—á—ë—Ç –æ—à–∏–±–∫–∏\n",
    "loss.backward() # –ü–æ–¥—Å—á—ë—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ dL/dw\n",
    "optimizer.step() # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏–ª–∏ –µ–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ Adam)\n",
    "optimizer.zero_grad() # –ó–∞–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –∏—Ö —Å–ø–æ–∫–æ–π–Ω–æ –º–µ–Ω—è—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:28:10.961328Z",
     "start_time": "2023-05-20T22:28:10.948109Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n",
    "                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cpu\",\n",
    "                    early_stopping=True):\n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss()#.cuda()\n",
    "    prev_avg_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        model.train()\n",
    "        for step, (_, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels) in enumerate(get_next_gen_batch(train)):\n",
    "            logits = model(batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask) # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "            loss = loss_function(logits, batch_labels) # –ü–æ–¥—Å—á—ë—Ç –æ—à–∏–±–∫–∏\n",
    "            loss.backward() # –ü–æ–¥—Å—á—ë—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ dL/dw\n",
    "            optimizer.step() # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏–ª–∏ –µ–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ Adam)\n",
    "            optimizer.zero_grad() # –ó–∞–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –∏—Ö —Å–ø–æ–∫–æ–π–Ω–æ –º–µ–Ω—è—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏\n",
    "            total_loss += loss.item()\n",
    "        val_total_loss = 0\n",
    "        val_batch_count = 0\n",
    "        model.eval()\n",
    "        for _, (_, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels) in enumerate(get_next_gen_batch(val)):\n",
    "            logits = model(batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask) # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "            val_total_loss += loss_function(logits, batch_labels) # –ü–æ–¥—Å—á—ë—Ç –æ—à–∏–±–∫–∏\n",
    "            val_batch_count += 1\n",
    "        avg_val_loss = val_total_loss/val_batch_count\n",
    "        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.eval()\n",
    "            break\n",
    "        prev_avg_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T22:57:01.944642Z",
     "start_time": "2023-05-20T22:28:11.838806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 5501365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñè                                        | 32/7541 [00:04<16:32,  7.57it/s]/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:39: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:69: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
      "7552it [23:03,  5.46it/s]                                                       \n",
      "1888it [05:45,  5.46it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Avg Train Loss = 1.5385, Avg val loss = 0.6367, Time = 1728.65s\n"
     ]
    }
   ],
   "source": [
    "model = DrQA_model(len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T23:03:19.522177Z",
     "start_time": "2023-05-20T23:03:19.510445Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_precision_and_recall(true_positive, false_positive, false_negative):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç—É –ø–æ TP, FP –∏ FN\n",
    "    \"\"\"\n",
    "    if false_positive + true_positive > 0:\n",
    "        precision = float(true_positive) / (true_positive + false_positive)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if false_negative + true_positive > 0:\n",
    "        recall = float(true_positive) / (true_positive + false_negative)\n",
    "    else:\n",
    "        recall = 0\n",
    "    return recall, precision\n",
    "\n",
    "def predict(model, samples):\n",
    "    model.eval()\n",
    "    tp, fp, fn = 0,0,0\n",
    "    for _, (indices, batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask, batch_labels) in enumerate(get_next_gen_batch(samples)):\n",
    "        logits = model(batch_qst, batch_qst_mask, batch_psg, batch_psg_fts, batch_psg_mask)\n",
    "        plabels = logits.max(dim=1)[1]\n",
    "        tp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j!=0) and (i==j)])\n",
    "        fp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j!=0) and (i!=j)])\n",
    "        fn += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j==0) and (i!=j)])\n",
    "\n",
    "    recall, precision = compute_precision_and_recall(tp, fp, fn)\n",
    "    try:\n",
    "        f_measure = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f_measure = 'None'\n",
    "    print('precision:', precision)\n",
    "    print('recall:   ', recall)\n",
    "    print('f1:       ', f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T23:38:22.418224Z",
     "start_time": "2023-05-20T23:03:20.887553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñè                                        | 32/7541 [00:05<20:47,  6.02it/s]/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:39: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(y_mask.data, -float('inf'))\n",
      "/var/folders/2q/qk755hxx3f5bjgcp14hnl93m_yy8wm/T/ipykernel_2698/2502503718.py:69: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
      "7552it [22:20,  5.63it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7421614409606404\n",
      "recall:    0.9502455690796497\n",
      "f1:        0.8334113681056279\n",
      "\n",
      " -------on val dataset:--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888it [04:41,  6.70it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6569435637285986\n",
      "recall:    0.869857262804366\n",
      "f1:        0.7485549132947977\n",
      "\n",
      " -------on test dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3296it [07:59,  6.88it/s]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6549426138467235\n",
      "recall:    0.8701426463354648\n",
      "f1:        0.7473595268272074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 4. [3 –±–∞–ª–ª–∞] BiDAF-–ø–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
    "\n",
    "–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Å—Ç–∞—Ç—å–µ: Bidirectional Attention Flow for Machine Comprehension\n",
    "\n",
    "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "\n",
    "https://arxiv.org/abs/1611.01603\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ BiDAF –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ SQuAD, –Ω–æ –ª–µ–≥–∫–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∫ —Ç–µ–∫—É—â–µ–º—É –∑–∞–¥–∞–Ω–∏—é. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö –±–ª–æ–∫–æ–≤:\n",
    "1. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫  –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –¥–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞: —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–≤–∞ –∏ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –∏–∑ CNN –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞. –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ –∏ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã. \n",
    "2. –°–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è (–¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–æ –≤ —Å—Ç–∞—Ç—å–µ, —Å–º. –ø—É–Ω–∫—Ç Attention Flow Layer)\n",
    "3. –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ —Ç—Ä–µ—Ö —á–∞—Å—Ç–µ–π (–≤—ã—Ö–æ–¥ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞,   Query2Context (–æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä) –∏ Context2Query (–º–∞—Ç—Ä–∏—Ü–∞) –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è\n",
    "\n",
    "4. –°–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. \n",
    "\n",
    "–ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ, –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã–ª–æ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ BiDAF, —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ, —á—Ç–æ –∏—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ ‚Äì¬†—ç—Ç–æ –º–µ—Ç–∫–∞ yes / no, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ—â–µ, —á–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ø–∞–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è SQuAD.\n",
    "\n",
    "–û—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. \n",
    "\n",
    "[bonus] –ó–∞–º–µ–Ω–∏—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞–º–∏, –Ω–∞ BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏. –£–ª—É—á—à–∏—Ç –ª–∏ —ç—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T07:16:08.315087Z",
     "start_time": "2023-05-21T07:16:08.032794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_set = list( {ch for sample in train for token in sample.question for ch in token} |\n",
    "                 {ch for sample in train for token in sample.passage for ch in token}\n",
    "               )\n",
    "char_set.insert(0, '<unk>'), char_set.insert(0, '<pad>')\n",
    "len(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:23:43.530297Z",
     "start_time": "2023-05-21T08:23:43.515153Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_gen_batch(samples, max_qst_len=50, max_psg_len=512, max_char_seq_len=40, batch_size=32):\n",
    "    indices = np.arange(len(samples))\n",
    "    np.random.shuffle(indices)\n",
    "    batch_begin = 0\n",
    "    with tqdm(total=len(samples)) as pbar:\n",
    "        while batch_begin < len(samples):\n",
    "            batch_indices = indices[batch_begin: batch_begin + 32]\n",
    "            batch_qst = []\n",
    "            batch_qst_ch = []\n",
    "            batch_psg = []\n",
    "            batch_psg_ch = []\n",
    "            batch_labels = []\n",
    "            for data_ind in batch_indices:\n",
    "                ind = list(batch_indices).index(data_ind)\n",
    "                sample = samples[data_ind]\n",
    "                \n",
    "                #–≤–æ–ø—Ä–æ—Å\n",
    "                question = torch.zeros(max_qst_len, dtype=torch.long) #.cuda()\n",
    "                qst_chars = torch.zeros((max_qst_len, max_char_seq_len), dtype=torch.long)#.cuda()\n",
    "                for token_num, token in enumerate(sample.question[:max_qst_len]):\n",
    "                    #—Å–ª–æ–≤–∞\n",
    "                    question[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                    #—Å–∏–º–≤–æ–ª—ã\n",
    "                    for char_num, char in enumerate(token[:max_char_seq_len]):\n",
    "                        qst_chars[token_num][char_num] = char_set.index(char) if char in char_set else char_set.index('<unk>')\n",
    "                \n",
    "                #–ø–∞—Ä–∞–≥—Ä–∞—Ñ\n",
    "                passage = torch.zeros(max_psg_len, dtype=torch.long) #.cuda()\n",
    "                psg_chars = torch.zeros((max_psg_len, max_char_seq_len), dtype=torch.long)#.cuda()\n",
    "                for token_num, token in enumerate(sample.passage[:max_psg_len]):\n",
    "                    #—Å–ª–æ–≤–∞\n",
    "                    passage[token_num] = word_set.index(token) if token in word_set else word_set.index('<unk>')\n",
    "                    #—Å–∏–º–≤–æ–ª—ã\n",
    "                    for char_num, char in enumerate(token[:max_char_seq_len]):\n",
    "                        psg_chars[token_num][char_num] = char_set.index(char) if char in char_set else char_set.index('<unk>')\n",
    "                \n",
    "                labels = sample.labels\n",
    "\n",
    "                batch_qst.append(question)\n",
    "                batch_qst_ch.append(qst_chars)\n",
    "                batch_psg.append(passage)\n",
    "                batch_psg_ch.append(psg_chars)\n",
    "                batch_labels.append(labels)\n",
    "              \n",
    "            batch_begin += batch_size\n",
    "            pbar.update(batch_size)\n",
    "          \n",
    "            batch_qst = torch.stack(batch_qst)\n",
    "            batch_psg = torch.stack(batch_psg)\n",
    "            batch_qst_ch = torch.stack(batch_qst_ch)\n",
    "            batch_psg_ch = torch.stack(batch_psg_ch)\n",
    "            batch_labels = torch.LongTensor(batch_labels)\n",
    "            yield batch_indices, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:23:48.648689Z",
     "start_time": "2023-05-21T08:23:44.201330Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñè                                        | 32/7541 [00:04<17:20,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "batch_qst            torch.Size([32, 50])\n",
      "batch_qst_mask       torch.Size([32, 50, 40])\n",
      "batch_psg            torch.Size([32, 512])\n",
      "batch_psg_fts        torch.Size([32, 512, 40])\n",
      "batch_labels         torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels in get_next_gen_batch(train):\n",
    "    1+1\n",
    "    break\n",
    "print()\n",
    "print( 'batch_qst'.ljust(20), batch_qst.size() )\n",
    "print( 'batch_qst_mask'.ljust(20), batch_qst_ch.size() )\n",
    "print( 'batch_psg'.ljust(20), batch_psg.size() )\n",
    "print( 'batch_psg_fts'.ljust(20), batch_psg_ch.size() )\n",
    "print( 'batch_labels'.ljust(20), batch_labels.size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T09:49:53.957944Z",
     "start_time": "2023-05-21T09:49:53.939007Z"
    },
    "code_folding": [
     27
    ]
   },
   "outputs": [],
   "source": [
    "class BiDAF_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 char_set_size, word_set_size,\n",
    "                 question_max_size = 50, passage_max_size = 512,\n",
    "                 char_embedding_dim=8, word_embedding_dim=64,\n",
    "                 lstm_embedding_dim = 24, \n",
    "                 char_max_seq_len=40, kernel_size=3,\n",
    "                 classes_count=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        self.char_max_seq_len = char_max_seq_len\n",
    "        self.char_embedding = nn.Embedding(char_set_size, char_embedding_dim)\n",
    "        self.char_cnn = nn.Conv1d(in_channels=char_embedding_dim, out_channels=1, kernel_size=kernel_size)\n",
    "        self.embedding = nn.Embedding(word_set_size, word_embedding_dim)\n",
    "        self.context_LSTM = nn.LSTM(word_embedding_dim+char_max_seq_len-(kernel_size-1), lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.modeling_LSTM1 = nn.LSTM(8*lstm_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.modeling_LSTM2 = nn.LSTM(2*lstm_embedding_dim, lstm_embedding_dim, batch_first=True, bidirectional=True)\n",
    "        self.lin = nn.Linear(passage_max_size, classes_count)\n",
    "\n",
    "        # Attention Flow Layer\n",
    "        self.att_weight_c = nn.Linear(2*lstm_embedding_dim, 1)\n",
    "        self.att_weight_q = nn.Linear(2*lstm_embedding_dim, 1)\n",
    "        self.att_weight_cq = nn.Linear(2*lstm_embedding_dim, 1)\n",
    "        \n",
    "    def forward(self, questions, question_chars, passages, passage_chars):\n",
    "        # https://github.com/galsang/BiDAF-pytorch/tree/master\n",
    "        def att_flow_layer(c, q):\n",
    "            \"\"\"\n",
    "            :param c: (batch, c_len, hidden_size * 2)\n",
    "            :param q: (batch, q_len, hidden_size * 2)\n",
    "            :return: (batch, c_len, q_len)\n",
    "            \"\"\"\n",
    "            c_len = c.size(1)\n",
    "            q_len = q.size(1)\n",
    "\n",
    "            cq = []\n",
    "            for i in range(q_len):\n",
    "                #(batch, 1, hidden_size * 2)\n",
    "                qi = q.select(1, i).unsqueeze(1)\n",
    "                #(batch, c_len, 1)\n",
    "                ci = self.att_weight_cq(c * qi).squeeze()\n",
    "                cq.append(ci)\n",
    "            # (batch, c_len, q_len)\n",
    "            cq = torch.stack(cq, dim=-1)\n",
    "\n",
    "            # (batch, c_len, q_len)\n",
    "            s = self.att_weight_c(c).expand(-1, -1, q_len) + \\\n",
    "                self.att_weight_q(q).permute(0, 2, 1).expand(-1, c_len, -1) + \\\n",
    "                cq\n",
    "\n",
    "            # (batch, c_len, q_len)\n",
    "            a = F.softmax(s, dim=2)\n",
    "            # (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -> (batch, c_len, hidden_size * 2)\n",
    "            c2q_att = torch.bmm(a, q)\n",
    "            # (batch, 1, c_len)\n",
    "            b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)\n",
    "            # (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -> (batch, hidden_size * 2)\n",
    "            q2c_att = torch.bmm(b, c).squeeze()\n",
    "            # (batch, c_len, hidden_size * 2) (tiled)\n",
    "            q2c_att = q2c_att.unsqueeze(1).expand(-1, c_len, -1)\n",
    "            # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
    "\n",
    "            # (batch, c_len, hidden_size * 8)\n",
    "            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=-1)\n",
    "            return x\n",
    "        \n",
    "        # —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ w2v\n",
    "        q_emb = self.embedding(questions)\n",
    "        p_emb = self.embedding(passages)\n",
    "        # –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –∏–∑ CNN –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞\n",
    "        q_c_emb = self.char_embedding(question_chars)\n",
    "        q_c_emb = q_c_emb.reshape(q_c_emb.size(0)*q_c_emb.size(1), self.char_max_seq_len, self.char_embedding_dim)\n",
    "        q_c_emb = q_c_emb.permute(0,2,1)\n",
    "        q_c_cnn = self.char_cnn(q_c_emb)\n",
    "        q_c_cnn = q_c_cnn.reshape(question_chars.size(0), question_chars.size(1), -1)\n",
    "        #----\n",
    "        p_c_emb = self.char_embedding(passage_chars)\n",
    "        p_c_emb = p_c_emb.reshape(p_c_emb.size(0)*p_c_emb.size(1), self.char_max_seq_len, self.char_embedding_dim)\n",
    "        p_c_emb = p_c_emb.permute(0,2,1)\n",
    "        p_c_cnn = self.char_cnn(p_c_emb)\n",
    "        p_c_cnn = p_c_cnn.reshape(passage_chars.size(0), passage_chars.size(1), -1)\n",
    "        # –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –µ–º–±–µ–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤\n",
    "        q = torch.cat([q_c_cnn, q_emb], dim=-1)\n",
    "        p = torch.cat([p_c_cnn, p_emb], dim=-1)\n",
    "        # –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–µ—Ç—å\n",
    "        q = self.context_LSTM(q)[0]\n",
    "        p = self.context_LSTM(p)[0]\n",
    "        # Attention Flow Layer\n",
    "        g = att_flow_layer(p, q)\n",
    "        # LSTM modeling\n",
    "        rnn1 = self.modeling_LSTM1(g)[0]\n",
    "        rnn2 = self.modeling_LSTM2(rnn1)[0]\n",
    "        # output\n",
    "        output = rnn2.mean(-1)\n",
    "        output = self.lin(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:23:51.933150Z",
     "start_time": "2023-05-21T08:23:50.578087Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BiDAF_model(len(char_set), len(word_set))\n",
    "\n",
    "lr=0.01\n",
    "device_name=\"cpu\"\n",
    "device = torch.device(device_name)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "\n",
    "logits = model(batch_qst, batch_qst_ch, batch_psg, batch_psg_ch) # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "loss = loss_function(logits, batch_labels) # –ü–æ–¥—Å—á—ë—Ç –æ—à–∏–±–∫–∏\n",
    "loss.backward() # –ü–æ–¥—Å—á—ë—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ dL/dw\n",
    "optimizer.step() # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏–ª–∏ –µ–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ Adam)\n",
    "optimizer.zero_grad() # –ó–∞–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –∏—Ö —Å–ø–æ–∫–æ–π–Ω–æ –º–µ–Ω—è—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:24:10.392627Z",
     "start_time": "2023-05-21T08:24:10.382964Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_gen_model(model, train_samples, val_samples, epochs_count=10, \n",
    "                    loss_every_nsteps=100, lr=0.01, save_path=\"model.pt\", device_name=\"cpu\",\n",
    "                    early_stopping=True):\n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss()#.cuda()\n",
    "    prev_avg_val_loss = None\n",
    "    for epoch in range(epochs_count):\n",
    "        model.train()\n",
    "        for step, (_, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels) in enumerate(get_next_gen_batch(train)):\n",
    "            logits = model(batch_qst, batch_qst_ch, batch_psg, batch_psg_ch) # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "            loss = loss_function(logits, batch_labels) # –ü–æ–¥—Å—á—ë—Ç –æ—à–∏–±–∫–∏\n",
    "            loss.backward() # –ü–æ–¥—Å—á—ë—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ dL/dw\n",
    "            optimizer.step() # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏–ª–∏ –µ–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ Adam)\n",
    "            optimizer.zero_grad() # –ó–∞–Ω—É–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –∏—Ö —Å–ø–æ–∫–æ–π–Ω–æ –º–µ–Ω—è—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏\n",
    "            total_loss += loss.item()\n",
    "        val_total_loss = 0\n",
    "        val_batch_count = 0\n",
    "        model.eval()\n",
    "        for _, (_, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels) in enumerate(get_next_gen_batch(val)):\n",
    "            logits = model(batch_qst, batch_qst_ch, batch_psg, batch_psg_ch) # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "            val_total_loss += loss_function(logits, batch_labels) # –ü–æ–¥—Å—á—ë—Ç –æ—à–∏–±–∫–∏\n",
    "            val_batch_count += 1\n",
    "        avg_val_loss = val_total_loss/val_batch_count\n",
    "        print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        if early_stopping and prev_avg_val_loss is not None and avg_val_loss > prev_avg_val_loss:\n",
    "            model.load_state_dict(torch.load(save_path))\n",
    "            model.eval()\n",
    "            break\n",
    "        prev_avg_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T08:56:28.020787Z",
     "start_time": "2023-05-21T08:24:11.149426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 2782102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [25:51,  4.87it/s]                                                       \n",
      "1888it [05:49,  5.41it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Avg Train Loss = 1.6096, Avg val loss = 0.6590, Time = 1900.84s\n"
     ]
    }
   ],
   "source": [
    "model = BiDAF_model(len(char_set), len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T09:02:43.843548Z",
     "start_time": "2023-05-21T09:02:43.835714Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, samples):\n",
    "    model.eval()\n",
    "    tp, fp, fn = 0,0,0\n",
    "    for _, (indices, batch_qst, batch_qst_ch, batch_psg, batch_psg_ch, batch_labels) in enumerate(get_next_gen_batch(samples)):\n",
    "        logits = model(batch_qst, batch_qst_ch, batch_psg, batch_psg_ch)\n",
    "        plabels = logits.max(dim=1)[1]\n",
    "        tp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j!=0) and (i==j)])\n",
    "        fp += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j!=0) and (i!=j)])\n",
    "        fn += len([(i,j) for i,j in zip(batch_labels.tolist(), plabels.tolist()) if (j==0) and (i!=j)])\n",
    "\n",
    "    recall, precision = compute_precision_and_recall(tp, fp, fn)\n",
    "    try:\n",
    "        f_measure = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f_measure = 'None'\n",
    "    print('precision:', precision)\n",
    "    print('recall:   ', recall)\n",
    "    print('f1:       ', f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T09:36:14.950363Z",
     "start_time": "2023-05-21T09:02:44.802404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [19:54,  6.32it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6210051717278875\n",
      "recall:    1.0\n",
      "f1:        0.7661976439790575\n",
      "\n",
      " -------on val dataset:--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888it [04:55,  6.38it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6314952279957582\n",
      "recall:    1.0\n",
      "f1:        0.7741306467338316\n",
      "\n",
      " -------on test dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3296it [08:39,  6.34it/s]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6217125382262997\n",
      "recall:    1.0\n",
      "f1:        0.7667358099189139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∏–ª–∞—Å—å - –≤—Å–µ–≥–¥–∞ –æ–¥–∏–Ω –æ—Ç–≤–µ—Ç*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### –ø–æ–ø—ã—Ç–∫–∞ ‚Ññ2 (BiDAF) -> smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T10:21:11.234821Z",
     "start_time": "2023-05-21T09:51:20.373629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 2782102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [23:15,  5.41it/s]                                                       \n",
      "1888it [06:05,  5.16it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Avg Train Loss = 1.5674, Avg val loss = 0.6579, Time = 1760.89s\n"
     ]
    }
   ],
   "source": [
    "model = BiDAF_model(len(char_set), len(word_set))\n",
    "train_gen_model(model, train, val, epochs_count=1, early_stopping=False, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T10:56:04.826082Z",
     "start_time": "2023-05-21T10:21:11.248213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------on train dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7552it [20:37,  6.10it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6219090667375698\n",
      "recall:    0.9989323083493488\n",
      "f1:        0.7665710774272839\n",
      "\n",
      " -------on val dataset:--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888it [05:04,  6.21it/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6312997347480106\n",
      "recall:    0.9991603694374476\n",
      "f1:        0.7737321196358907\n",
      "\n",
      " -------on test dataset:-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3296it [09:11,  5.97it/s]                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6215554194733619\n",
      "recall:    0.9985243482538121\n",
      "f1:        0.7661822985468958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('on train dataset:'.center(30, '-'))\n",
    "predict(model, train)\n",
    "\n",
    "print('\\n', 'on val dataset:'.center(30, '-'))\n",
    "predict(model, val)\n",
    "\n",
    "print('\\n', 'on test dataset:'.center(30, '-'))\n",
    "predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### —É–≤—ã –Ω–µ –ø–æ–º–æ–≥–ª–æ ‚òùÔ∏èüòî"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ DrQA –∏ BiDAF:\n",
    "    \n",
    "![](https://www.researchgate.net/profile/Felix_Wu6/publication/321069852/figure/fig1/AS:560800147881984@1510716582560/Schematic-layouts-of-the-BiDAF-left-and-DrQA-right-architectures-We-propose-to.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç—å 5. [1 –±–∞–ª–ª] –ò—Ç–æ–≥–∏\n",
    "–ù–∞–ø–∏—à–∏—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ß—Ç–æ –ø–æ–º–æ–≥–ª–æ –≤–∞–º –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã, —á–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∞–ª–æ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** baseline = 0.6291 (fasttext –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ —Å–∫–ª–µ–µ–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∞–±–∑–∞—Ü–µ–≤) <br>\n",
    "   –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤ —Å–∏–ª—É —Å–≤–æ–µ–π –ø—Ä–æ—Å—Ç–æ—Ç—ã –Ω–µ –º–æ–∂–µ—Ç –≤—ã—è–≤–∏—Ç—å  <br>\n",
    "   –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏  <br>\n",
    "**2)** bert = 0.6067 <br>\n",
    "   —è –±—ã —Å–∫–∞–∑–∞–ª, —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ —Ç–∞ –∂–µ - –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –∞–±–∑–∞—Ü–∞ –∏ –≤–æ–ø—Ä–æ—Å–∞ –Ω–µ  <br>\n",
    "   –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—á–µ—Å—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –Ω–∏–º–∏  <br>\n",
    "**3)** DrQA = 0.6549 <br>\n",
    "   –≤–æ–∑–º–æ–∂–Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤—ã—Ö–æ–¥–∞–º–∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π –Ω–∞–¥ <br>\n",
    "   –≤–æ–ø—Ä–æ—Å–æ–º –∏ –∞–±–∑–∞—Ü–µ–º <br>\n",
    "**4)** BiDAF = 0.6217 <br>\n",
    "   –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∏–ª–∞—Å—å - –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã–¥–∞–µ—Ç —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –æ—Ç–≤–µ—Ç –≤—ã–±–æ—Ä–∫–∏\n",
    "   –≤–æ–∑–º–æ–∂–Ω–æ –º–æ–∂–Ω–æ –æ–±–æ–π—Ç–∏—Å—å –æ–¥–Ω–æ–π —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π —Å–µ—Ç—å—é –ø–µ—Ä–µ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –ª–∏–Ω–µ–π–Ω—ã–º —Å–ª–æ–µ–º\n",
    "<br>\n",
    "<br>\n",
    "–º–æ–¥–µ–ª—å DrQA –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –º–µ–∂–¥—É –∞–±–∑–∞—Ü–µ–º –∏ <br>\n",
    "–≤–æ–ø—Ä–æ—Å–æ–º –ø–æ–∑–≤–æ—è–ª–µ—Ç –ª—É—á—à–µ –æ–±—É—á–∞—Ç—å—Å—è —Å–µ—Ç–∏ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞, <br>\n",
    "–æ–¥–Ω–∞–∫–æ –≤ —Å–ª—É—á–∞–µ —Å BiDAF –æ–±—É—á–∏—Ç—å —Å–µ—Ç—å –Ω–µ —É–¥–∞–ª–æ—Å—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip', \n",
    "#                            'pre-trained-fasttext.zip')\n",
    "\n",
    "# from zipfile import ZipFile\n",
    "# with ZipFile('pre-trained-fasttext.zip', 'r') as f:\n",
    "#     f.extractall('pre-trained-fasttext')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

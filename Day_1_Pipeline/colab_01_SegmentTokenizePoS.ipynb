{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EvFlNWZ1eDbA"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scB5smameDYo"
      },
      "source": [
        "# Настройка\n",
        "Все зависимости перечислены в ячейке ниже. Кроме того, есть ещё дополнительные данные (opencorpora, например). Они тоже скачиваются в первых ячейках."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PoyIMujeDYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c16b47c-ed1f-4a66-bcf2-cd81cd88c4e6"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "nltk>=3.4.5\n",
        "razdel>=0.4.0\n",
        "rusenttokenize>=0.0.5\n",
        "b-labs-models>=2017.8.22\n",
        "lxml>=4.2.1\n",
        "spacy>=2.1.4\n",
        "pymystem3>=0.2.0\n",
        "rnnmorph>=0.4.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ysaxzVeDYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce33728e-8959-4355-8078-dc55cd5173b4"
      },
      "source": [
        "import sys\n",
        "!pip install --user --upgrade --force-reinstall -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk>=3.4.5\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting razdel>=0.4.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting rusenttokenize>=0.0.5\n",
            "  Downloading rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
            "Collecting b-labs-models>=2017.8.22\n",
            "  Downloading b_labs_models-2017.8.22-py2.py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lxml>=4.2.1\n",
            "  Downloading lxml-4.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy>=2.1.4\n",
            "  Downloading spacy-3.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymystem3>=0.2.0\n",
            "  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting rnnmorph>=0.4.0\n",
            "  Downloading rnnmorph-0.4.1.tar.gz (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2022.10.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.0/770.0 KB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
            "  Downloading pydantic-1.10.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
            "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
            "Collecting typer<0.8.0,>=0.3.0\n",
            "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-67.6.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.13.0\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
            "  Downloading thinc-8.1.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.5/922.5 KB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathy>=0.10.0\n",
            "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting numpy>=1.15.0\n",
            "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.6/491.6 KB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=0.19.0\n",
            "  Downloading scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=0.18.1\n",
            "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=2.1.4\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=2.7.0\n",
            "  Downloading h5py-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy2>=0.8\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting russian-tagsets==0.6\n",
            "  Downloading russian-tagsets-0.6.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonpickle>=0.9.4\n",
            "  Downloading jsonpickle-3.0.1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.2.0\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: rnnmorph, russian-tagsets, docopt\n",
            "  Building wheel for rnnmorph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rnnmorph: filename=rnnmorph-0.4.1-py3-none-any.whl size=19746379 sha256=51061e9e38086f1ddac213906d48d561a0e0703c303e52dc997f2c1b6e7863c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/a0/38/a38dd247132c2af64dec89a6575614d1c28b7a117374431123\n",
            "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for russian-tagsets: filename=russian_tagsets-0.6-py3-none-any.whl size=24637 sha256=368894f0481a4fa31cfe9d0da29047f96bcb380087494feef95e53395a9aba78\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/0a/91/1210a0142a88e04eccee020a9ed9b970aca66e48b09962cce5\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=27f56648d4ad7a81099edf0addbf8fa62604f063e5139232596654204fbefe9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "Successfully built rnnmorph russian-tagsets docopt\n",
            "Installing collected packages: russian-tagsets, rusenttokenize, razdel, python-crfsuite, pymorphy2-dicts-ru, docopt, dawg-python, cymem, wasabi, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, setuptools, regex, pymorphy2, packaging, numpy, murmurhash, MarkupSafe, lxml, langcodes, keras, jsonpickle, joblib, idna, click, charset-normalizer, certifi, catalogue, b-labs-models, typer, srsly, scipy, requests, pydantic, preshed, nltk, jinja2, h5py, blis, scikit-learn, pymystem3, pathy, confection, thinc, rnnmorph, spacy\n",
            "\u001b[33m  WARNING: The script razdel-ctl is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script pymorphy is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.9 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script normalizer is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script pathy is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script spacy is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "tensorflow 2.11.0 requires keras<2.12,>=2.11.0, but you have keras 2.12.0 which is incompatible.\n",
            "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.2.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\n",
            "cvxpy 1.3.0 requires setuptools<=64.0.2, but you have setuptools 67.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.2 b-labs-models-2017.8.22 blis-0.7.9 catalogue-2.0.8 certifi-2022.12.7 charset-normalizer-3.1.0 click-8.1.3 confection-0.0.4 cymem-2.0.7 dawg-python-0.7.2 docopt-0.6.2 h5py-3.8.0 idna-3.4 jinja2-3.1.2 joblib-1.2.0 jsonpickle-3.0.1 keras-2.12.0 langcodes-3.3.0 lxml-4.9.2 murmurhash-1.0.9 nltk-3.8.1 numpy-1.24.2 packaging-23.0 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.6 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 pymystem3-0.2.0 python-crfsuite-0.9.9 razdel-0.5.0 regex-2022.10.31 requests-2.28.2 rnnmorph-0.4.1 rusenttokenize-0.0.5 russian-tagsets-0.6 scikit-learn-1.2.2 scipy-1.10.1 setuptools-67.6.0 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 threadpoolctl-3.1.0 tqdm-4.65.0 typer-0.7.0 typing-extensions-4.5.0 urllib3-1.26.15 wasabi-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocGvOjsE9GKh"
      },
      "source": [
        "# Restart kernel\n",
        "import os\n",
        "os._exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FJ2-w6keDYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc24ef7-485e-4b75-c541-d9bb9f61e98f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')     #for splitting a long text into sentences\n",
        "nltk.download('stopwords') #most common words in data that you do not want to use"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dqa3TH-eDYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ed8cc1-f92c-40b6-f5da-68f5214bcd6f"
      },
      "source": [
        "import sys\n",
        "!python -m spacy download en_core_web_sm #маленькая модель для английского языка"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-21 14:25:04.370752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-21 14:25:05.700409: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-21 14:25:05.700556: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-21 14:25:05.700585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /root/.local/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: jinja2 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /root/.local/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /root/.local/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /root/.local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /root/.local/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /root/.local/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/.local/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A7fL9saeDYz"
      },
      "source": [
        "### Дополнительные данные\n",
        "\n",
        "Opencorpora: 31 Мб по сети, 530 Мб в распакованном виде"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BnW_XqBeDY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aaa9c63-64ec-4569-8d5a-e341932904b3"
      },
      "source": [
        "!wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:25:18--  http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2\n",
            "Resolving opencorpora.org (opencorpora.org)... 164.92.197.18\n",
            "Connecting to opencorpora.org (opencorpora.org)|164.92.197.18|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32754997 (31M) [application/x-bzip2]\n",
            "Saving to: ‘annot.opcorpora.xml.bz2’\n",
            "\n",
            "annot.opcorpora.xml 100%[===================>]  31.24M  16.0MB/s    in 1.9s    \n",
            "\n",
            "2023-03-21 14:25:21 (16.0 MB/s) - ‘annot.opcorpora.xml.bz2’ saved [32754997/32754997]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wX8NtUveDY2"
      },
      "source": [
        "!bzip2 -d annot.opcorpora.xml.bz2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiKkGUnbeDY4"
      },
      "source": [
        "### Тестовые примеры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdqD-e7beDY5"
      },
      "source": [
        "example1 = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
        "example2 = \"\"\"\n",
        "    An ambitious campus expansion plan was proposed by Fr. Vernon F. Gallagher in 1952.\n",
        "    Assumption Hall, the first student dormitory, was opened in 1954,\n",
        "    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.\n",
        "    It was during the tenure of F. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.\n",
        "\"\"\"\n",
        "example3 = \"\"\"\n",
        "    А что насчёт русского языка? Хорошо ли сегментируются имена?\n",
        "    Ай да А.С. Пушкин! Ай да сукин сын!\n",
        "    «Как же так?! Захар...» — воскликнут Пронин.\n",
        "    - \"Так в чем же дело?\" - \"Не ра-ду-ют\".\n",
        "    И т. д. и т. п. В общем, вся газета.\n",
        "    Православие... более всего подходит на роль такой идеи...\n",
        "    Нефть за $27/барр. не снится.\n",
        "\"\"\"\n",
        "example4 = \"\"\"\n",
        "    Кружка-термос на 0.5л (50/64 см³, 516;...) стоит $3.88\n",
        "\"\"\"\n",
        "example5 = \"\"\"\n",
        "    Good muffins cost $3.88 in New York.  Please buy me two of them. Thanks.\n",
        "\"\"\""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cqtzsw4eDY7"
      },
      "source": [
        "# Сегментация предложений\n",
        "Первая задача - разбиение текста на предложения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGebcOPseDY8"
      },
      "source": [
        "### Экперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiEkV_MdeDY9"
      },
      "source": [
        "##### NLTK - Natural Language Toolkit\n",
        "Популярная платформа для анализа текстов. Особенно хорошо работает для английского. В основном не содержит ничего из машинного обучения, только старые добрые правила."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42DWKe5FeDY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd43d085-5b58-4f74-fbf8-8dd53398e6d2"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(example1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"this's a sent tokenize test.\",\n",
              " 'this is sent two.',\n",
              " 'is this sent three?',\n",
              " 'sent 4 is cool!',\n",
              " 'Now it’s your turn.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWpr39pPeDZA"
      },
      "source": [
        "А вот тут что-то пошло не так"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8bXtc-6eDZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd8263c-d222-47bf-c1d4-8f49a69cf703"
      },
      "source": [
        "sent_tokenize(example2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    An ambitious campus expansion plan was proposed by Fr.',\n",
              " 'Vernon F. Gallagher in 1952.',\n",
              " 'Assumption Hall, the first student dormitory, was opened in 1954,\\n    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.',\n",
              " 'It was during the tenure of F. Henry J. McAnulty that Fr.',\n",
              " \"Gallagher's ambitious plans were put to action.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dpmBe70eDZE"
      },
      "source": [
        "А что насчёт русского языка?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUNCCwnzeDZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7afbef-bec3-4d1a-ffa2-9834d0359b25"
      },
      "source": [
        "sent_tokenize(example3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    А что насчёт русского языка?',\n",
              " 'Хорошо ли сегментируются имена?',\n",
              " 'Ай да А.С.',\n",
              " 'Пушкин!',\n",
              " 'Ай да сукин сын!',\n",
              " '«Как же так?!',\n",
              " 'Захар...» — воскликнут Пронин.',\n",
              " '- \"Так в чем же дело?\"',\n",
              " '- \"Не ра-ду-ют\".',\n",
              " 'И т. д. и т. п. В общем, вся газета.',\n",
              " 'Православие... более всего подходит на роль такой идеи...\\n    Нефть за $27/барр.',\n",
              " 'не снится.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rWZ0bF4eDZH"
      },
      "source": [
        "https://github.com/Mottl/ru_punkt\n",
        "\n",
        "Data for sentence tokenization was taken from 3 sources:\n",
        "\n",
        "  * Articles from Russian Wikipedia (about 1 million sentences)\n",
        "  * Common Russian abbreviations from Russian orthographic dictionary, edited by V. V. Lopatin;\n",
        "  * Generated names initials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwr9BaZ3eDZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b61742e-b2f1-4022-bcac-aeeae634b471"
      },
      "source": [
        "sent_tokenize(example3, language=\"russian\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    А что насчёт русского языка?',\n",
              " 'Хорошо ли сегментируются имена?',\n",
              " 'Ай да А.С. Пушкин!',\n",
              " 'Ай да сукин сын!',\n",
              " '«Как же так?!',\n",
              " 'Захар...» — воскликнут Пронин.',\n",
              " '- \"Так в чем же дело?\"',\n",
              " '- \"Не ра-ду-ют\".',\n",
              " 'И т. д. и т. п. В общем, вся газета.',\n",
              " 'Православие... более всего подходит на роль такой идеи...\\n    Нефть за $27/барр.',\n",
              " 'не снится.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erWAIVigeDZL"
      },
      "source": [
        "https://github.com/natasha/razdel\n",
        "\n",
        "razdel старается разбивать текст на предложения и токены так, как это сделано в 4 датасетах: SynTagRus, OpenCorpora, ГИКРЯ и РНК из репозитория morphoRuEval-2017.\n",
        "\n",
        "В основном это новостные тексты и литература. Правила razdel заточены под них.\n",
        "\n",
        "На текстах другой тематики (социальные сети, научные статьи) библиотека может работать хуже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY1qpHh6eDZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d110d16-f990-4b7a-c001-1c2ae39ae44f"
      },
      "source": [
        "from razdel import sentenize\n",
        "list(sentenize(example3))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Substring(5, 33, 'А что насчёт русского языка?'),\n",
              " Substring(34, 65, 'Хорошо ли сегментируются имена?'),\n",
              " Substring(70, 88, 'Ай да А.С. Пушкин!'),\n",
              " Substring(89, 105, 'Ай да сукин сын!'),\n",
              " Substring(110, 123, '«Как же так?!'),\n",
              " Substring(124, 154, 'Захар...» — воскликнут Пронин.'),\n",
              " Substring(159, 181, '- \"Так в чем же дело?\"'),\n",
              " Substring(182, 198, '- \"Не ра-ду-ют\".'),\n",
              " Substring(203, 218, 'И т. д. и т. п.'),\n",
              " Substring(219, 239, 'В общем, вся газета.'),\n",
              " Substring(244,\n",
              "           301,\n",
              "           'Православие... более всего подходит на роль такой идеи...'),\n",
              " Substring(306, 335, 'Нефть за $27/барр. не снится.')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6DDGryLeDZP"
      },
      "source": [
        "https://github.com/deepmipt/ru_sentence_tokenizer\n",
        "    \n",
        "A simple and fast rule-based sentence segmentation. Tested on OpenCorpora and SynTagRus datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6iYyZ1TeDZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071462aa-9f5c-46c6-99ff-0279eb4e2898"
      },
      "source": [
        "from rusenttokenize import ru_sent_tokenize\n",
        "ru_sent_tokenize(example3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Something went wrong while tokenizing\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['А что насчёт русского языка?',\n",
              " 'Хорошо ли сегментируются имена?',\n",
              " 'Ай да А.С. Пушкин!',\n",
              " 'Ай да сукин сын!',\n",
              " '«Как же так?!',\n",
              " 'Захар...» — воскликнут Пронин.',\n",
              " '- \"Так в чем же дело?\"',\n",
              " '- \"Не ра-ду-ют\".',\n",
              " 'И т. д. и т. п.',\n",
              " 'В общем, вся газета.',\n",
              " 'Православие... более всего подходит на роль такой идеи...',\n",
              " 'Нефть за $27/барр. не снится.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwNAtibneDZS"
      },
      "source": [
        "### Бенчмарки\n",
        "Много вариантов... Нужно измерять"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwae8cmleDZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96323590-e4e2-4d8f-83bb-2d8a0195979a"
      },
      "source": [
        "# WARNING: RAM bound task, XML parsing is expensive\n",
        "# Similar to https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb\n",
        "import re\n",
        "from lxml import etree\n",
        "\n",
        "# \\W -> Any non-word character\n",
        "RE_ENDS_WITH_PUNCT = re.compile(r\".*\\W$\") #сбор регулярного выражения в отдельный объект, который может быть использован для поиска\n",
        "\n",
        "OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n",
        "sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()')) #лист предложений\n",
        "singles = []   #список преложений\n",
        "compounds = [] #список попарных предолжений, стоящих рядом\n",
        "s2 = sentences.pop().strip() #берет последний элемент (удаляет его из списка) + удаление пробелов из начала и конца\n",
        "singles.append(s2)\n",
        "while sentences:\n",
        "    s1 = sentences.pop().strip()\n",
        "    singles.append(s1)\n",
        "    if RE_ENDS_WITH_PUNCT.match(s1) and not s1.endswith(':') and not s2.startswith('—'):\n",
        "        compounds.append((s1, s2))\n",
        "    s2 = s1\n",
        "        \n",
        "print(f'Read {len(singles)} sentences from {OPENCORPORA_FILE}')\n",
        "        \n",
        "del sentences"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 110304 sentences from annot.opcorpora.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2havDX5seDZV"
      },
      "source": [
        "def check_sent_tokenizer(tokenizer, singles, compounds):\n",
        "    correct_count_in_singles = 0\n",
        "    for sentence in singles:\n",
        "        correct_count_in_singles += len(tokenizer(sentence)) == 1 #подсчет единичных предложений из списка single\n",
        "\n",
        "    correct_count_in_compounds = 0\n",
        "    for s1, s2 in compounds:\n",
        "        correct_count_in_compounds += tokenizer(s1 + ' ' + s2) == [s1, s2] #подсчет пар предложений из списка compounds\n",
        "\n",
        "    return (correct_count_in_singles / len(singles), correct_count_in_compounds / len(compounds))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvWnP6bEeDZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da00683-bb5a-462b-aaee-61b38f6961a2"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(sent_tokenize, singles, compounds)\n",
        "print(f'sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.25 s, sys: 32 ms, total: 9.28 s\n",
            "Wall time: 9.45 s\n",
            "sent_tokenizer scores: 94.28%, 86.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f9m7nnneDZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6b1e94-cf98-4a72-d69b-ef50957e27cc"
      },
      "source": [
        "russian_sent_tokenize = lambda s : sent_tokenize(s, language=\"russian\")\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(russian_sent_tokenize, singles, compounds)\n",
        "print(f'russian sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.06 s, sys: 28.1 ms, total: 8.08 s\n",
            "Wall time: 8.18 s\n",
            "russian sent_tokenizer scores: 96.73%, 88.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClqfLRveDZh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00ec9c8-309f-4255-b5f0-6754fae973d2"
      },
      "source": [
        "from razdel import sentenize\n",
        "razdel_sent_tokenize = lambda text : [s.text for s in sentenize(text)]\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(razdel_sent_tokenize, singles, compounds)\n",
        "print(f'razdel scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6.6 s, sys: 18.6 ms, total: 6.62 s\n",
            "Wall time: 6.65 s\n",
            "razdel scores: 99.07%, 95.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuAOcawmeDZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f72e54c-795f-438c-ffc5-48dd57434d43"
      },
      "source": [
        "from rusenttokenize import ru_sent_tokenize\n",
        "deepmipt_sent_tokenize = ru_sent_tokenize\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(deepmipt_sent_tokenize, singles, compounds)\n",
        "print(f'deepmipt scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.4 s, sys: 15.9 ms, total: 7.42 s\n",
            "Wall time: 7.52 s\n",
            "deepmipt scores: 98.73%, 93.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tQJPC51eDZs"
      },
      "source": [
        "Аналогичные бенчмарки:\n",
        "- https://github.com/natasha/razdel/blob/master/eval.ipynb\n",
        "- https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zmDYwWleDZt"
      },
      "source": [
        "### Задание 1: \"Кирпич\"\n",
        "Скачайте предложенный текст. Найдите первое предложение, которое отличается в разбиениях, порождённых rusenttokenize и razdel. Верните номер этого предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKi00K-seDZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3565056f-c77a-4559-9da8-046033d0875c"
      },
      "source": [
        "!wget https://www.dropbox.com/s/q5wo34gfbepc7am/htbg.txt"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:29:10--  https://www.dropbox.com/s/q5wo34gfbepc7am/htbg.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/q5wo34gfbepc7am/htbg.txt [following]\n",
            "--2023-03-21 14:29:11--  https://www.dropbox.com/s/raw/q5wo34gfbepc7am/htbg.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8e079619ea28ff9f5be2185928.dl.dropboxusercontent.com/cd/0/inline/B4qgZGznBVpoqNKCgkee6te6jkS8s7Lx792AY0hfYiDH97pP_gZlxY-A2ftV05Dei_9Jx3B_JkI-_heI3TYThJwW1F_2uMNslIAzfPktSxDmRdPRAuuysoXmu1y1Ua9tTIrxUi61w2FsgGvSN4r5GWkEdRT7ducWx0puLyNq9ZGGDQ/file# [following]\n",
            "--2023-03-21 14:29:12--  https://uc8e079619ea28ff9f5be2185928.dl.dropboxusercontent.com/cd/0/inline/B4qgZGznBVpoqNKCgkee6te6jkS8s7Lx792AY0hfYiDH97pP_gZlxY-A2ftV05Dei_9Jx3B_JkI-_heI3TYThJwW1F_2uMNslIAzfPktSxDmRdPRAuuysoXmu1y1Ua9tTIrxUi61w2FsgGvSN4r5GWkEdRT7ducWx0puLyNq9ZGGDQ/file\n",
            "Resolving uc8e079619ea28ff9f5be2185928.dl.dropboxusercontent.com (uc8e079619ea28ff9f5be2185928.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc8e079619ea28ff9f5be2185928.dl.dropboxusercontent.com (uc8e079619ea28ff9f5be2185928.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 592726 (579K) [text/plain]\n",
            "Saving to: ‘htbg.txt’\n",
            "\n",
            "htbg.txt            100%[===================>] 578.83K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-21 14:29:13 (11.2 MB/s) - ‘htbg.txt’ saved [592726/592726]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLzXHecqeDZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137d0a96-a003-4dd1-f484-5a07997d2282"
      },
      "source": [
        "from razdel import sentenize\n",
        "from rusenttokenize import ru_sent_tokenize\n",
        "\n",
        "\n",
        "with open(\"htbg.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    \n",
        "def get_first_different_sentence(text: str) -> int:\n",
        "    # YOUR CODE HERE\n",
        "    tok_1 = [s.text for s in sentenize(text)]\n",
        "    tok_2 = ru_sent_tokenize(text)\n",
        "    for i in range(min(len(tok_1), len(tok_2))):\n",
        "      if tok_1[i] != tok_2[i]:\n",
        "        return i\n",
        "    return -1\n",
        "\n",
        "assert get_first_different_sentence(text) == 329"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Something went wrong while tokenizing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQq4iPN8eDZ2"
      },
      "source": [
        "### Задание 2: Lazy baseline\n",
        "Напишите свой sent_tokenize, который будет делить предложения только по точкам, восклицательным и вопросительным знакам. Измерьте для него время работы и метрики на opencorpora."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "def my_sent_tokenize(text):\n",
        "    # YOUR CODE HERE\n",
        "    sent_list = re.split('(?<=\\.) |(?<=\\!) |(?<=\\?) ', text)\n",
        "    return sent_list\n",
        "\n",
        "assert my_sent_tokenize(example1) == sent_tokenize(example1)\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(my_sent_tokenize, singles, compounds)\n",
        "assert singles_score >= 0.85\n",
        "print(f'your scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZzk6YYYTJnq",
        "outputId": "dc0bc8a2-cb20-419d-80b5-6b01e0950e16"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.23 s, sys: 33 ms, total: 3.26 s\n",
            "Wall time: 3.32 s\n",
            "your scores: 93.97%, 85.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvte2OBUeDZ5"
      },
      "source": [
        "# Токенизация\n",
        "\n",
        "Самый наивный способ токенизировать текст -- разделить с помощью split. Но split упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0BjSKCneDZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5830b06e-9899-43c4-ddd0-8b415fb01183"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(example5))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30jl5PpZeDZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ca78ef-3c27-4eee-c9b9-3ba9f1ff2a1b"
      },
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LegalitySyllableTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'NLTKWordTokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'SyllableTokenizer',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordDetokenizer']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD3EUO5WeDZ_"
      },
      "source": [
        "Они умеют выдавать индексы начала и конца каждого токена:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2z_ZybxeDaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c141dda9-7dea-4592-ca32-225bed40ef3a"
      },
      "source": [
        "from nltk import tokenize\n",
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "print(list(wh_tok.span_tokenize(example5)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(5, 9), (10, 17), (18, 22), (23, 28), (29, 31), (32, 35), (36, 41), (43, 49), (50, 53), (54, 56), (57, 60), (61, 63), (64, 69), (70, 77)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4HjdWCceDaE"
      },
      "source": [
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyzF35cOeDaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a499da74-a05a-42b8-b685-3aa19bf14cd4"
      },
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do', \"n't\", 'stop', 'me']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQCmuQpGeDaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66811204-074b-49ca-f4f2-b9cab0f7c815"
      },
      "source": [
        "import spacy\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "doc = spacy_nlp(example5, disable=[\"parser\"])\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n    ', 'Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', ' ', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg9w4wNSeDaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5faf6576-62ce-4d92-d8cd-2e25ebf6f757"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(example4))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Кружка-термос', 'на', '0.5л', '(', '50/64', 'см³', ',', '516', ';', '...', ')', 'стоит', '$', '3.88']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVvJnHfKeDaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6402b081-c540-4830-ec77-38e9864ec331"
      },
      "source": [
        "from razdel import tokenize\n",
        "list(tokenize(example4))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Substring(5, 18, 'Кружка-термос'),\n",
              " Substring(19, 21, 'на'),\n",
              " Substring(22, 25, '0.5'),\n",
              " Substring(25, 26, 'л'),\n",
              " Substring(27, 28, '('),\n",
              " Substring(28, 33, '50/64'),\n",
              " Substring(34, 37, 'см³'),\n",
              " Substring(37, 38, ','),\n",
              " Substring(39, 42, '516'),\n",
              " Substring(42, 43, ';'),\n",
              " Substring(43, 46, '...'),\n",
              " Substring(46, 47, ')'),\n",
              " Substring(48, 53, 'стоит'),\n",
              " Substring(54, 55, '$'),\n",
              " Substring(55, 59, '3.88')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVCUdgpOeDaR"
      },
      "source": [
        "### Задание 3: Diff\n",
        "Напишите функцию, которая будет выводить разницу между токенизацией razdel'а и nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gRApSXtKeDaS"
      },
      "source": [
        "from difflib import SequenceMatcher # USE THIS\n",
        "from razdel import tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# with open(\"htbg.txt\", \"r\") as f:\n",
        "#     text = f.read()\n",
        "import io\n",
        "with io.open(\"htbg.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "    \n",
        "def get_tokenization_differences(text: str) -> int:\n",
        "    differences = []\n",
        "    # YOUR CODE HERE\n",
        "    tok_1 = [s.text for s in tokenize(text)]\n",
        "    tok_2 = word_tokenize(text)\n",
        "    s = SequenceMatcher(None, tok_1, tok_2)\n",
        "    for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
        "      if tag != 'equal':\n",
        "        differences.append(tok_2[j1:j2])\n",
        "    return differences\n",
        "\n",
        "assert len(get_tokenization_differences(text)) == 613"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IPBbpoVeDaU"
      },
      "source": [
        "# Стоп-слова и пунктуация\n",
        "\n",
        "Стоп-слова - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конкретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CwruKoseDaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b562f5c5-06d2-4fa8-ff1c-927165e57db1"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('russian'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PVbyxz-eDaW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f1b42ad6-a8eb-48f2-ca29-c40e1fb8b5e5"
      },
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rksBLbveDaZ"
      },
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAbQc-VzeDab"
      },
      "source": [
        "### Задание 4: Стоп-слова from scratch\n",
        "Постройте свой список стоп-слов на основе Opencorpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGRerPDseDac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66a876d-2844-4258-b569-57d5d81a0d4c"
      },
      "source": [
        "import re\n",
        "from lxml import etree\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n",
        "sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n",
        "print(f'Read {len(sentences)} sentences from {OPENCORPORA_FILE}')\n",
        "\n",
        "# # YOUR CODE HERE\n",
        "word_list = [s_t for s in sentences for s_t in word_tokenize(s)]  \n",
        "st_counter = Counter(word_list).most_common(100) \n",
        "my_stop_words = [i[0] for i in st_counter]\n",
        "\n",
        "del sentences\n",
        "\n",
        "print(my_stop_words[:10])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 110304 sentences from annot.opcorpora.xml\n",
            "[',', '.', 'в', 'и', 'на', 'не', '«', '»', '—', 'с']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkGQgO59KZ9G"
      },
      "source": [
        "# Стемминг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZsLS4NQKW77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67666e4e-cd80-4f60-af21-bd60236cfe76"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from razdel import tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\") \n",
        "print([stemmer.stem(token.text) for token in tokenize(example3)])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['а', 'что', 'насчет', 'русск', 'язык', '?', 'хорош', 'ли', 'сегментир', 'им', '?', 'а', 'да', 'а', '.', 'с', '.', 'пушкин', '!', 'а', 'да', 'сукин', 'сын', '!', '«', 'как', 'же', 'так', '?!', 'захар', '...', '»', '—', 'воскликнут', 'пронин', '.', '-', '\"', 'так', 'в', 'чем', 'же', 'дел', '?', '\"', '-', '\"', 'не', 'ра-ду-ют', '\"', '.', 'и', 'т', '.', 'д', '.', 'и', 'т', '.', 'п', '.', 'в', 'общ', ',', 'вся', 'газет', '.', 'православ', '...', 'бол', 'всег', 'подход', 'на', 'рол', 'так', 'ид', '...', 'нефт', 'за', '$', '27', '/', 'барр', '.', 'не', 'снит', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6ufubEeDae"
      },
      "source": [
        "# Лемматизация и морфологический анализ\n",
        "\n",
        "Лемматизация – это сведение разных форм одного слова к начальной форме – лемме. Почему это хорошо?\n",
        "* Мы хотим рассматривать как отдельную фичу каждое слово, а не каждую его отдельную форму.\n",
        "* Некоторые стоп-слова стоят только в начальной форме, и без лемматизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть два хороших лемматизатора: mystem и pymorphy. С pymorphy всё сразу понятно.\n",
        "\n",
        "Но как работать с Mystem:\n",
        "* Можно скачать mystem и запускать из терминала с разными параметрами\n",
        "* pymystem3 - обертка для питона, работает медленнее, но это удобно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfork2jueDaf"
      },
      "source": [
        "## Mystem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ZoKAS5eDag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee3f338-fc91-4695-f283-e11e7edea1cf"
      },
      "source": [
        "from pymystem3 import Mystem\n",
        "mystem_analyzer = Mystem()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print('before'.ljust(20), locale.getpreferredencoding())\n",
        "\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "print('after'.ljust(20), locale.getpreferredencoding())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxTovpDkl1om",
        "outputId": "6d18abe6-4a86-496f-f10f-6fdedbcff626"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before               ANSI_X3.4-1968\n",
            "after                UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7McqUjduMYxR"
      },
      "source": [
        "!chmod +x /root/.local/bin/mystem"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsSzbzVWeDai"
      },
      "source": [
        "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
        "\n",
        "    mystem_bin - путь к mystem, если их несколько\n",
        "    grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
        "    disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
        "    entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
        "\n",
        "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
        "\n",
        "Можно просто лемматизировать текст:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK6iAkmWeDai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48a54a4-49a3-4e25-be4f-04702cd631bf"
      },
      "source": [
        "print(mystem_analyzer.lemmatize(example3))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', '    ', 'а', ' ', 'что', ' ', 'насчет', ' ', 'русский', ' ', 'язык', '? ', 'хорошо', ' ', 'ли', ' ', 'сегментироваться', ' ', 'имя', '?', '\\n', '    ', 'ай', ' ', 'да', ' ', 'а', '.', 'с', '. ', 'пушкин', '! ', 'ай', ' ', 'да', ' ', 'сукин', ' ', 'сын', '!', '\\n', '    «', 'как', ' ', 'же', ' ', 'так', '?! ', 'захар', '...', '» — ', 'восклицать', ' ', 'пронин', '.', '\\n', '    - \"', 'так', ' ', 'в', ' ', 'чем', ' ', 'же', ' ', 'дело', '?\" - ', '\"', 'не', ' ', 'ра', '-', 'ду', '-', 'ют', '\"', '.', '\\n', '    ', 'и', ' ', 'т', '.', ' ', 'д', '.', ' ', 'и', ' ', 'т', '.', ' ', 'п', '. ', 'в', ' ', 'общий', ', ', 'весь', ' ', 'газета', '.', '\\n', '    ', 'православие', '...', ' ', 'много', ' ', 'все', ' ', 'подходить', ' ', 'на', ' ', 'роль', ' ', 'такой', ' ', 'идея', '...', '\\n', '    ', 'нефть', ' ', 'за', ' ', '$', '27', '/', 'барра', '.', ' ', 'не', ' ', 'сниться', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZg3McyqeDal"
      },
      "source": [
        "А можно получить грамматическую информацию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F_PNsqneDal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4842857-a2e4-4c31-c97a-0b3389a7ddf1"
      },
      "source": [
        "mystem_analyzer.analyze(example3)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '\\n'},\n",
              " {'text': '    '},\n",
              " {'analysis': [{'lex': 'а', 'wt': 0.9822148501, 'gr': 'CONJ='}], 'text': 'А'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'что',\n",
              "    'wt': 0.2934446278,\n",
              "    'gr': 'SPRO,ед,сред,неод=(вин|им)'}],\n",
              "  'text': 'что'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'насчет', 'wt': 1, 'gr': 'PR='}], 'text': 'насчёт'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'русский',\n",
              "    'wt': 0.9496245492,\n",
              "    'gr': 'A=(вин,ед,полн,муж,од|род,ед,полн,муж|род,ед,полн,сред)'}],\n",
              "  'text': 'русского'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'язык', 'wt': 0.9963354032, 'gr': 'S,муж,неод=род,ед'}],\n",
              "  'text': 'языка'},\n",
              " {'text': '? '},\n",
              " {'analysis': [{'lex': 'хорошо', 'wt': 0.0008292704217, 'gr': 'ADV=вводн'}],\n",
              "  'text': 'Хорошо'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'ли', 'wt': 0.7719288688, 'gr': 'PART='}],\n",
              "  'text': 'ли'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'сегментироваться',\n",
              "    'wt': 1,\n",
              "    'gr': 'V,несов,нп=непрош,мн,изъяв,3-л'}],\n",
              "  'text': 'сегментируются'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'имя', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|им,мн)'}],\n",
              "  'text': 'имена'},\n",
              " {'text': '?'},\n",
              " {'text': '\\n'},\n",
              " {'text': '    '},\n",
              " {'analysis': [{'lex': 'ай', 'wt': 0.9841479957, 'gr': 'INTJ='}],\n",
              "  'text': 'Ай'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'да', 'wt': 0.3749187405, 'gr': 'PART='}],\n",
              "  'text': 'да'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'а', 'wt': 0.9822148501, 'gr': 'CONJ='}], 'text': 'А'},\n",
              " {'text': '.'},\n",
              " {'analysis': [{'lex': 'с',\n",
              "    'wt': 2.216901809e-05,\n",
              "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
              "  'text': 'С'},\n",
              " {'text': '. '},\n",
              " {'analysis': [{'lex': 'пушкин',\n",
              "    'wt': 0.9968761998,\n",
              "    'gr': 'S,фам,муж,од=им,ед'}],\n",
              "  'text': 'Пушкин'},\n",
              " {'text': '! '},\n",
              " {'analysis': [{'lex': 'ай', 'wt': 0.9841479957, 'gr': 'INTJ='}],\n",
              "  'text': 'Ай'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'да', 'wt': 0.6249755963, 'gr': 'CONJ='}],\n",
              "  'text': 'да'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'сукин',\n",
              "    'wt': 1,\n",
              "    'gr': 'A,полн,притяж=(вин,ед,муж,неод|им,ед,муж)'}],\n",
              "  'text': 'сукин'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'сын', 'wt': 1, 'gr': 'S,муж,од=им,ед'}],\n",
              "  'text': 'сын'},\n",
              " {'text': '!'},\n",
              " {'text': '\\n'},\n",
              " {'text': '    «'},\n",
              " {'analysis': [{'lex': 'как', 'wt': 0.3756069802, 'gr': 'ADVPRO='}],\n",
              "  'text': 'Как'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'же', 'wt': 0.9351936974, 'gr': 'PART='}],\n",
              "  'text': 'же'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'так', 'wt': 0.9840554802, 'gr': 'ADVPRO='}],\n",
              "  'text': 'так'},\n",
              " {'text': '?! '},\n",
              " {'analysis': [{'lex': 'захар', 'wt': 1, 'gr': 'S,имя,муж,од=им,ед'}],\n",
              "  'text': 'Захар'},\n",
              " {'text': '...'},\n",
              " {'text': '» — '},\n",
              " {'analysis': [{'lex': 'восклицать',\n",
              "    'wt': 1,\n",
              "    'gr': 'V,нп=непрош,мн,изъяв,3-л,сов'}],\n",
              "  'text': 'воскликнут'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'пронин', 'wt': 1, 'gr': 'S,фам,муж,од=им,ед'}],\n",
              "  'text': 'Пронин'},\n",
              " {'text': '.'},\n",
              " {'text': '\\n'},\n",
              " {'text': '    - \"'},\n",
              " {'analysis': [{'lex': 'так', 'wt': 0.9840554802, 'gr': 'ADVPRO='}],\n",
              "  'text': 'Так'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'чем', 'wt': 0.8023791472, 'gr': 'CONJ='}],\n",
              "  'text': 'чем'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'же', 'wt': 0.9351936974, 'gr': 'PART='}],\n",
              "  'text': 'же'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'дело',\n",
              "    'wt': 0.9999356856,\n",
              "    'gr': 'S,сред,неод=(вин,ед|им,ед)'}],\n",
              "  'text': 'дело'},\n",
              " {'text': '?\" - '},\n",
              " {'text': '\"'},\n",
              " {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'Не'},\n",
              " {'text': ' '},\n",
              " {'analysis': [], 'text': 'ра'},\n",
              " {'text': '-'},\n",
              " {'analysis': [], 'text': 'ду'},\n",
              " {'text': '-'},\n",
              " {'analysis': [{'lex': 'ют',\n",
              "    'wt': 0.5926217878,\n",
              "    'gr': 'S,муж,неод=(вин,ед|им,ед)'}],\n",
              "  'text': 'ют'},\n",
              " {'text': '\"'},\n",
              " {'text': '.'},\n",
              " {'text': '\\n'},\n",
              " {'text': '    '},\n",
              " {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'И'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'т',\n",
              "    'wt': 1,\n",
              "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
              "  'text': 'т'},\n",
              " {'text': '.'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'д',\n",
              "    'wt': 1,\n",
              "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
              "  'text': 'д'},\n",
              " {'text': '.'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'и', 'wt': 0.9999770357, 'gr': 'CONJ='}], 'text': 'и'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'т',\n",
              "    'wt': 1,\n",
              "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
              "  'text': 'т'},\n",
              " {'text': '.'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'п',\n",
              "    'wt': 1,\n",
              "    'gr': 'S,сокр=(пр,мн|пр,ед|вин,мн|вин,ед|дат,мн|дат,ед|род,мн|род,ед|твор,мн|твор,ед|им,мн|им,ед)'}],\n",
              "  'text': 'п'},\n",
              " {'text': '. '},\n",
              " {'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'В'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'общий',\n",
              "    'wt': 1,\n",
              "    'gr': 'A=(пр,ед,полн,муж|пр,ед,полн,сред)'}],\n",
              "  'text': 'общем'},\n",
              " {'text': ', '},\n",
              " {'analysis': [{'lex': 'весь', 'wt': 1, 'gr': 'APRO=им,ед,жен'}],\n",
              "  'text': 'вся'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'газета', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}],\n",
              "  'text': 'газета'},\n",
              " {'text': '.'},\n",
              " {'text': '\\n'},\n",
              " {'text': '    '},\n",
              " {'analysis': [{'lex': 'православие',\n",
              "    'wt': 1,\n",
              "    'gr': 'S,сред,неод=(вин,ед|им,ед)'}],\n",
              "  'text': 'Православие'},\n",
              " {'text': '...'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'много', 'wt': 5.317491368e-05, 'gr': 'ADV=срав'}],\n",
              "  'text': 'более'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'все',\n",
              "    'wt': 0.6138339797,\n",
              "    'gr': 'SPRO,ед,сред,неод=род'}],\n",
              "  'text': 'всего'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'подходить',\n",
              "    'wt': 1,\n",
              "    'gr': 'V,нп=непрош,ед,изъяв,3-л,несов'}],\n",
              "  'text': 'подходит'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'на', 'wt': 0.9989522965, 'gr': 'PR='}], 'text': 'на'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'роль',\n",
              "    'wt': 0.9994611082,\n",
              "    'gr': 'S,жен,неод=(вин,ед|им,ед)'}],\n",
              "  'text': 'роль'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'такой',\n",
              "    'wt': 1,\n",
              "    'gr': 'APRO=(пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен|вин,ед,муж,неод|им,ед,муж)'}],\n",
              "  'text': 'такой'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'идея',\n",
              "    'wt': 0.9998233538,\n",
              "    'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}],\n",
              "  'text': 'идеи'},\n",
              " {'text': '...'},\n",
              " {'text': '\\n'},\n",
              " {'text': '    '},\n",
              " {'analysis': [{'lex': 'нефть', 'wt': 1, 'gr': 'S,жен,неод=(вин,ед|им,ед)'}],\n",
              "  'text': 'Нефть'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'за', 'wt': 1, 'gr': 'PR='}], 'text': 'за'},\n",
              " {'text': ' '},\n",
              " {'text': '$'},\n",
              " {'text': '27'},\n",
              " {'text': '/'},\n",
              " {'analysis': [{'lex': 'барра',\n",
              "    'wt': 0.991379209,\n",
              "    'qual': 'bastard',\n",
              "    'gr': 'S,жен,неод=род,мн'}],\n",
              "  'text': 'барр'},\n",
              " {'text': '.'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'сниться',\n",
              "    'wt': 1,\n",
              "    'gr': 'V,несов,нп=непрош,ед,изъяв,3-л'}],\n",
              "  'text': 'снится'},\n",
              " {'text': '.'},\n",
              " {'text': '\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJH0dy1YeDan"
      },
      "source": [
        "## Pymorphy\n",
        "\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5tsjLpjeDao"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOudNTMJeDap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13202ee2-6a87-40b3-cd54-0569806039e3"
      },
      "source": [
        "pymorphy2_analyzer.parse(\"мою\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='мою', tag=OpencorporaTag('ADJF,Apro femn,sing,accs'), normal_form='мой', score=0.970588, methods_stack=((DictionaryAnalyzer(), 'мою', 2049, 10),)),\n",
              " Parse(word='мою', tag=OpencorporaTag('VERB,impf,tran sing,1per,pres,indc'), normal_form='мыть', score=0.029411, methods_stack=((DictionaryAnalyzer(), 'мою', 2074, 1),))]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmRbpXNTB72C"
      },
      "source": [
        "### Задание 5: Анализ частей речи\n",
        "\n",
        "Используя pymorphy2, определите топ-10 самых частотных существительных и глаголов в тексте"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My2CZHRwCODO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd914a62-3c47-4f71-d6d6-215d1b3e1a10"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "pymorphy2_analyzer = MorphAnalyzer()\n",
        "# with open(\"htbg.txt\", \"r\") as f:\n",
        "#     text = f.read()\n",
        "import io\n",
        "with io.open(\"htbg.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "# YOUR CODE HERE\n",
        "word_list = word_tokenize(text)\n",
        "noun_list = []\n",
        "for word_i in word_list:\n",
        "  if str(pymorphy2_analyzer.parse(word_i)[0].tag).split(',')[0] == 'NOUN':\n",
        "    noun_list.append(word_i)\n",
        "Counter(noun_list).most_common(10)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Румата', 546),\n",
              " ('дон', 298),\n",
              " ('Рэба', 135),\n",
              " ('Дон', 88),\n",
              " ('дона', 83),\n",
              " ('Антон', 79),\n",
              " ('руки', 59),\n",
              " ('время', 58),\n",
              " ('Анка', 56),\n",
              " ('Пашка', 55)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8qJvkxveDas"
      },
      "source": [
        "## mystem vs. pymorphy\n",
        "\n",
        "1) Mystem работает невероятно медленно под windows на больших текстах.\n",
        "\n",
        "2) Снятие омонимии. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO55wLFkeDas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45cddc16-0c6e-4bfc-e677-4695b846a80c"
      },
      "source": [
        "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
        "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
        "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
        "\n",
        "print(mystem_analyzer.analyze(homonym1)[-5])\n",
        "print(mystem_analyzer.analyze(homonym2)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292664, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
            "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GApdUILveDax"
      },
      "source": [
        "## Rnnmorph\n",
        "Обёртка над pymorphy с разрешением омонимии\n",
        "\n",
        "https://github.com/IlyaGusev/rnnmorph\n",
        "\n",
        "https://habr.com/ru/post/339954/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUqNO1leDa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "610aaf4a-963e-4105-cb5d-d795424f407b"
      },
      "source": [
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "from razdel import tokenize\n",
        "\n",
        "predictor = RNNMorphPredictor(language=\"ru\")\n",
        "homonym = \"Косил косой косой косой\"\n",
        "print(predictor.predict([t.text for t in tokenize(homonym)])[1])\n",
        "print(predictor.predict([t.text for t in tokenize(homonym)])[-1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4afb17407919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrnnmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRNNMorphPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrazdel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNMorphPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ru\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhomonym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Косил косой косой косой\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/rnnmorph/predictor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrussian_tagsets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrnnmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTMMorphoAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrnnmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preparation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_tag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_from_opencorpora_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_gram_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrnnmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_preparation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_form\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordFormOut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.9/site-packages/rnnmorph/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymorphy2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrussian_tagsets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mconcatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine.functional'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeFOWPD17PC3"
      },
      "source": [
        "## GramEval-2020\n",
        "\n",
        "Соревнование по определению морфологических характеристик, определению синтаксических зависимостей и лемматизации. Готовых инструментов не получилось, но весь код всех конкурсантов доступен.\n",
        "* https://github.com/dialogue-evaluation/GramEval2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvFlNWZ1eDbA"
      },
      "source": [
        "### Задание 6: Формат\n",
        "\n",
        "Используя стандартные инструменты переведите корпус htbg.txt в формат CoNLL-U.\n",
        "Используйте следующие колонки: \n",
        "    1. Номер предложения в тексте\n",
        "    2. Токен в том виде, в котором он встретился в тексте\n",
        "    3. Лемма токена\n",
        "    4. POS-таг токена\n",
        "    5. Вектор грамматических значений токена\n",
        "    6. Целевая метка (сделайте метку везде OUT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from razdel import sentenize, tokenize\n",
        "from pymystem3 import Mystem\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "mystem_analyzer = Mystem()\n",
        "pymorphy2_analyzer = MorphAnalyzer()\n",
        "\n",
        "with open(\"htbg.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "sentenses = [i.text for i in sentenize(text)]\n",
        "\n",
        "conll_list = []\n",
        "for num_i, sent_i in enumerate(sentenses[:10]):\n",
        "  for tok_i in [j.text for j in tokenize(sent_i)]:\n",
        "\n",
        "    lem_tok_i = mystem_analyzer.lemmatize(tok_i)[0]\n",
        "    pos_tag_tok_i = str(pymorphy2_analyzer.parse(tok_i)[0].tag).split(',')[0]\n",
        "    gram_vector = '-' #????????\n",
        "    target = 'OUT'\n",
        "    conll_list.append([num_i, tok_i, lem_tok_i, pos_tag_tok_i, gram_vector, target])\n",
        "\n",
        "pd.DataFrame(conll_list, columns=['sentence number', 'token', 'lemma', 'pos_tag', 'gram', 'targer']).sample(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "mqV5CFCVsmPx",
        "outputId": "2174cfec-7395-4d44-fb2d-c0594d2e97e8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sentence number   token        lemma pos_tag gram targer\n",
              "74                 5       ?            ?    PNCT    -    OUT\n",
              "123                7  черной       черный    ADJF    -    OUT\n",
              "93                 6   стали  становиться    VERB    -    OUT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bcb3c72c-44e0-48df-8d21-7f197afc2d94\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence number</th>\n",
              "      <th>token</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos_tag</th>\n",
              "      <th>gram</th>\n",
              "      <th>targer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>5</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>PNCT</td>\n",
              "      <td>-</td>\n",
              "      <td>OUT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>7</td>\n",
              "      <td>черной</td>\n",
              "      <td>черный</td>\n",
              "      <td>ADJF</td>\n",
              "      <td>-</td>\n",
              "      <td>OUT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>6</td>\n",
              "      <td>стали</td>\n",
              "      <td>становиться</td>\n",
              "      <td>VERB</td>\n",
              "      <td>-</td>\n",
              "      <td>OUT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bcb3c72c-44e0-48df-8d21-7f197afc2d94')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bcb3c72c-44e0-48df-8d21-7f197afc2d94 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bcb3c72c-44e0-48df-8d21-7f197afc2d94');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfew_CvgeDbB"
      },
      "source": [
        "# Regex 101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDYSLv67eDbB"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPMiIDMEeDbE"
      },
      "source": [
        "#### match\n",
        "ищет по заданному шаблону в начале строки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ4RY9dveDbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52265e5-53dd-470b-c88d-d52d50931d5f"
      },
      "source": [
        "result = re.match('ab+c.', 'abcdefghijkabcabc') # ищем по шаблону 'ab+c.' \n",
        "print (result) # совпадение найдено:"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 4), match='abcd'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2qN9Q1keDbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7826fbae-58b7-4c04-8b84-05569775d2e6"
      },
      "source": [
        "print(result.group(0)) # выводим найденное совпадение"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqg4wISLeDbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517e50b4-b284-4808-e14f-d40cd71f0fd0"
      },
      "source": [
        "result = re.match('abc.', 'abdefghijkabcabc')\n",
        "print(result) # совпадение не найдено"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dop3xhkteDbN"
      },
      "source": [
        "#### search\n",
        "ищет по всей строке, возвращает только первое найденное совпадение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsP6-GN-eDbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62839005-8ed0-4093-a4e9-ecf20136ddff"
      },
      "source": [
        "result = re.search('ab+c.', 'aefgabchijkabcabc') \n",
        "print(result) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(4, 8), match='abch'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqd_IVNNeDbS"
      },
      "source": [
        "#### findall\n",
        "возвращает список всех найденных совпадений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox0yWO57eDbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d61d92-a776-4b7d-dadc-0e6ac96d7645"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abcd', 'abca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ncd6YPeDbV"
      },
      "source": [
        "Вопросы: \n",
        "1) почему нет последнего abc?\n",
        "2) почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN71xIw6eDbY"
      },
      "source": [
        "#### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsWWmhUKeDbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd769ac5-ca31-423b-9388-7bf75a64e5ae"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGR8y8uCeDbc"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alEOle-XeDbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b1a10b-cab0-4974-a937-b084c867a076"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit = 2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDIt0zSKeDbg"
      },
      "source": [
        "#### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d79CKEy5eDbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce889db-3997-4409-b3da-df9c5b04a642"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bbcbbc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhBHsBTyeDbo"
      },
      "source": [
        "#### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r3PuZ3feDbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84df653d-3b87-4e8a-dc9f-a6274c8b430d"
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_gSGRhKeDbu"
      },
      "source": [
        "# Ваш код"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}